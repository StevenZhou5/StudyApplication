{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tud\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "import pandas as pd \n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors\n",
    "from torchtext import data, datasets\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "# 设置随机数的seed，这样保证每次测试的数据一致\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 超参数hyper parameters初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数hyper paramerters \n",
    "BATCH_SIZE = 64\n",
    "EMBEDDING_SIZE = 100\n",
    "HIDDEN_SIZE = 100\n",
    "MAX_VOCAB_SIZE = 50000\n",
    "NUM_EPOCHS = 2\n",
    "LEARNING_RATE = 0.001\n",
    "GRAD_CLIP = 5.0 # 为了防止梯度爆炸，设置的权重上下限的绝对值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 根据IMDB数据集，对电影评论进行正面和负面评论分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备数据\n",
    "1、TorchText中的一个重要概念是Field。Field决定了你的数据会被怎样处理。在我们的情感分类任务中,我们所需要接触到的数据又文本字符串和两种情感，”pos“或者”neg“。\n",
    "2、Field的参数指定了数据会被怎样处理。\n",
    "3、我们使用TEXT field来定义如何处理电影评论，使用LABEL field来处理两个情感类别。\n",
    "4、我们的TEXT field带有tokenize=‘spacy’, 这表示我们会用spaCy tokenizer来tokenize英文句子。如果我们不特别声明tokenize这个参数，那么默认的分词方法是使用空格。\n",
    "\n",
    "5、安装spaCy\n",
    "    pip3 install -U spacy\n",
    "    python3 -m spacy download en\n",
    "6、LABEL由LabelField定义。这是一种特别的用来处理label的Field。\n",
    "7、更多关于Fields，参见https://github.com/pytorch/text/blob/master/torchtext/data/field.py\n",
    "8、和之前一样，我们会设定random seeds使实验可以复现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize='spacy')\n",
    "LABEL = data.LabelField(dtype=torch.float)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1、TorchText支持很多常见的自然语言处理数据集\n",
    "2、下面的代码会自动下贱IMDb数据集，然后分成train/test两个torchtext.datasets类别。数据被前面的Fieds处理。\n",
    "    IMDb数据集一共有50000电影评论，每个评论都被标注为正面的或负面的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples:25000\n",
      "Number of testing examples:25000\n"
     ]
    }
   ],
   "source": [
    "# 默认是下载到当前目录的'.data目录下面'，当然也可以通过root='path'来指定path\n",
    "# 下载地址'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "# 下载完记得解压\n",
    "train_data,test_data = datasets.IMDB.splits(TEXT,LABEL,root='/Users/zhenwuzhou/AiProject/data/')\n",
    "#查看每个数据split有多少条数据\n",
    "print(f'Number of training examples:{len(train_data)}')\n",
    "print(f'Number of testing examples:{len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['For', 'a', 'movie', 'that', 'gets', 'no', 'respect', 'there', 'sure', 'are', 'a', 'lot', 'of', 'memorable', 'quotes', 'listed', 'for', 'this', 'gem', '.', 'Imagine', 'a', 'movie', 'where', 'Joe', 'Piscopo', 'is', 'actually', 'funny', '!', 'Maureen', 'Stapleton', 'is', 'a', 'scene', 'stealer', '.', 'The', 'Moroni', 'character', 'is', 'an', 'absolute', 'scream', '.', 'Watch', 'for', 'Alan', '\"', 'The', 'Skipper', '\"', 'Hale', 'jr', '.', 'as', 'a', 'police', 'Sgt', '.'], 'label': 'pos'}\n"
     ]
    }
   ],
   "source": [
    "# 查看一个example\n",
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " 1、由于我们现在只有train/test 这两个数据集分类，所以我们需要创建一个新的validation set.我们可以使用.split()创建新的分类。\n",
    " 2、默认的数据分割比为70：30，如果我们声明split_ratio，可以改变split之间的比例，split_ratio = 0.8表示80%的数据是训练集，20%的是验证集。\n",
    " 3、我们还声明random_state这个参数，确保我们每次分割的数据集都是一样的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "train_data,valid_data = train_data.split(random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples:17500\n",
      "Number of validation examples:7500\n",
      "Number of testing examples:25000\n"
     ]
    }
   ],
   "source": [
    "# 检查一下现在每个部分有多少条数据\n",
    "print(f'Number of training examples:{len(train_data)}')\n",
    "print(f'Number of validation examples:{len(valid_data)}')\n",
    "print(f'Number of testing examples:{len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image1](../common/images/4-pytorch.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary:25002\n",
      "Unique tokens in lABEL vocabulary:2\n"
     ]
    }
   ],
   "source": [
    "# 对训练数据建立词向量表，\n",
    "# Glove:global vectors for word representation:表示词的全局向量\n",
    "# 其中vectors=\"glove.6B.100d\"：使用glove预训练的词向量\n",
    "# 下载地址：http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# 可以使用vectors_cache来指定glove预训练的词向量缓存位置\n",
    "TEXT.build_vocab(train_data,max_size=25000,vectors=\"glove.6B.100d\",vectors_cache=\"/Users/zhenwuzhou/AiProject/data/vector_cache\",unk_init=torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train_data)\n",
    "print(f'Unique tokens in TEXT vocabulary:{len(TEXT.vocab)}')\n",
    "print(f'Unique tokens in lABEL vocabulary:{len(LABEL.vocab)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Yaktocat](../common/images/4-pytorch.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 202476), (',', 192116), ('.', 165496), ('a', 109230), ('and', 109174), ('of', 101087), ('to', 93504), ('is', 76398), ('in', 61292), ('I', 54008), ('it', 53328), ('that', 48904), ('\"', 44043), (\"'s\", 43247), ('this', 42369), ('-', 37002), ('/><br', 35684), ('was', 34978), ('as', 30125), ('with', 29740)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', 'the', ',', '.', 'a', 'and', 'of', 'to', 'is']\n"
     ]
    }
   ],
   "source": [
    "# 我们可以直接使用stoi(string to int) 或者 itos（int to string）来查看我们的单词表\n",
    "print(TEXT.vocab.itos[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'neg': 0, 'pos': 1})\n"
     ]
    }
   ],
   "source": [
    "# 查看labels\n",
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最后一步的数据预处理\n",
    "1、最后一步数据的准备是创建iterators。每个iteartion都会返回一个batch的examples。\n",
    "2、我们会使用BucketIterator。BucketIterator会把长度产不多的句子放到统一个batch中，确保每个batch中不出现太多的padding。\n",
    "3、严格来说，我们这份notebook中的模型代码都有一个问题，也就是我们把<pad>也当做了模型的输入进行训练。更好的做法是在模型中把由<pad>产生的输出给消除掉。这里我们暂时简单处理，直接把<pad>也用作模型的输入了。由于<pad>数量不多，模型的效果也不差\n",
    "4、如果我们有GPU，还可以指定每个Iteration返回的tensor都在GPU上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# BucketIterator 会按照句子长度对句子进行排序，尽量把长度相近的句子放在一个batch中，\n",
    "# 这样就不会出现太多的padding\n",
    "train_iterator,valid_iterator,test_iterator = data.BucketIterator.splits(\n",
    "    (train_data,valid_data,test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    device=device)\n",
    "\n",
    "# seq_len * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 64]\n",
       "\t[.text]:[torch.LongTensor of size 49x64]\n",
       "\t[.label]:[torch.FloatTensor of size 64]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(valid_iterator))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0, 1422,  393,  ...,   66,  148,   11],\n",
      "        [  46,  520,  395,  ...,   23,  860,   57],\n",
      "        [2369, 5033, 1589,  ...,   97,    7,   28],\n",
      "        ...,\n",
      "        [  39,  520,   68,  ...,    1,    1,    1],\n",
      "        [  39,  205,    4,  ...,    1,    1,    1],\n",
      "        [  39,    4,    1,  ...,    1,    1,    1]])\n",
      "tensor([1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
      "        0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
      "        0., 1., 1., 1., 1., 0., 0., 1., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "print(batch.text)\n",
    "print(batch.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk> from Germany and I love the <unk> . I go 200 times a year . Tonight I saw \" Pecker \" , it was a wonderful evening . Thank you , Mr. Waters . Everybody who has a chance to see the movie , go ! ! !\n"
     ]
    }
   ],
   "source": [
    "# itos是一个字符串list，这里list里面通过缩影位置对应这字典表里面的字符串\n",
    "print(\" \".join(TEXT.vocab.itos[i] for i in batch.text[:,0].data.cpu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Averaging模型\n",
    "1、我们首先介绍一个简单的Word Averaging模型。这个模型非常简单，我们把每个单词都通过Embedding层投射成word embedding vector，然后把一句话中的所有word vector做个平均，就是整个句子的vector表示了。接下来把这个sentence vector传入一个Linear层，做分类即可。\n",
    "![image.png](../common/images/4-pytorch03.png)\n",
    "\n",
    "2、我们使用avg_pool2d来做average pooling。我们的目标是把sentence length那个维度平均成1，然后保留embedding这个维度。\n",
    "![image.png](../common/images/4-pytorch04.png)\n",
    "\n",
    "3、avg_pool2d的kernel size是(embedded.shape[1],1),所以句子长度的那个维度压扁。\n",
    "![image.png](../common/images/4-pytorch05.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义一个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class WordAVGModel(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_size,output_size,pad_idx):\n",
    "        super(WordAVGModel,self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size,embedding_size, padding_idx=pad_idx)\n",
    "        self.linear = nn.Linear(embedding_size,output_size)\n",
    "        \n",
    "    def forward(self,text):\n",
    "        #进行embed操作\n",
    "        embeded = self.embed(text) # [seq_len,batch_size,embedding_size]\n",
    "        # 把 seq_len和batch_size换一下位置\n",
    "        # embeded = embeded.transpose(1,0) # [batch_size,seq_len,embedding_size],这是一种交换维度1和维度2的方法：就是转置\n",
    "        embeded = embeded.permute(1,0,2) # [batch_size,seq_len,embedding_size], 这是一种任意交换顺序的方式\n",
    "        \n",
    "        # 对embeding的每一个维度做针对此维度上句子中所有词的平均池化，最后得到的是一个embeding的句子信息向量\n",
    "        # 池化后的seq_len维度将会消失：[batch_size,1,embedding_size]，所以可以直接用squeeze()来讲此维度去掉\n",
    "        pooled = F.avg_pool2d(embeded,(embeded.shape[1],1)).squeeze() # [batch_size,embedding_size]\n",
    "        \n",
    "        # 最后在做一层全连接返回\n",
    "        return self.linear(pooled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "EMBEDDING_SIZE = 100\n",
    "OUTPUT_SIZE = 1\n",
    "# 这个值得是字符串'<pad>'在字典中的索引，就是embedding操作中需要传入的值：可能是为了\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token] \n",
    "\n",
    "model = WordAVGModel(vocab_size=VOCAB_SIZE,\n",
    "                     embedding_size=EMBEDDING_SIZE,\n",
    "                     output_size=OUTPUT_SIZE,\n",
    "                     pad_idx=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordAVGModel(\n",
       "  (embed): Embedding(25002, 100, padding_idx=1)\n",
       "  (linear): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看模型\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500200"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).numel() # numel()方法会帮你输出参数有多少个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500301"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算模型一共有多少参数\n",
    "def count_parameers(model):\n",
    "    # 把所有需要梯度下降的参数都统计出来，最后得到参数总个数\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count_parameers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1807,  0.2208, -0.8832,  ...,  0.8083, -0.2904,  0.1586],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.7884, -2.0365, -1.1188,  ..., -0.2311, -1.8755, -0.9730],\n",
       "        ...,\n",
       "        [-0.0396, -0.3214, -0.6717,  ...,  0.2224, -0.9313, -0.6235],\n",
       "        [-1.2636,  1.4050, -0.0456,  ..., -0.9127,  0.1079,  0.8846],\n",
       "        [-0.2148,  0.5147, -0.9955,  ..., -0.5769,  0.9640,  1.3750]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embed.weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型参数的一些初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25002, 100])\n",
      "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n",
      "        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n",
      "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
      "        ...,\n",
      "        [ 0.4765,  0.2254,  0.3035,  ..., -0.2082,  0.1948,  0.8972],\n",
      "        [-0.2472, -1.1190,  0.3695,  ..., -0.5236, -1.1763,  1.4334],\n",
      "        [-0.2821,  0.0417,  0.4807,  ..., -0.5425, -0.7024,  1.3024]])\n"
     ]
    }
   ],
   "source": [
    "# 这是斯坦福的glove预训练词向量\n",
    "pretrained_embedding = TEXT.vocab.vectors\n",
    "print(pretrained_embedding.shape)\n",
    "print(pretrained_embedding)\n",
    "# 我们在初始化词向量的时候设置成glov的词向量\n",
    "model.embed.weight.data.copy_(pretrained_embedding)\n",
    "\n",
    "# 这个值得是字符串'<unk>'在字典中的索引，\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "# 把'<pad>'和‘<unk>’的词向量权重初始化为0\n",
    "model.embed.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_SIZE)\n",
    "model.embed.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化器\n",
    "optimizer = torch.optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义Loss_fun:\n",
    "![image.png](../common/images/BCELoss.png) \n",
    "# BCEWithLogitsLoss = Sigmoid+BCELoss，\n",
    "# 当网络最后一层使用nn.Sigmoid时，就用BCELoss，\n",
    "# 当网络最后一层不使用nn.Sigmoid时，就用BCEWithLogitsLoss。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因为我们最后一层没有用nn.Sigmoid，所以要用BCEWithLogitsLoss\n",
    "loss_fun = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 根据当前device转成对应的cpu或者gpu格式\n",
    "model = model.to(device)\n",
    "loss_fun = loss_fun.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算预测的准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binart_accuracy(preds,y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义训练方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,iterator,optimier,loss_fun):\n",
    "    epoch_loss,epoch_acc = 0.,0.\n",
    "    total_len = 0.\n",
    "    model.train()\n",
    "    i = 0;\n",
    "    for batch in iterator:\n",
    "        # 前向传播,返回的是一个shape[batch_size*1],我们要把它压平到[batch_size],\n",
    "        # 这样才能用loss_fun去计算\n",
    "        preds = model(batch.text).squeeze() # [batch_size]\n",
    "        # 计算loss\n",
    "        loss = loss_fun(preds,batch.label)\n",
    "        \n",
    "        # 每隔10个epoch打印一下loss\n",
    "#         if i % 100 == 0:\n",
    "#             print(\"batch:\",i,\"; loss:\",loss)\n",
    "#         i+=1\n",
    "        \n",
    "        # 计算正确率\n",
    "        acc = binart_accuracy(preds,batch.label)\n",
    "        \n",
    "        # 清除梯度\n",
    "        optimier.zero_grad()\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        # 梯度下降\n",
    "        optimier.step()\n",
    "        \n",
    "        # 注意这里的loss是针对一个epoch的每条数据的平均值，\n",
    "        # 所以要为了后面计算总的loss平均值，\n",
    "        # 我么需要把每个epoch的loss都计算出来\n",
    "        epoch_loss += loss.item() * len(batch.label)\n",
    "        epoch_acc += acc.item() * len(batch.label)\n",
    "        total_len += len(batch.label)\n",
    "        \n",
    "    # 最后返回总的平均loss和总的平均正确率acc\n",
    "    # 但是二分类问题用f1Score会更加准确一些,可以避免偏斜类维问题(正负样本的比例悬殊)\n",
    "    return epoch_loss/total_len, epoch_acc/total_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义评价方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,iterator,loss_fun):\n",
    "    epoch_loss,epoch_acc = 0.,0.\n",
    "    total_len = 0.\n",
    "    model.eval()\n",
    "    for batch in iterator:\n",
    "        # 前向传播\n",
    "        preds = model(batch.text).squeeze()\n",
    "        # 计算loss\n",
    "        loss = loss_fun(preds,batch.label)\n",
    "        # 计算正确率\n",
    "        acc = binart_accuracy(preds,batch.label)\n",
    "        \n",
    "        # 不需要进行梯度下降\n",
    "        \n",
    "        epoch_loss += loss.item() * len(batch.label)\n",
    "        epoch_acc += acc.item() * len(batch.label)\n",
    "        total_len += len(batch.label)\n",
    "        \n",
    "    # 在评价完毕后需要把模型修改为训练模式\n",
    "    model.train()\n",
    "    # 最后返回总的平均loss和总的平均正确率acc\n",
    "    # 但是二分类问题用f1Score会更加准确一些,可以避免偏斜类维问题(正负样本的比例悬殊)\n",
    "    return epoch_loss/total_len, epoch_acc/total_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordAVGModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train Loss: 0.6845390934807913 Train Acc 0.6138857142584665\n",
      "Epoch 0 Valid Loss: 0.6252183569908142 Valid Acc 0.6944000000317891\n",
      "Epoch 1 Train Loss: 0.6419278195108686 Train Acc 0.7377142857142858\n",
      "Epoch 1 Valid Loss: 0.531813969039917 Valid Acc 0.7402666666984558\n",
      "Epoch 2 Train Loss: 0.5729172680309841 Train Acc 0.7889714285850525\n",
      "Epoch 2 Valid Loss: 0.4714519806067149 Valid Acc 0.7912\n",
      "Epoch 3 Train Loss: 0.4991948090893882 Train Acc 0.8277714285714286\n",
      "Epoch 3 Valid Loss: 0.41284211173057556 Valid Acc 0.8317333333333333\n",
      "Epoch 4 Train Loss: 0.4395295122078487 Train Acc 0.8586285714694432\n",
      "Epoch 4 Valid Loss: 0.40660085196495055 Valid Acc 0.8490666666666666\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-e26b3a38972b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbest_valid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mvaild_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-124-acbafb84f5dd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimier, loss_fun)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moptimier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# 反向传播\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;31m# 梯度下降\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0moptimier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "best_valid_acc = 0.\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, train_acc = train(model,train_iterator,optimizer,loss_fun)\n",
    "    vaild_loss, valid_acc = evaluate(model,valid_iterator,loss_fun)\n",
    "    \n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        torch.save(model.state_dict(),\"/Users/zhenwuzhou/.keras/models/wordavg/1model_dict.pth\")\n",
    "        torch.save(model,\"/Users/zhenwuzhou/.keras/models/wordavg/1model.pth\")\n",
    "            \n",
    "    print(\"Epoch\",epoch,\"Train Loss:\",train_loss,\"Train Acc\",train_acc)\n",
    "    print(\"Epoch\",epoch,\"Valid Loss:\",vaild_loss,\"Valid Acc\",valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从指定路径下加载训练好的模型\n",
    "model.load_state_dict(torch.load(\"/Users/zhenwuzhou/.keras/models/wordavg/1model_dict.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对给出的一条新的评论进行判断\n",
    "import spacy\n",
    "nlp = spacy.load(\"en\")\n",
    "\n",
    "def predict_sentiment(sentence):\n",
    "    # 利用spacy来进行对句子进行分词\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device) # [seq_len]\n",
    "    tensor = tensor.unsqueeze(1) # [seq_len * batch_size(1)]\n",
    "    pred = torch.sigmoid(model(tensor))\n",
    "    return pred.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2129119869873648e-08\n",
      "0.9547051787376404\n",
      "4.03268041182514e-09\n",
      "0.9999457597732544\n",
      "1.0582433367101185e-11\n",
      "0.7428852915763855\n"
     ]
    }
   ],
   "source": [
    "# 预测一下新评论\n",
    "print(predict_sentiment(\"This film is horrible\"))\n",
    "\n",
    "print(predict_sentiment(\"This film is terrificl\"))\n",
    "\n",
    "print(predict_sentiment(\"This film is terrible\"))\n",
    "\n",
    "print(predict_sentiment(\"This film is good\"))\n",
    "\n",
    "print(predict_sentiment(\"This film is not bad\"))\n",
    "\n",
    "print(predict_sentiment(\"This film is not good\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999959468841553"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN模型来实现情感分类\n",
    "#1、下面我们尝试吧模型换成一个recurrent neural network(RNN).RNN经常会被用来encode一个sequence：\n",
    "                ![image.png](../common/images/4-pytroch06.png) \n",
    "#2、我们使用最后一个hidden state hT 来表示整个句子。\n",
    "#3、然后我们把hT通过一个线性变换f(全连接)，然后用来预测句子的情感。\n",
    "![image.png](../common/images/4-pytroch07.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义个RNN的Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_size,output_size,pad_idx,hidden_size,drop_out):\n",
    "        super(RNNModel,self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size,embedding_size, padding_idx=pad_idx)\n",
    "        # bidirectional表示双向RNN，num_layers表示神经网络层数\n",
    "        # 这里定义成双向两层的RNN神经网络\n",
    "        # batch_first = true 可以使得输出的batch_size在第一个维度\n",
    "        self.lstm = nn.LSTM(embedding_size,hidden_size,bidirectional=True,num_layers=2)\n",
    "        # 这里改为双向RNN，全连接的输入将变为embedding_size*2\n",
    "        self.linear = nn.Linear(embedding_size*2,output_size)\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        \n",
    "    def forward(self,text):\n",
    "        #进行embed操作\n",
    "        embeded = self.embed(text) # [seq_len,batch_size,embedding_size]\n",
    "        # 我们为了防止过拟合，做一dropout正则化\n",
    "        embeded = self.dropout(embeded)\n",
    "        \n",
    "        # 然后通过LSTM进行tensor传递计算\n",
    "        # ouput 包含了每个时刻最后一层输出的h_t的集合：(seq_len, batch, num_directions * hidden_size)\n",
    "        # hidden 是t=seq_len时即最后一个时刻时每一层的h_t(L = 1-num_layers): (num_layers * num_directions, batch, hidden_size)\n",
    "        # cell 和hidden一样，只是它存的是cell的信息：(num_layers * num_directions, batch, hidden_size)\n",
    "        output,(hidden,cell) = self.lstm(embeded)\n",
    "        \n",
    "        # hidden：[2*batch_size*hiden_size]\n",
    "        # h_n of shape (num_layers * num_directions, batch, hidden_size)\n",
    "        # 因为我们设置了双向num_directions = 2 ，两层num_layers=2\n",
    "        # hidden存放每一层的两个方法的hidden,\n",
    "        # 我们去取hidden的最后两个(因为最后一层的双向输出)来做拼接\n",
    "        hidden = torch.cat([hidden[-1],hidden[-2]],dim=1) # dim=1表示对第一个维度做拼接\n",
    "        \n",
    "        hidden = self.dropout(hidden.squeeze())\n",
    "        \n",
    "        # 最后在做一层全连接返回\n",
    "        return self.linear(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [1,2,3]\n",
    "A[-1] # 取最后一个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(vocab_size=VOCAB_SIZE,\n",
    "                 embedding_size=EMBEDDING_SIZE,\n",
    "                 output_size=OUTPUT_SIZE,\n",
    "                 pad_idx=PAD_IDX,\n",
    "                 hidden_size=100,\n",
    "                 drop_out=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNModel的参数输出化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25002, 100])\n",
      "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n",
      "        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n",
      "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
      "        ...,\n",
      "        [ 0.4765,  0.2254,  0.3035,  ..., -0.2082,  0.1948,  0.8972],\n",
      "        [-0.2472, -1.1190,  0.3695,  ..., -0.5236, -1.1763,  1.4334],\n",
      "        [-0.2821,  0.0417,  0.4807,  ..., -0.5425, -0.7024,  1.3024]])\n"
     ]
    }
   ],
   "source": [
    "# 这是斯坦福的glove预训练词向量\n",
    "pretrained_embedding = TEXT.vocab.vectors\n",
    "print(pretrained_embedding.shape)\n",
    "print(pretrained_embedding)\n",
    "# 我们在初始化词向量的时候设置成glov的词向量\n",
    "model.embed.weight.data.copy_(pretrained_embedding)\n",
    "\n",
    "# 这个值得是字符串'<unk>'在字典中的索引，\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "# 把'<pad>'和‘<unk>’的词向量权重初始化为0\n",
    "model.embed.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_SIZE)\n",
    "model.embed.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化器\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# 因为我们最后一层没有用nn.Sigmoid，所以要用BCEWithLogitsLoss\n",
    "loss_fun = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 根据当前device转成对应的cpu或者gpu格式\n",
    "model = model.to(device)\n",
    "loss_fun = loss_fun.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始RNN模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-f54270f3f1bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meopoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mvaild_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-79-acbafb84f5dd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimier, loss_fun)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# 前向传播,返回的是一个shape[batch_size*1],我们要把它压平到[batch_size],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# 这样才能用loss_fun去计算\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m# 计算loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-92-ce0a98999668>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# 然后通过LSTM进行tensor传递计算\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# hidden：[2*batch_size*hiden_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n\u001b[0;32m--> 526\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._get_flat_weights(), self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "best_valid_acc = 0.\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, train_acc = train(model,train_iterator,optimizer,loss_fun)\n",
    "    vaild_loss, valid_acc = evaluate(model,valid_iterator,loss_fun)\n",
    "    \n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        torch.save(model.state_dict(),\"/Users/zhenwuzhou/.keras/models/wordavg/rnnmodel_dict.pth\")\n",
    "        torch.save(model,\"/Users/zhenwuzhou/.keras/models/wordavg/rnnmodel.pth\")\n",
    "            \n",
    "    print(\"Epoch\",epoch,\"Train Loss:\",train_loss,\"Train Acc\",train_acc)\n",
    "    print(\"Epoch\",epoch,\"Valid Loss:\",vaild_loss,\"Valid Acc\",valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs,(hidden,cell) = model.lstm(model.embed(batch.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([49, 64, 200])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape#(seq_len, batch, num_directions * hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 100])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.shape# (num_layers * num_directions, batch, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN模型来实现情感分类\n",
    "1、我们先对每个词做embedding\n",
    "2、把每个句子变成等长的词组成，然后每个句子就可以变成seq_len * embedding_size的矩阵\n",
    "3、做词向量卷积，用卷积单词数*embedding_size当做filter的kernelSize\n",
    "4、选取不同单词数的多个filter对文本做卷积操作；\n",
    "5、卷积后做池化操作MAXpooling或者AVGpooling，最后把不同单词数filter的pooling结果cat连接在一起\n",
    "6、做全连接最后输出想要的维度的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_size,output_size,pad_idx,num_filters,filter_sizes,drop_out):\n",
    "        super(CNNModel,self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size,embedding_size, padding_idx=pad_idx)\n",
    "        # 定义多个词的卷积:filter_sizes将是一个数组[3,4,5],分别取3个词，4个词，5个词的词向量卷积\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=1,out_channels=num_filters,\n",
    "                      kernel_size=(fs,embedding_size))for fs in filter_sizes\n",
    "        ])\n",
    "        \n",
    "        # 单一的词数卷积\n",
    "#         self.conv = nn.Conv2d(in_channels=1,out_channels = num_filters,\n",
    "#                               kernel_size=(filter_size,embedding_size))\n",
    "        self.linear = nn.Linear(num_filters * len(filter_sizes),output_size)\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        \n",
    "    def forward(self,text):\n",
    "        #把[seq_len,batch_size]变成[batch_size,seq_len]\n",
    "        text = text.permute(1,0) # [batch_size,seq_len]\n",
    "        embeded = self.embed(text) # [batch_size,seq_len,embedding_size]\n",
    "        # 因为输入的文本只有一个channel，需要把channel的维度加上\n",
    "        # conv2d的输入要求为:Input: (N:样本数, C_{in}:channel数, H_{in}:高度, W_{in}:宽度)\n",
    "        embeded = embeded.unsqueeze(1) # [batch_size,1,seq_len,embedding_size]\n",
    "        \n",
    "        # 单个词数卷积 和池化\n",
    "        # 在卷积操作的时候用relu作为激活函数\n",
    "#         conved = F.relu(self.conv(embeded)) # [batch_size,num_filters,seq_len-filter_size+1,1]\n",
    "#         # 把第三个维度压平\n",
    "#         conved = conved.squeeze(3) # [batch_size,num_filters,seq_len-filter_size+1]\n",
    "        # 做pooling\n",
    "#         pooled = F.max_pool1d(conved,conved.shape[2])# [batch_size,num_filters,1]\n",
    "#         pooled = pooled.squeeze(2)\n",
    "    \n",
    "    \n",
    "    \n",
    "        # 多个词数量的卷积和池化\n",
    "        conved = [F.relu(conv(embeded)).squeeze(3) for conv in self.convs]\n",
    "        pooled = [F.max_pool1d(conv,conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        # 把多个词的卷积连接起来\n",
    "        pooled = torch.cat(pooled,dim=1) # [batch_size,num_filters * len(filter_sizes)]\n",
    "        \n",
    "        # 最后一层做一下dropout\n",
    "        pooled = self.dropout(pooled)\n",
    "        \n",
    "        return self.linear(pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25002, 100])\n",
      "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n",
      "        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n",
      "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
      "        ...,\n",
      "        [ 0.4765,  0.2254,  0.3035,  ..., -0.2082,  0.1948,  0.8972],\n",
      "        [-0.2472, -1.1190,  0.3695,  ..., -0.5236, -1.1763,  1.4334],\n",
      "        [-0.2821,  0.0417,  0.4807,  ..., -0.5425, -0.7024,  1.3024]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-5c645b8b4b43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mbest_valid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mvaild_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-79-acbafb84f5dd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimier, loss_fun)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moptimier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# 反向传播\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;31m# 梯度下降\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0moptimier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = CNNModel(vocab_size=VOCAB_SIZE,\n",
    "                 embedding_size=EMBEDDING_SIZE,\n",
    "                 output_size=OUTPUT_SIZE,\n",
    "                 pad_idx=PAD_IDX,\n",
    "                 num_filters=100, # 用多少个filter\n",
    "                 filter_sizes= [3,4,5], # 用几个单词做卷积\n",
    "                 drop_out=0.5)\n",
    "\n",
    "# 这是斯坦福的glove预训练词向量\n",
    "pretrained_embedding = TEXT.vocab.vectors\n",
    "print(pretrained_embedding.shape)\n",
    "print(pretrained_embedding)\n",
    "# 我们在初始化词向量的时候设置成glov的词向量\n",
    "model.embed.weight.data.copy_(pretrained_embedding)\n",
    "\n",
    "# 这个值得是字符串'<unk>'在字典中的索引，\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "# 把'<pad>'和‘<unk>’的词向量权重初始化为0\n",
    "model.embed.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_SIZE)\n",
    "model.embed.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_SIZE)\n",
    "\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# 因为我们最后一层没有用nn.Sigmoid，所以要用BCEWithLogitsLoss\n",
    "loss_fun = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 根据当前device转成对应的cpu或者gpu格式\n",
    "model = model.to(device)\n",
    "loss_fun = loss_fun.to(device)\n",
    "\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "best_valid_acc = 0.\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, train_acc = train(model,train_iterator,optimizer,loss_fun)\n",
    "    vaild_loss, valid_acc = evaluate(model,valid_iterator,loss_fun)\n",
    "    \n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        torch.save(model.state_dict(),\"/Users/zhenwuzhou/.keras/models/wordavg/cnnmodel_dict.pth\")\n",
    "        torch.save(model,\"/Users/zhenwuzhou/.keras/models/wordavg/cnnmodel.pth\")\n",
    "            \n",
    "    print(\"Epoch\",epoch,\"Train Loss:\",train_loss,\"Train Acc\",train_acc)\n",
    "    print(\"Epoch\",epoch,\"Valid Loss:\",vaild_loss,\"Valid Acc\",valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 长文本分类:hierarchical分层的\n",
    "hierarchical LSTM "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
