{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2, 1])\n",
      "tensor([[[0.7809],\n",
      "         [0.2537]],\n",
      "\n",
      "        [[0.9649],\n",
      "         [0.1673]],\n",
      "\n",
      "        [[0.8156],\n",
      "         [0.7914]],\n",
      "\n",
      "        [[0.3099],\n",
      "         [0.2186]],\n",
      "\n",
      "        [[0.1139],\n",
      "         [0.0593]]])\n",
      "torch.Size([5, 1, 2])\n",
      "tensor([[[0.7809, 0.2537]],\n",
      "\n",
      "        [[0.9649, 0.1673]],\n",
      "\n",
      "        [[0.8156, 0.7914]],\n",
      "\n",
      "        [[0.3099, 0.2186]],\n",
      "\n",
      "        [[0.1139, 0.0593]]])\n",
      "torch.Size([5, 2, 2])\n",
      "tensor([[[0.7809, 0.2537],\n",
      "         [0.7809, 0.2537]],\n",
      "\n",
      "        [[0.9649, 0.1673],\n",
      "         [0.9649, 0.1673]],\n",
      "\n",
      "        [[0.8156, 0.7914],\n",
      "         [0.8156, 0.7914]],\n",
      "\n",
      "        [[0.3099, 0.2186],\n",
      "         [0.3099, 0.2186]],\n",
      "\n",
      "        [[0.1139, 0.0593],\n",
      "         [0.1139, 0.0593]]])\n"
     ]
    }
   ],
   "source": [
    "# 测试expand方法的作用\n",
    "A = torch.rand(5,2,1)\n",
    "print(A.shape)\n",
    "print(A)\n",
    "B = A.permute(0,2,1)\n",
    "print(B.shape)\n",
    "print(B)\n",
    "C = B.expand(-1,2,-1) # 扩展就是把哪个维度上的值复制几份\n",
    "print(C.shape)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0.3635, 0.6664],\n",
      "        [0.9040, 0.8429],\n",
      "        [0.2058, 0.5958]]), tensor([[0.4231, 0.3937],\n",
      "        [0.9176, 0.7017],\n",
      "        [0.6273, 0.0342]])]\n",
      "torch.Size([3, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3635, 0.4231],\n",
       "         [0.6664, 0.3937]],\n",
       "\n",
       "        [[0.9040, 0.9176],\n",
       "         [0.8429, 0.7017]],\n",
       "\n",
       "        [[0.2058, 0.6273],\n",
       "         [0.5958, 0.0342]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stack操作测试\n",
    "A = [torch.rand(3,2),torch.rand(3,2)]\n",
    "print(A)\n",
    "stackA = torch.stack(A,-1)\n",
    "print(stackA.shape)\n",
    "stackA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "tensor([[2., 5.],\n",
      "        [3., 5.]])\n",
      "tensor([[0.0474, 0.9526],\n",
      "        [0.1192, 0.8808]])\n"
     ]
    }
   ],
   "source": [
    "# F的softmax的作用\n",
    "A = torch.tensor([[2.,5.],\n",
    "                 [3.,5.]])\n",
    "print(A.shape)\n",
    "print(A)\n",
    "print(F.softmax(A,dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2, 3])\n",
      "tensor([[[0.5847, 0.9882, 0.0402],\n",
      "         [0.3335, 0.5820, 0.9439]],\n",
      "\n",
      "        [[0.8150, 0.7132, 0.5204],\n",
      "         [0.9753, 0.1078, 0.1540]],\n",
      "\n",
      "        [[0.3415, 0.0234, 0.2402],\n",
      "         [0.5227, 0.4375, 0.7785]],\n",
      "\n",
      "        [[0.0743, 0.8308, 0.0052],\n",
      "         [0.7166, 0.3797, 0.4055]],\n",
      "\n",
      "        [[0.8669, 0.9656, 0.6711],\n",
      "         [0.1539, 0.7742, 0.6488]]])\n",
      "===============\n",
      "torch.return_types.max(\n",
      "values=tensor([[0.5847, 0.9882, 0.9439],\n",
      "        [0.9753, 0.7132, 0.5204],\n",
      "        [0.5227, 0.4375, 0.7785],\n",
      "        [0.7166, 0.8308, 0.4055],\n",
      "        [0.8669, 0.9656, 0.6711]]),\n",
      "indices=tensor([[0, 0, 1],\n",
      "        [1, 0, 0],\n",
      "        [1, 1, 1],\n",
      "        [1, 0, 1],\n",
      "        [0, 0, 0]]))\n",
      "===============\n",
      "torch.Size([5, 3])\n",
      "tensor([[0.5847, 0.9882, 0.9439],\n",
      "        [0.9753, 0.7132, 0.5204],\n",
      "        [0.5227, 0.4375, 0.7785],\n",
      "        [0.7166, 0.8308, 0.4055],\n",
      "        [0.8669, 0.9656, 0.6711]])\n"
     ]
    }
   ],
   "source": [
    "#torch.max,的测试\n",
    "A = torch.rand(5,2,3)\n",
    "print(A.shape)\n",
    "print(A)\n",
    "print(\"===============\")\n",
    "# dim=0 torch.Size([2, 3])取出了第0维度中的所有2*3=6个数相加和最大的那一组数\n",
    "# dim=1 torch.Size([5, 3])每一组中每一列取最大值构成新的1*3的一组数\n",
    "# dim=2 torch.Size([5, 2])每一组中每一行取最大值构成新的2*1的一组\n",
    "maxA2 = torch.max(A,dim=1)\n",
    "# print(maxA2.shape)\n",
    "print(maxA2)\n",
    "print(\"===============\")\n",
    "print(maxA2[0].shape)\n",
    "print(maxA2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2861,  1.8419],\n",
      "        [ 1.0022, -0.1568],\n",
      "        [-0.4694,  0.9617]])\n",
      "tensor([[ 1.1856, -1.4431],\n",
      "        [-0.0639,  0.3934],\n",
      "        [ 1.1764, -1.4525]])\n",
      "================\n",
      "tensor([[1.1856, 1.8419],\n",
      "        [1.0022, 0.3934],\n",
      "        [1.1764, 0.9617]])\n",
      "tensor([[ 1.1856, -1.4431],\n",
      "        [-0.0639,  0.3934],\n",
      "        [ 1.1764, -1.4525]])\n"
     ]
    }
   ],
   "source": [
    "#torch.max2,的测试\n",
    "A = torch.randn(3,2)\n",
    "print(A)\n",
    "B = torch.randn(3,2)\n",
    "print(B)\n",
    "torch.max(A,B,out=A)\n",
    "print(\"================\")\n",
    "print(A)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# power操作\n",
    "np.power(4,0.5) #  4的0.5次方，在基于负采样的的取词比例中可以用到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0295, 0.2603, 0.8781, 0.3195],\n",
      "         [0.6234, 0.3218, 0.2369, 0.1595]]])\n",
      "tensor([[[0.8979, 0.4294, 0.6101],\n",
      "         [0.6974, 0.4764, 0.1956],\n",
      "         [0.4635, 0.3978, 0.5712],\n",
      "         [0.0248, 0.5652, 0.2259]]])\n",
      "tensor([[[0.7361, 0.5180, 0.6232],\n",
      "         [0.9799, 0.0461, 0.6374],\n",
      "         [0.2546, 0.6357, 0.1136],\n",
      "         [0.1184, 0.0487, 0.7997]]])\n",
      "tensor([[[0.4151, 0.9929, 0.2187],\n",
      "         [0.0835, 0.5420, 0.9282],\n",
      "         [0.5881, 0.4215, 0.6292],\n",
      "         [0.1119, 0.4513, 0.1579]]])\n",
      "\n",
      "Q的shape torch.Size([1, 2, 3])\n",
      "tensor([[[0.6230, 0.6666, 0.6427],\n",
      "         [0.8979, 0.6053, 0.6146]]])\n",
      "K的shape torch.Size([1, 2, 3])\n",
      "tensor([[[0.5383, 0.6011, 0.5396],\n",
      "         [0.8534, 0.4961, 0.7480]]])\n",
      "V的shape torch.Size([1, 2, 3])\n",
      "tensor([[[0.5861, 0.6848, 0.8511],\n",
      "         [0.4428, 0.9652, 0.6092]]])\n",
      "attn1的shape: torch.Size([1, 2, 2])\n",
      "tensor([[[1.0828, 1.3431],\n",
      "         [1.1788, 1.5263]]])\n",
      "attn2的shape: torch.Size([1, 2, 2])\n",
      "tensor([[[0.1353, 0.1679],\n",
      "         [0.1473, 0.1908]]])\n",
      "attn3的shape: torch.Size([1, 2, 2])\n",
      "tensor([[[0.4919, 0.5081],\n",
      "         [0.4891, 0.5109]]])\n",
      "tensor([[[0.5133, 0.8273, 0.7282],\n",
      "         [0.5129, 0.8280, 0.7275]]])\n"
     ]
    }
   ],
   "source": [
    "# Self Attention的Q,K,V 向量化矩阵计算\n",
    "X = torch.rand(1,2,4)\n",
    "print(X)\n",
    "Wq = torch.rand(1,4,3)\n",
    "print(Wq)\n",
    "Wk = torch.rand(1,4,3)\n",
    "print(Wk)\n",
    "Wv = torch.rand(1,4,3)\n",
    "print(Wv)\n",
    "\n",
    "print()\n",
    "Q = torch.bmm(X,Wq)\n",
    "print(\"Q的shape\",Q.shape)\n",
    "print(Q)\n",
    "K = torch.bmm(X,Wk)\n",
    "print(\"K的shape\",K.shape)\n",
    "print(K)\n",
    "V = torch.bmm(X,Wv)\n",
    "print(\"V的shape\",V.shape)\n",
    "print(V)\n",
    "\n",
    "attn1 = torch.bmm(Q,K.transpose(2,1))\n",
    "print(\"attn1的shape:\",attn1.shape)\n",
    "print(attn1)\n",
    "\n",
    "attn2 = attn1 / 8.0\n",
    "print(\"attn2的shape:\",attn2.shape)\n",
    "print(attn2)\n",
    "\n",
    "attn3 = torch.nn.Softmax(dim=2)(attn2)\n",
    "print(\"attn3的shape:\",attn3.shape)\n",
    "print(attn3)\n",
    "\n",
    "output = torch.bmm(attn3,V)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5647, 0.1576, 0.6414],\n",
      "        [0.3598, 0.8444, 0.0225],\n",
      "        [0.6837, 0.0784, 0.2989],\n",
      "        [0.4277, 0.8252, 0.0287],\n",
      "        [0.2266, 0.9268, 0.3346]])\n",
      "13\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# argmax()测试\n",
    "A = torch.rand(5,3)\n",
    "print(A)\n",
    "print(A.argmax().item()) # 最大的那个值在矩阵被打平为一维List时中的索引位置\n",
    "print(A[0].argmax().item()) # 最大的那个值在列表中的索引位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1464,  1.0439,  0.4604],\n",
      "        [-0.0702,  1.5683, -0.3675],\n",
      "        [-0.4626,  0.3654,  0.7164],\n",
      "        [ 0.7878,  0.6938, -0.7750],\n",
      "        [-0.4429,  0.6910,  0.4281]])\n",
      "tensor(4.4905)\n",
      "torch.Size([5, 3])\n",
      "tensor([[1.3580],\n",
      "        [1.1306],\n",
      "        [0.6191],\n",
      "        [0.7066],\n",
      "        [0.6763]])\n",
      "cumsumA: tensor([[-0.1464,  1.0439,  0.4604],\n",
      "        [-0.2166,  2.6122,  0.0929],\n",
      "        [-0.6792,  2.9776,  0.8093],\n",
      "        [ 0.1086,  3.6714,  0.0343],\n",
      "        [-0.3343,  4.3624,  0.4625]])\n"
     ]
    }
   ],
   "source": [
    "# sum测试\n",
    "A = torch.randn(5,3)\n",
    "print(A)\n",
    "sumA = A.sum()\n",
    "print(sumA)\n",
    "print(A.size())\n",
    "sum_to_sizeA = A.sum_to_size(A.size()[0],1) # 这样相当于每行相加\n",
    "print(sum_to_sizeA)\n",
    "cumsumA = A.cumsum(0) # 0代表列依次递加，1:代表行依次递加\n",
    "print(\"cumsumA:\",cumsumA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.3479, -1.0443, -0.0359, -0.5181, -1.1864],\n",
      "        [ 1.0870,  1.9154,  0.0949, -0.0053,  0.7588],\n",
      "        [ 0.9656, -1.9067,  0.0141, -3.3089, -0.3452],\n",
      "        [ 0.3732, -1.6621,  0.3642, -0.0252, -0.0075],\n",
      "        [ 0.6322, -0.5308,  0.7938, -1.0148,  1.3878],\n",
      "        [-2.0887,  0.1318, -0.2714,  0.0504,  0.0523],\n",
      "        [ 0.4787,  0.4455,  1.2219, -2.2179,  1.4942],\n",
      "        [-0.1906,  0.2788,  0.5600,  2.0845, -1.0782],\n",
      "        [-0.3614, -2.3480, -0.4730, -0.2444,  1.5346],\n",
      "        [-0.3884,  1.0739,  1.2118, -0.8928, -0.1041],\n",
      "        [-0.2341,  0.3947,  0.2525,  0.4502,  0.0733]])\n",
      "tensor([[-1.4368],\n",
      "        [ 3.8508],\n",
      "        [-4.5811],\n",
      "        [-0.9574],\n",
      "        [ 1.2682],\n",
      "        [-2.1256],\n",
      "        [ 1.4223],\n",
      "        [ 1.6545],\n",
      "        [-1.8921],\n",
      "        [ 0.9005],\n",
      "        [ 0.9366]])\n",
      "(tensor([ 1.3479, -1.0443, -0.0359, -0.5181, -1.1864]), tensor([-1.4368]))\n",
      "==================\n",
      "11\n",
      "0 0\n",
      "1 2\n",
      "2 3\n",
      "3 9\n",
      "4 1\n",
      "5 7\n",
      "6 8\n",
      "7 4\n",
      "8 5\n",
      "9 10\n",
      "10 6\n",
      "8\n",
      "================\n",
      "20\n",
      "0 0\n",
      "1 3\n",
      "2 2\n",
      "3 8\n",
      "4 4\n",
      "5 10\n",
      "6 6\n",
      "7 3\n",
      "8 2\n",
      "9 1\n",
      "10 5\n",
      "11 3\n",
      "12 4\n",
      "13 10\n",
      "14 10\n",
      "15 6\n",
      "16 7\n",
      "17 8\n",
      "18 3\n",
      "19 5\n",
      "================\n",
      "6\n",
      "0 [tensor([[ 0.6322, -0.5308,  0.7938, -1.0148,  1.3878],\n",
      "        [ 0.4787,  0.4455,  1.2219, -2.2179,  1.4942]]), tensor([[1.2682],\n",
      "        [1.4223]])]\n",
      "1 [tensor([[-0.3614, -2.3480, -0.4730, -0.2444,  1.5346],\n",
      "        [-0.1906,  0.2788,  0.5600,  2.0845, -1.0782]]), tensor([[-1.8921],\n",
      "        [ 1.6545]])]\n",
      "2 [tensor([[-0.2341,  0.3947,  0.2525,  0.4502,  0.0733],\n",
      "        [ 1.3479, -1.0443, -0.0359, -0.5181, -1.1864]]), tensor([[ 0.9366],\n",
      "        [-1.4368]])]\n",
      "3 [tensor([[ 1.0870,  1.9154,  0.0949, -0.0053,  0.7588],\n",
      "        [-2.0887,  0.1318, -0.2714,  0.0504,  0.0523]]), tensor([[ 3.8508],\n",
      "        [-2.1256]])]\n",
      "4 [tensor([[ 0.3732, -1.6621,  0.3642, -0.0252, -0.0075],\n",
      "        [-0.3884,  1.0739,  1.2118, -0.8928, -0.1041]]), tensor([[-0.9574],\n",
      "        [ 0.9005]])]\n",
      "5 [tensor([[ 0.9656, -1.9067,  0.0141, -3.3089, -0.3452]]), tensor([[-4.5811]])]\n"
     ]
    }
   ],
   "source": [
    "# Data处理测试\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "trainX = torch.randn(11,5) # 11个样本，每个样本有5个feature [11,5]\n",
    "trainY = trainX.sum_to_size(trainX.size()[0],1) # [11,1]\n",
    "print(trainX)\n",
    "print(trainY)\n",
    "train_dataset = TensorDataset(trainX,trainY) # 只要传入的tensor的第一个维度值相同即可;\n",
    "print(train_dataset.__getitem__(0)) # TensorDataset里面是一个tuple，可以用__getitem__(index)来取出某一个tensor\n",
    "\n",
    "print(\"==================\")\n",
    "train_sampler_no_replacement = RandomSampler(train_dataset) # 会将原始数据随机打乱\n",
    "print(len(train_sampler_no_replacement))\n",
    "for idx,item in enumerate(train_sampler_no_replacement):\n",
    "    print(idx,item)\n",
    "print(next(iter(train_sampler_no_replacement)))\n",
    "\n",
    "print(\"================\")\n",
    "# replacement表示是否可以重复，num_samples是要取的样本数目(因为可以重复，所以样本数取多少都可以)，默认不填写就是样本总数\n",
    "train_sampler_with_replacemen = RandomSampler(train_dataset,replacement=True,num_samples=20)\n",
    "print(len(train_sampler_with_replacemen))\n",
    "for idx,item in enumerate(train_sampler_with_replacemen):\n",
    "    print(idx,item)\n",
    "\n",
    "print(\"================\")\n",
    "train_dataloader = DataLoader(train_dataset,sampler=train_sampler_no_replacement,batch_size=2)\n",
    "print(len(train_dataloader)) # 这里总共11个样本，2个一组组成一个batch，最终可以组成11/2 +1 =6个batch\n",
    "for idx,item in enumerate(train_dataloader):\n",
    "    print(idx,item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2.6666666666666665\n"
     ]
    }
   ],
   "source": [
    "print(8 // 3)\n",
    "print(8/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# max_steps >0；t_total\n",
    "max_steps = 6\n",
    "len_train_dataloader = 5 # 分组后的batch数目\n",
    "gradient_accumulation_steps = 2\n",
    "num_train_epochs = 20 # 需要训练的epochs\n",
    "if max_steps > 0:\n",
    "    t_total = max_steps \n",
    "    num_train_epochs = max_steps // (len_train_dataloader // gradient_accumulation_steps) + 1\n",
    "else:\n",
    "    t_total = len(train_dataloader) // gradient_accumulation_steps * num_train_epochs\n",
    "print(num_train_epochs)\n",
    "print(t_total) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "# max_steps =0\n",
    "max_steps = 0\n",
    "len_train_dataloader = 5 # 分组后的batch数目\n",
    "gradient_accumulation_steps = 2\n",
    "num_train_epochs = 20 # 需要训练的epochs\n",
    "if max_steps > 0:\n",
    "    t_total = max_steps \n",
    "    num_train_epochs = max_steps // (len_train_dataloader // gradient_accumulation_steps) + 1\n",
    "else:\n",
    "    t_total = len(train_dataloader) // gradient_accumulation_steps * num_train_epochs\n",
    "print(num_train_epochs)\n",
    "print(t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5727,  1.4850, -0.3282],\n",
      "        [ 1.3668, -0.9526,  1.1699],\n",
      "        [-1.1580,  0.3889, -1.7746],\n",
      "        [ 0.8349,  0.0372,  0.1035],\n",
      "        [-0.9505, -0.2258,  0.4844]])\n",
      "tensor([[-0.5727,  1.4850, -0.3282],\n",
      "        [ 1.3668, -0.9526,  1.1699],\n",
      "        [-1.1580,  0.3889, -1.7746],\n",
      "        [ 0.8349,  0.0372,  0.1035],\n",
      "        [-0.9505, -0.2258,  0.4844]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# is_sparse\n",
    "A = torch.randn(5,3)\n",
    "print(A)\n",
    "print(A.data)\n",
    "A.data.is_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.8500, 1.9333])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# addcdiv_ 测试\n",
    "A = torch.Tensor([5,2])\n",
    "step_size = 0.1\n",
    "exp_avg = torch.Tensor([3,2])\n",
    "denom = torch.Tensor([2,3])\n",
    "A.data.addcdiv(-step_size,exp_avg,denom) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 10/10 [00:00<00:00, 7017.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# tqdm 进度条trang测试\n",
    "from tqdm import tqdm, trange \n",
    "num_train_epochs = 10\n",
    "train_iterator = trange(int(num_train_epochs),desc=\"Epoch\")\n",
    "for item in train_iterator:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
