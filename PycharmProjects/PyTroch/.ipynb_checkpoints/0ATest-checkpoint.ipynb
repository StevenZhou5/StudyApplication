{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2, 1])\n",
      "tensor([[[0.7809],\n",
      "         [0.2537]],\n",
      "\n",
      "        [[0.9649],\n",
      "         [0.1673]],\n",
      "\n",
      "        [[0.8156],\n",
      "         [0.7914]],\n",
      "\n",
      "        [[0.3099],\n",
      "         [0.2186]],\n",
      "\n",
      "        [[0.1139],\n",
      "         [0.0593]]])\n",
      "torch.Size([5, 1, 2])\n",
      "tensor([[[0.7809, 0.2537]],\n",
      "\n",
      "        [[0.9649, 0.1673]],\n",
      "\n",
      "        [[0.8156, 0.7914]],\n",
      "\n",
      "        [[0.3099, 0.2186]],\n",
      "\n",
      "        [[0.1139, 0.0593]]])\n",
      "torch.Size([5, 2, 2])\n",
      "tensor([[[0.7809, 0.2537],\n",
      "         [0.7809, 0.2537]],\n",
      "\n",
      "        [[0.9649, 0.1673],\n",
      "         [0.9649, 0.1673]],\n",
      "\n",
      "        [[0.8156, 0.7914],\n",
      "         [0.8156, 0.7914]],\n",
      "\n",
      "        [[0.3099, 0.2186],\n",
      "         [0.3099, 0.2186]],\n",
      "\n",
      "        [[0.1139, 0.0593],\n",
      "         [0.1139, 0.0593]]])\n"
     ]
    }
   ],
   "source": [
    "# 测试expand方法的作用\n",
    "A = torch.rand(5,2,1)\n",
    "print(A.shape)\n",
    "print(A)\n",
    "B = A.permute(0,2,1)\n",
    "print(B.shape)\n",
    "print(B)\n",
    "C = B.expand(-1,2,-1) # 扩展就是把哪个维度上的值复制几份\n",
    "print(C.shape)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0.3635, 0.6664],\n",
      "        [0.9040, 0.8429],\n",
      "        [0.2058, 0.5958]]), tensor([[0.4231, 0.3937],\n",
      "        [0.9176, 0.7017],\n",
      "        [0.6273, 0.0342]])]\n",
      "torch.Size([3, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3635, 0.4231],\n",
       "         [0.6664, 0.3937]],\n",
       "\n",
       "        [[0.9040, 0.9176],\n",
       "         [0.8429, 0.7017]],\n",
       "\n",
       "        [[0.2058, 0.6273],\n",
       "         [0.5958, 0.0342]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stack操作测试\n",
    "A = [torch.rand(3,2),torch.rand(3,2)]\n",
    "print(A)\n",
    "stackA = torch.stack(A,-1)\n",
    "print(stackA.shape)\n",
    "stackA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "tensor([[2., 5.],\n",
      "        [3., 5.]])\n",
      "tensor([[0.0474, 0.9526],\n",
      "        [0.1192, 0.8808]])\n"
     ]
    }
   ],
   "source": [
    "# F的softmax的作用\n",
    "A = torch.tensor([[2.,5.],\n",
    "                 [3.,5.]])\n",
    "print(A.shape)\n",
    "print(A)\n",
    "print(F.softmax(A,dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2, 3])\n",
      "tensor([[[0.5565, 0.5760, 0.4266],\n",
      "         [0.9779, 0.6552, 0.6916]],\n",
      "\n",
      "        [[0.0872, 0.1673, 0.2115],\n",
      "         [0.7215, 0.2807, 0.1919]],\n",
      "\n",
      "        [[0.3809, 0.5997, 0.8505],\n",
      "         [0.6466, 0.4969, 0.8279]],\n",
      "\n",
      "        [[0.9324, 0.3479, 0.8052],\n",
      "         [0.1896, 0.7805, 0.2646]],\n",
      "\n",
      "        [[0.0463, 0.3359, 0.5810],\n",
      "         [0.5031, 0.5021, 0.3640]]])\n",
      "torch.return_types.max(\n",
      "values=tensor([[0.5760, 0.9779],\n",
      "        [0.2115, 0.7215],\n",
      "        [0.8505, 0.8279],\n",
      "        [0.9324, 0.7805],\n",
      "        [0.5810, 0.5031]]),\n",
      "indices=tensor([[1, 0],\n",
      "        [2, 0],\n",
      "        [2, 2],\n",
      "        [0, 1],\n",
      "        [2, 0]]))\n",
      "torch.Size([5, 2])\n",
      "tensor([[0.5760, 0.9779],\n",
      "        [0.2115, 0.7215],\n",
      "        [0.8505, 0.8279],\n",
      "        [0.9324, 0.7805],\n",
      "        [0.5810, 0.5031]])\n"
     ]
    }
   ],
   "source": [
    "#torch.max的测试\n",
    "A = torch.rand(5,2,3)\n",
    "print(A.shape)\n",
    "print(A)\n",
    "maxA2 = torch.max(A,dim=2)\n",
    "# print(maxA2.shape)\n",
    "print(maxA2)\n",
    "print(maxA2[0].shape)\n",
    "print(maxA2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# power操作\n",
    "np.power(4,0.5) #  4的0.5次方，在基于负采样的的取词比例中可以用到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0295, 0.2603, 0.8781, 0.3195],\n",
      "         [0.6234, 0.3218, 0.2369, 0.1595]]])\n",
      "tensor([[[0.8979, 0.4294, 0.6101],\n",
      "         [0.6974, 0.4764, 0.1956],\n",
      "         [0.4635, 0.3978, 0.5712],\n",
      "         [0.0248, 0.5652, 0.2259]]])\n",
      "tensor([[[0.7361, 0.5180, 0.6232],\n",
      "         [0.9799, 0.0461, 0.6374],\n",
      "         [0.2546, 0.6357, 0.1136],\n",
      "         [0.1184, 0.0487, 0.7997]]])\n",
      "tensor([[[0.4151, 0.9929, 0.2187],\n",
      "         [0.0835, 0.5420, 0.9282],\n",
      "         [0.5881, 0.4215, 0.6292],\n",
      "         [0.1119, 0.4513, 0.1579]]])\n",
      "\n",
      "Q的shape torch.Size([1, 2, 3])\n",
      "tensor([[[0.6230, 0.6666, 0.6427],\n",
      "         [0.8979, 0.6053, 0.6146]]])\n",
      "K的shape torch.Size([1, 2, 3])\n",
      "tensor([[[0.5383, 0.6011, 0.5396],\n",
      "         [0.8534, 0.4961, 0.7480]]])\n",
      "V的shape torch.Size([1, 2, 3])\n",
      "tensor([[[0.5861, 0.6848, 0.8511],\n",
      "         [0.4428, 0.9652, 0.6092]]])\n",
      "attn1的shape: torch.Size([1, 2, 2])\n",
      "tensor([[[1.0828, 1.3431],\n",
      "         [1.1788, 1.5263]]])\n",
      "attn2的shape: torch.Size([1, 2, 2])\n",
      "tensor([[[0.1353, 0.1679],\n",
      "         [0.1473, 0.1908]]])\n",
      "attn3的shape: torch.Size([1, 2, 2])\n",
      "tensor([[[0.4919, 0.5081],\n",
      "         [0.4891, 0.5109]]])\n",
      "tensor([[[0.5133, 0.8273, 0.7282],\n",
      "         [0.5129, 0.8280, 0.7275]]])\n"
     ]
    }
   ],
   "source": [
    "# Self Attention的Q,K,V 向量化矩阵计算\n",
    "X = torch.rand(1,2,4)\n",
    "print(X)\n",
    "Wq = torch.rand(1,4,3)\n",
    "print(Wq)\n",
    "Wk = torch.rand(1,4,3)\n",
    "print(Wk)\n",
    "Wv = torch.rand(1,4,3)\n",
    "print(Wv)\n",
    "\n",
    "print()\n",
    "Q = torch.bmm(X,Wq)\n",
    "print(\"Q的shape\",Q.shape)\n",
    "print(Q)\n",
    "K = torch.bmm(X,Wk)\n",
    "print(\"K的shape\",K.shape)\n",
    "print(K)\n",
    "V = torch.bmm(X,Wv)\n",
    "print(\"V的shape\",V.shape)\n",
    "print(V)\n",
    "\n",
    "attn1 = torch.bmm(Q,K.transpose(2,1))\n",
    "print(\"attn1的shape:\",attn1.shape)\n",
    "print(attn1)\n",
    "\n",
    "attn2 = attn1 / 8.0\n",
    "print(\"attn2的shape:\",attn2.shape)\n",
    "print(attn2)\n",
    "\n",
    "attn3 = torch.nn.Softmax(dim=2)(attn2)\n",
    "print(\"attn3的shape:\",attn3.shape)\n",
    "print(attn3)\n",
    "\n",
    "output = torch.bmm(attn3,V)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5647, 0.1576, 0.6414],\n",
      "        [0.3598, 0.8444, 0.0225],\n",
      "        [0.6837, 0.0784, 0.2989],\n",
      "        [0.4277, 0.8252, 0.0287],\n",
      "        [0.2266, 0.9268, 0.3346]])\n",
      "13\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# argmax()测试\n",
    "A = torch.rand(5,3)\n",
    "print(A)\n",
    "print(A.argmax().item()) # 最大的那个值在矩阵被打平为一维List时中的索引位置\n",
    "print(A[0].argmax().item()) # 最大的那个值在列表中的索引位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
