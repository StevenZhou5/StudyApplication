{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tud\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "import pandas as pd \n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "# 设置随机数的seed，这样保证每次测试的数据一致\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 语言模型\n",
    "1、学习语言模型，以及如何训练一个语言模型\n",
    "2、学习torchtext的基本使用方法\n",
    "    2-1、构建vocabulary\n",
    "    2-2、word to inde 和 index to word\n",
    "3、学习torch.nn的一些基本模型\n",
    "    3-1、Linear\n",
    "    3-2、RNN\n",
    "    3-3、LSTM\n",
    "    3-4、GRU\n",
    "4、RNN的训练技巧\n",
    "    4-1、Gradient Clipping # 防止梯度爆炸的限制操作\n",
    "5、如何保存和读取模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数hyper paramerters \n",
    "BATCH_SIZE = 32\n",
    "EMBEDDING_SIZE = 100\n",
    "HIDDEN_SIZE = 100\n",
    "MAX_VOCAB_SIZE = 50000\n",
    "NUM_EPOCHS = 2\n",
    "LEARNING_RATE = 0.001\n",
    "GRAD_CLIP = 5.0 # 为了防止梯度爆炸，设置的权重上下限的绝对值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用torchtext来进行数据预处理\n",
    "1、我们继续使用text8作为训练，验证，测试数据\n",
    "2、TorchText的一个重要概念是Field，它决定了你的数据会如何被处理。我们使用TEXT这个field来处理文本数据。\n",
    "   我们的TEXT field有lower=True这个参数，所以所有的单词都会被lowercase(小写)\n",
    "3、torchtext提供了LanguageModelingDataset这个class来帮助我们处理语言模型数据集。\n",
    "4、build_vocab可以根据我们提供的训练数据集来创建最高频单词的单词表，max_size帮助我们限定单词总量\n",
    "5、BPTTIterator可以连续地得到连贯的句子，BPTT的全过程是back propagation through time。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = torchtext.data.Field(lower=True) #lower 意味着全部都小写\n",
    "train,val,test = torchtext.datasets.LanguageModelingDataset.splits(path=\"/Users/zhenwuzhou/.keras/datasets/text8/\",\n",
    "                                                 train=\"text8.train.txt\",\n",
    "                                                 validation=\"text8.dev.txt\",\n",
    "                                                 test=\"text8.test.txt\",text_field=TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用torchtext的Field来构建单词表\n",
    "TEXT.build_vocab(train,max_size=MAX_VOCAB_SIZE)\n",
    "VOCAB_SIZE = len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50002"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', 'the', 'of', 'and', 'one', 'in', 'a', 'to', 'zero']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用itos代表的是index to String；来查看前10个值\n",
    "TEXT.vocab.itos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stoi 表示String to index；通过单词来查它的对应索引位置\n",
    "TEXT.vocab.stoi[\"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义要使用的device\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "#Back Propagation Through Time（常简称为 BPTT，可译为“时序反向传播算法”）\n",
    "#bptt_len表示取字符串的长度，repeat表示不重复取字符串\n",
    "#batch_size和batch_sizes是两个不一样的参数，batch_size是三个集合采用一样的batch_size,\n",
    "#而batch_sizes接收的是长度为3的list:[32,16,128]:分别代表训练集，验证集，测试集上的batch_size\n",
    "train_iter,val_iter,test_iter = torchtext.data.BPTTIterator.splits(\n",
    "    (train,val,test),batch_size=BATCH_SIZE,device=device,\n",
    "    bptt_len=50,repeat=False,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9566"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在执行这个之前一定要调用：TEXT.build_vocab(train,max_size=MAX_VOCAB_SIZE)\n",
    "# 不然是无法进行next(it)\n",
    "it = iter(train_iter)\n",
    "batch = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 32]\n",
       "\t[.text]:[torch.LongTensor of size 50x32]\n",
       "\t[.target]:[torch.LongTensor of size 50x32]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch #  batch由两个部分组成，一部分是text，一部分是target；50是句子的长度，32是batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 32])\n",
      "tensor([[4815,   50,    6,  ..., 9116,   33,    7],\n",
      "        [3143, 2748,  495,  ...,  893,  277,  317],\n",
      "        [  13,    8,  850,  ...,  664,  824, 1602],\n",
      "        ...,\n",
      "        [   8,   34,  522,  ..., 5237,    3,   12],\n",
      "        [3628, 1266,  968,  ...,    3,    2,    6],\n",
      "        [   2,   54,   78,  ...,   12,  185, 3027]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 4815,  3143,    13,     7,   196,     3,  3017,    48,    61,   157,\n",
       "          129,   743,   463, 10546,   135,     2, 25882,     3,     2,   110,\n",
       "          835,     4,     2, 16433,     0,     3,     2,   154,   835,  3500,\n",
       "            2,   196,    12,   188,    61,     6,     7, 10669,   215,     8,\n",
       "         1334,   104,   439,    21,    61,  2773,   357,     8,  3628,     2])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(batch.text.shape) # batch.text是一个有50个词组成的句子，32是batch_size，代表每个batch有32个句子\n",
    "print(batch.text) # 这里batch里面的文本数据\n",
    "batch.text[:,0] # 这个是吧第一个句子取出来，即为第一列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 32])\n",
      "tensor([[3143, 2748,  495,  ...,  893,  277,  317],\n",
      "        [  13,    8,  850,  ...,  664,  824, 1602],\n",
      "        [   7,  328,   62,  ..., 9289,  231, 1367],\n",
      "        ...,\n",
      "        [3628, 1266,  968,  ...,    3,    2,    6],\n",
      "        [   2,   54,   78,  ...,   12,  185, 3027],\n",
      "        [ 711,    3,  620,  ...,    7,   16,   30]])\n"
     ]
    }
   ],
   "source": [
    "print(batch.target.shape) # batch.target是一个有50个词组成的句子，32是batch_size\n",
    "print(batch.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans <unk> of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the\n",
      "\n",
      "originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans <unk> of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization\n"
     ]
    }
   ],
   "source": [
    "# 把句子打印出来itos[i] 是吧index转换成句子\n",
    "# 打印出来的text\n",
    "print(\" \".join(TEXT.vocab.itos[i] for i in batch.text[:,0].data.cpu()))\n",
    "print()\n",
    "# 打印出来的target\n",
    "print(\" \".join(TEXT.vocab.itos[i] for i in batch.target[:,0].data.cpu()))\n",
    "\n",
    "# 从打印结果可以看出，每个target和text差了一个单词，说明我们希望每一次预测的就是下一个句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philosophy is the belief that rulers are unnecessary and should be abolished although there are differing\n",
      "\n",
      "of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philosophy is the belief that rulers are unnecessary and should be abolished although there are differing interpretations\n",
      "1\n",
      "interpretations of what this means anarchism also refers to related social movements that advocate the elimination of authoritarian institutions particularly the state the word anarchy as most anarchists use it does not imply chaos nihilism or <unk> but rather a harmonious anti authoritarian society in place of what are regarded\n",
      "\n",
      "of what this means anarchism also refers to related social movements that advocate the elimination of authoritarian institutions particularly the state the word anarchy as most anarchists use it does not imply chaos nihilism or <unk> but rather a harmonious anti authoritarian society in place of what are regarded as\n",
      "2\n",
      "as authoritarian political structures and coercive economic institutions anarchists advocate social relations based upon voluntary association of autonomous individuals mutual aid and self governance while anarchism is most easily defined by what it is against anarchists also offer positive visions of what they believe to be a truly free society\n",
      "\n",
      "authoritarian political structures and coercive economic institutions anarchists advocate social relations based upon voluntary association of autonomous individuals mutual aid and self governance while anarchism is most easily defined by what it is against anarchists also offer positive visions of what they believe to be a truly free society however\n",
      "3\n",
      "however ideas about how an anarchist society might work vary considerably especially with respect to economics there is also disagreement about how a free society might be brought about origins and predecessors kropotkin and others argue that before recorded history human society was organized on anarchist principles most anthropologists follow\n",
      "\n",
      "ideas about how an anarchist society might work vary considerably especially with respect to economics there is also disagreement about how a free society might be brought about origins and predecessors kropotkin and others argue that before recorded history human society was organized on anarchist principles most anthropologists follow kropotkin\n",
      "4\n",
      "kropotkin and engels in believing that hunter gatherer bands were egalitarian and lacked division of labour accumulated wealth or decreed law and had equal access to resources william godwin anarchists including the the anarchy organisation and rothbard find anarchist attitudes in taoism from ancient china kropotkin found similar ideas in\n",
      "\n",
      "and engels in believing that hunter gatherer bands were egalitarian and lacked division of labour accumulated wealth or decreed law and had equal access to resources william godwin anarchists including the the anarchy organisation and rothbard find anarchist attitudes in taoism from ancient china kropotkin found similar ideas in stoic\n"
     ]
    }
   ],
   "source": [
    "# 我们继续迭代5次，看看后面5个有什么规律\n",
    "for i in range(5):\n",
    "    batch = next(it)\n",
    "    print(i)\n",
    "    print(\" \".join(TEXT.vocab.itos[i] for i in batch.text[:,0].data.cpu()))\n",
    "    print()\n",
    "    print(\" \".join(TEXT.vocab.itos[i] for i in batch.target[:,0].data.cpu()))\n",
    "# 我们发现每一个batch的第一句话是连续的：interpretations - as - however- ropotkin- stoic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theories of how atoms combine which explains how atoms first combine in pairs and then group into trios of pairs which are the smallest visible units of matter this parallels with the structure of modern atomic theory in which pairs or triplets of supposedly fundamental quarks combine to create most\n",
      "\n",
      "of how atoms combine which explains how atoms first combine in pairs and then group into trios of pairs which are the smallest visible units of matter this parallels with the structure of modern atomic theory in which pairs or triplets of supposedly fundamental quarks combine to create most typical\n"
     ]
    }
   ],
   "source": [
    "# 我们再来看看batch中的第二个句子，发现第二个句子和第一个句子并没有什么关系\n",
    "print(\" \".join(TEXT.vocab.itos[i] for i in batch.text[:,1].data.cpu()))\n",
    "print()\n",
    "# 打印出来的target\n",
    "print(\" \".join(TEXT.vocab.itos[i] for i in batch.target[:,1].data.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意我们的TEXT.build_vocab的单词表是50002个单词，自动赠送了两个特殊token:\n",
    "# <unk> 表示未知的单词，<pad>表示padding\n",
    "# 模型的输入是一串文字，模型的输出也是一串文字，他们之间相差一个位置，\n",
    "# 因为语言模型的目标是根据之前的单词预测下一个单词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义模型\n",
    "1、继承nn.Module\n",
    "2、初始化函数\n",
    "3、forward函数\n",
    "4、其余可以根据模型需要定义相关的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_size,hidden_size):\n",
    "        super(RNNModel,self).__init__()\n",
    "        # 首先还是需要把单词的one-hot进行embedding\n",
    "        self.embed = nn.Embedding(vocab_size,embed_size)\n",
    "        # 定义循环神经层，其中循环神经单元为LSTM，它接收两个参数，一个是单词embedding后的size，\n",
    "        # 另外一个是隐藏层的size\n",
    "        # 可以使用batch_first=True把batch_size变成第一个维度，因为默认第一个维度是句子中单词的数量seq_length\n",
    "        self.lstm = nn.LSTM(embed_size,hidden_size)\n",
    "        # 最受的输出层还是要转为one-hot的维度来对应具体的输出单词；（\n",
    "        # 这就是所谓的输出层词嵌入矩阵，但是一般词嵌入矩阵采用的都是输入层的词嵌入矩阵）\n",
    "        self.linear = nn.Linear(hidden_size,vocab_size)\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "    def forward(self,text,hidden):\n",
    "        # forward pass\n",
    "        # text:[seq_length * batch_size] ： 注意每一列是一个句子\n",
    "        emb = self.embed(text)# [seq_length*batch_size*embed_size]\n",
    "        # lstm会把最终的每一个预测的输出和hidden\n",
    "        output , hidden = self.lstm(emb,hidden) \n",
    "        # output: [seq_length * batch_size * hidden_size]\n",
    "        # 这列的num_layers是为1的\n",
    "        # hidden: [num_layers*batch_size*hidden_size],[num_layers * batch_size * hidden_size]\n",
    "        # 这里要做下reshape操作，因为后面的全连接层是不支持三维数据的\n",
    "        # 把最后的hidden_size的维度保留，前面两个维度合并\n",
    "        output_reshape = output.view(-1,output.shape[2])#[(seq_length * batch_size)*hidden_size]\n",
    "        \n",
    "        # 最后的输出每一个位置预测的单词\n",
    "        out_vocab = self.linear(output_reshape) # [(seq_length * batch_size)*vocab_size]\n",
    "        # 最后要reshape成三维的，对应的应该是和output的前两个维度是一样的，最后一个维度是one-hot词向量的维度\n",
    "        # [seq_length * batch_size * vocab_size]\n",
    "        out_vocab_reshape = out_vocab.view(output.size(0),output.size(1),out_vocab.size(-1))\n",
    "        \n",
    "        return out_vocab_reshape,hidden\n",
    "    \n",
    "    # 自定义初始化hidden的权重矩阵\n",
    "    def init_hidden(self,batch_size,requires_grad=True):\n",
    "        weight = next(self.parameters()) # 这里先从模型中把所有的参数都取出来\n",
    "        # 这里要返回两个全0的\n",
    "        return (weight.new_zeros((1,batch_size,self.hidden_size),requires_grad=requires_grad),\n",
    "                weight.new_zeros((1,batch_size,self.hidden_size),requires_grad=requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  初始化一个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(vocab_size=VOCAB_SIZE,\n",
    "                 embed_size=EMBEDDING_SIZE,\n",
    "                 hidden_size=HIDDEN_SIZE)\n",
    "if USE_CUDA:\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.6914, -0.1955, -0.0534,  ...,  0.1854, -0.9802,  1.0892],\n",
       "        [-1.8782, -0.2626, -0.4415,  ...,  0.3620,  2.5087,  0.5897],\n",
       "        [-0.6326,  0.3006, -0.2671,  ..., -0.6473,  0.4356, -0.4093],\n",
       "        ...,\n",
       "        [-1.2651,  1.2818, -0.5194,  ..., -0.3482, -0.8409,  0.2882],\n",
       "        [-1.9637,  1.2868,  0.4306,  ..., -0.6555, -1.0185,  1.4404],\n",
       "        [ 0.7882, -2.0590,  2.1683,  ...,  0.7193,  1.1773, -0.0051]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义loss_fn和optimizer\n",
    "loss_fn = nn.CrossEntropyLoss() # softmax多分类问题的损失函数\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "\n",
    "# 定义一个scheduler 来实现模型的learning——rate的动态下降\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,0.5) #0.5表示learning_rate降一半"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型：\n",
    "1、模型需要若干个epoch\n",
    "2、每个epoch我们都把所有的的数据分成若干个batch\n",
    "3、把每个batch的输入和输出都包装成cuda tensor\n",
    "4、forward pass,通过输入的句子预测每个单词的下一个单词\n",
    "5、用模型的预测和正确的下一个单词计算cross entropy loss\n",
    "6、清空模型当前的gradient\n",
    "7、backward pass\n",
    "8、gradient clipping，防止梯度爆炸\n",
    "9、更新模型参数\n",
    "10、每隔一定的iteration输出模型在当前iteration的loss，以及在验证集上做模型的评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddentest = model.init_hidden(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], requires_grad=True),\n",
       " tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], requires_grad=True))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiddentest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这是为了把两个hidden的值记录下来传递给下面一个\n",
    "def repackage_hidden(h):\n",
    "    if isinstance(h,torch.Tensor):\n",
    "        return h.detach() # detach()可以把前面的h隐藏车的权重矩阵值给赋值下来，而不去管\n",
    "    else: \n",
    "        # 如果有若干个hidde，就把每个都repackage_hidden一下，我们传入的就是一个元组\n",
    "        # 所以要把元组中的每个hidden都repackage_hidden，最后再把新的元组返回回去\n",
    "        return tuple(repackage_hidden(v) for v in h) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, eva_data):\n",
    "    model.eval() # 改为预测模式\n",
    "    total_loss = 0.\n",
    "    total_count = 0.\n",
    "    it = iter(eva_data)\n",
    "    with torch.no_grad(): # 评测时不需要进行梯度下降\n",
    "       hidden = model.init_hidden(BATCH_SIZE,requires_grad=False) # 这里要把梯度下降的开关关上\n",
    "       for i,batch in enumerate(it):\n",
    "            data, target = batch.text, batch.target\n",
    "           \n",
    "            hidden = repackage_hidden(hidden)\n",
    "        \n",
    "            # 进行前向传播\n",
    "            output,hidden = model(data,hidden)\n",
    "        \n",
    "            loss_predict = output.view(-1,VOCAB_SIZE) # 预测值\n",
    "            loss_target = target.view(-1) # 真实值\n",
    "            loss = loss_fn(loss_predict,loss_target) \n",
    "            \n",
    "            # 因为loss计算的是平均值，所以要计算total_loss=loss*总数\n",
    "            total_count = np.multiply(*data.size())\n",
    "            total_loss = loss.item()*total_count\n",
    "            \n",
    "            if i == 3:\n",
    "                break\n",
    "    \n",
    "    loss = total_loss / total_count\n",
    "    model.train() # 模型评测完了之后在改为训练模式\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1600, 50002])\n",
      "torch.Size([1600])\n",
      "loss: 6.4924421310424805\n",
      "val_loss: 7.410778999328613\n",
      "best model saved to /Users/zhenwuzhou/.keras/models/text8/1m.pth\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-b0e636c4c9e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# 进行前向传播\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-78-f6119e534884>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text, hidden)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# 最后的输出每一个位置预测的单词\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mout_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_reshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [(seq_length * batch_size)*vocab_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;31m# 最后要reshape成三维的，对应的应该是和output的前两个维度是一样的，最后一个维度是one-hot词向量的维度\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# [seq_length * batch_size * vocab_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "printshape = True\n",
    "val_losses = [] # 存放loss的值 \n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    # train_iter使用torchtext通过train数据处理后的结构化数据，里面有text和target\n",
    "    it = iter(train_iter)\n",
    "    hidden = model.init_hidden(BATCH_SIZE)\n",
    "    for i,batch in enumerate(it):\n",
    "        # 先把text和target取出\n",
    "        data, target = batch.text, batch.target\n",
    "        \n",
    "        # 因为语言模型我们前面看到前面一个batch的0位置的文章是和下一个batch的0位置文章接起来的\n",
    "        # 所以我们可以吧上一个训练的hidden权重值传递给下一个batch；\n",
    "        # 这样相当于我们的hidden使用所有数据训练的，这是语言预测模型的一个特殊的地方\n",
    "        # 其他翻译模型，或者情感分类问题就不用这么做了，因为前后两个batch并没有什么关系\n",
    "        # 在每次前向传播把之前梯度下降的计算出的hidden的值取出来\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        \n",
    "        # 进行前向传播\n",
    "        output,hidden = model(data,hidden)\n",
    "        \n",
    "        \n",
    "        # 进行loss计算\n",
    "        # 这个属于N分类问题，\n",
    "        # output 预测需要输入0-N每一种的可能性百分百，\n",
    "        # optt_len 是句子的长度\n",
    "        # 所以output的shape为[(batch_size*optt_len) *VOCAB_SIZE ];\n",
    "        # target中只需要每个词数字0-N,所以shape只需要为[batch_size*optt_len]\n",
    "        loss_predict = output.view(-1,VOCAB_SIZE) # 预测值\n",
    "        loss_target = target.view(-1) # 真实值\n",
    "        loss = loss_fn(loss_predict,loss_target)\n",
    "        \n",
    "        if(printshape):\n",
    "            print(loss_predict.shape)#torch.Size([128, 4])\n",
    "            print(loss_target.shape)#torch.Size([128])\n",
    "            printshape = False\n",
    "        \n",
    "        # 把grad清零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        \n",
    "        # 为了防止梯度爆炸，我们需要在梯度下降前，对权重矩阵的值设置上下限\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),GRAD_CLIP)\n",
    "        \n",
    "        #梯度下降\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 打印loss\n",
    "        if i % 10 == 0:\n",
    "            print(\"loss:\",loss.item())\n",
    "            \n",
    "        \n",
    "        # 存储模型\n",
    "        if i % 10 == 0:\n",
    "            # 用验证数据来去对模型进行评价\n",
    "            val_loss = evaluate(model,val_iter)\n",
    "            print(\"val_loss:\",val_loss)\n",
    "            # 如果是第一次迭代到1000或者val_loss比之前的loss都要好，我们就把模型保存下来\n",
    "            if len(val_losses) == 0 or val_loss < min(val_losses):\n",
    "                # 注意这里是save的是model还是model.state_dict(),一定别忘了加()\n",
    "                torch.save(model.state_dict(),\"/Users/zhenwuzhou/.keras/models/text8/1m_dict.pth\")\n",
    "                torch.save(model,\"/Users/zhenwuzhou/.keras/models/text8/1m.pth\")\n",
    "                print(\"best model saved to /Users/zhenwuzhou/.keras/models/text8/1m.pth\")\n",
    "            else:\n",
    "                # 发现模型的loss无法下降了\n",
    "                # 我们可以调整learning_rate:\n",
    "                print(\"leatnig_rate decay\")\n",
    "                scheduler.step() # 这里用scheduler来进行把optimizer的learning_rate进行下降\n",
    "            val_losses.append(val_loss)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('embed.weight',\n",
       "              tensor([[ 1.6611, -0.2188, -0.0851,  ...,  0.1512, -1.0154,  1.0818],\n",
       "                      [-1.8782, -0.2626, -0.4415,  ...,  0.3620,  2.5087,  0.5897],\n",
       "                      [-0.6337,  0.2750, -0.2720,  ..., -0.6772,  0.4118, -0.3973],\n",
       "                      ...,\n",
       "                      [-1.2651,  1.2818, -0.5194,  ..., -0.3482, -0.8409,  0.2882],\n",
       "                      [-1.9637,  1.2868,  0.4306,  ..., -0.6555, -1.0185,  1.4404],\n",
       "                      [ 0.7882, -2.0590,  2.1683,  ...,  0.7193,  1.1773, -0.0051]])),\n",
       "             ('lstm.weight_ih_l0',\n",
       "              tensor([[ 0.0121,  0.0105,  0.0924,  ...,  0.0238, -0.0003, -0.0184],\n",
       "                      [ 0.0208,  0.0203, -0.0756,  ...,  0.0249,  0.0014, -0.0071],\n",
       "                      [ 0.0675, -0.0209, -0.0174,  ..., -0.0327, -0.0212, -0.0648],\n",
       "                      ...,\n",
       "                      [-0.0174,  0.0620, -0.1052,  ..., -0.0327, -0.0500, -0.0674],\n",
       "                      [ 0.0380, -0.0515, -0.0777,  ..., -0.0546, -0.0297, -0.0405],\n",
       "                      [-0.0737,  0.1109,  0.0091,  ..., -0.0338,  0.0713,  0.0049]])),\n",
       "             ('lstm.weight_hh_l0',\n",
       "              tensor([[-0.0137, -0.1134, -0.0960,  ...,  0.0697,  0.0582,  0.0373],\n",
       "                      [-0.0253,  0.0283,  0.0103,  ...,  0.1042,  0.0756,  0.0400],\n",
       "                      [ 0.0066, -0.0261,  0.0366,  ...,  0.0178,  0.0368, -0.0451],\n",
       "                      ...,\n",
       "                      [-0.1108, -0.0188, -0.0250,  ...,  0.1293, -0.0288,  0.0322],\n",
       "                      [-0.0752, -0.0772, -0.1138,  ...,  0.0633,  0.0484,  0.1171],\n",
       "                      [ 0.0375,  0.0595, -0.0360,  ..., -0.0468, -0.0272,  0.0599]])),\n",
       "             ('lstm.bias_ih_l0',\n",
       "              tensor([-0.0345,  0.1093,  0.0022,  0.0025,  0.1234,  0.1148,  0.0473,  0.0809,\n",
       "                       0.1248,  0.0493,  0.0845, -0.0228,  0.1108,  0.1353, -0.0251,  0.0852,\n",
       "                       0.1178,  0.0461,  0.0936, -0.0155,  0.0922,  0.0636,  0.0184,  0.1318,\n",
       "                       0.0427,  0.1148,  0.0159,  0.1098, -0.0386,  0.1097,  0.0622, -0.0272,\n",
       "                       0.1360,  0.0746, -0.0600,  0.0224,  0.0177, -0.0528,  0.0643,  0.0123,\n",
       "                       0.1015,  0.0376, -0.0037,  0.0674,  0.0196,  0.0091,  0.1282, -0.0103,\n",
       "                       0.0669,  0.0460, -0.0032,  0.0952,  0.0917, -0.0253, -0.0054,  0.0162,\n",
       "                       0.0091,  0.0840,  0.0769,  0.0102, -0.0429,  0.0693,  0.1162,  0.1209,\n",
       "                       0.0508, -0.0039, -0.0726,  0.0549,  0.0772,  0.1322,  0.0013,  0.0475,\n",
       "                      -0.0234,  0.1113,  0.1063,  0.0995, -0.0101,  0.0740, -0.0764, -0.0528,\n",
       "                       0.0646,  0.1294,  0.0207,  0.0397, -0.0346, -0.0501,  0.0497,  0.1308,\n",
       "                      -0.0154,  0.0040,  0.1051,  0.0551, -0.0331,  0.1245, -0.0296,  0.0717,\n",
       "                       0.0968,  0.1082, -0.0681,  0.1135,  0.1323,  0.0498,  0.0019,  0.1316,\n",
       "                      -0.0208, -0.0271, -0.0305,  0.1213,  0.0526,  0.0926,  0.0384,  0.0678,\n",
       "                       0.0691,  0.1306, -0.0106,  0.0472,  0.0247,  0.0196, -0.0128, -0.0681,\n",
       "                       0.0723,  0.0628,  0.0608,  0.0853,  0.0815,  0.0555, -0.0508,  0.0951,\n",
       "                       0.0774, -0.0339,  0.1066, -0.0056, -0.0546, -0.0379,  0.0095,  0.0581,\n",
       "                      -0.0064,  0.1081,  0.0298, -0.0463, -0.0017,  0.0951,  0.0026,  0.0823,\n",
       "                       0.0452,  0.0258,  0.0995,  0.1262,  0.0983,  0.0937,  0.0750,  0.0314,\n",
       "                       0.0817,  0.0798, -0.0633, -0.0597, -0.0506,  0.0110,  0.0756,  0.0141,\n",
       "                       0.1368, -0.0369,  0.0783, -0.0490,  0.0201, -0.0048,  0.1094, -0.0514,\n",
       "                       0.0616,  0.1204,  0.0526,  0.0006, -0.0018,  0.0639,  0.0760, -0.0429,\n",
       "                      -0.0580, -0.0219,  0.0254, -0.0189, -0.0452,  0.1104, -0.0398,  0.0996,\n",
       "                       0.0893,  0.0061,  0.0011,  0.0191,  0.1080,  0.0402,  0.1038, -0.0168,\n",
       "                      -0.0020,  0.0102,  0.1327, -0.0214, -0.0477,  0.0252,  0.1015,  0.1371,\n",
       "                      -0.0594, -0.0976,  0.0461, -0.0065,  0.0528, -0.1339, -0.0913, -0.0297,\n",
       "                      -0.0934, -0.1149,  0.0350,  0.0597, -0.0889, -0.0507, -0.0964,  0.0323,\n",
       "                      -0.1296,  0.0354,  0.1049,  0.0415, -0.1117, -0.0765,  0.0503,  0.0799,\n",
       "                      -0.0328,  0.1164, -0.0081,  0.0543,  0.1146, -0.0522, -0.0657, -0.1040,\n",
       "                      -0.0351,  0.0608,  0.1106,  0.1017, -0.0055, -0.1278,  0.0562, -0.1370,\n",
       "                       0.0857,  0.0287, -0.1247,  0.0482, -0.0772, -0.0717,  0.0550,  0.0872,\n",
       "                       0.0055,  0.0662, -0.1222, -0.0186, -0.1183,  0.0144, -0.0350, -0.1251,\n",
       "                      -0.0464, -0.0429, -0.0680,  0.0264, -0.0090, -0.0412, -0.1195,  0.1059,\n",
       "                      -0.1246,  0.0632,  0.1287,  0.0540, -0.0498, -0.0885,  0.1170,  0.1123,\n",
       "                       0.0412,  0.0794,  0.0764,  0.1345, -0.0957, -0.0624, -0.0043,  0.0412,\n",
       "                      -0.0087,  0.0561, -0.1298, -0.0435, -0.0257,  0.0519, -0.0254, -0.0874,\n",
       "                       0.0089,  0.1206, -0.0814,  0.0020, -0.1056,  0.0012, -0.0352, -0.1333,\n",
       "                      -0.1112,  0.1131,  0.0944,  0.1230,  0.0622,  0.0355,  0.0461,  0.0357,\n",
       "                      -0.0630,  0.0471, -0.0049,  0.0238, -0.0045, -0.0029,  0.0095,  0.0951,\n",
       "                       0.0989,  0.1010,  0.0522,  0.0089,  0.1224,  0.0337, -0.0282, -0.0030,\n",
       "                      -0.0543,  0.1245, -0.0650,  0.0104, -0.0031,  0.0612, -0.0042,  0.0916,\n",
       "                      -0.0105, -0.0573, -0.0442, -0.0565,  0.1191,  0.0978, -0.0544, -0.0187,\n",
       "                      -0.0023,  0.0531, -0.0228, -0.0339, -0.0248,  0.0220,  0.0010, -0.0331,\n",
       "                      -0.0333, -0.0450, -0.0415,  0.0754, -0.0858,  0.0021,  0.0907,  0.0945,\n",
       "                      -0.0478, -0.0427, -0.0398, -0.0325, -0.0057,  0.0189,  0.0693,  0.0327,\n",
       "                       0.0220,  0.0461, -0.0737,  0.0145, -0.0272, -0.0338, -0.0007, -0.0249,\n",
       "                      -0.0379,  0.0185,  0.0477,  0.0670, -0.0276,  0.0937, -0.0439,  0.1162,\n",
       "                      -0.0586,  0.0537,  0.0798,  0.0782, -0.0419,  0.0523,  0.0488,  0.0396,\n",
       "                      -0.0539, -0.0253, -0.0667,  0.0875,  0.0270,  0.0126,  0.0593,  0.0083,\n",
       "                       0.0172,  0.0758,  0.0112,  0.0491, -0.0544,  0.0293,  0.0081,  0.0541])),\n",
       "             ('lstm.bias_hh_l0',\n",
       "              tensor([ 0.0311,  0.0988,  0.1017,  0.0249,  0.0481,  0.1246,  0.1263,  0.0778,\n",
       "                       0.0640,  0.0368,  0.0653, -0.0573,  0.0014, -0.0146,  0.0649, -0.0321,\n",
       "                       0.0846,  0.0156,  0.0558,  0.0678,  0.1046,  0.1154, -0.0761, -0.0543,\n",
       "                      -0.0606,  0.0110,  0.0973,  0.0548,  0.0917, -0.0009,  0.0984,  0.0100,\n",
       "                       0.1260, -0.0690, -0.0490,  0.0735, -0.0811, -0.0075,  0.0275, -0.0395,\n",
       "                       0.0769,  0.0432, -0.0196, -0.0395, -0.0714,  0.0799,  0.0321,  0.0111,\n",
       "                       0.0889, -0.0050,  0.0574, -0.0395,  0.0523, -0.0410,  0.0244,  0.0685,\n",
       "                       0.0573,  0.1290, -0.0375,  0.1169, -0.0493,  0.0238,  0.0358,  0.1129,\n",
       "                       0.1254,  0.0528,  0.0634,  0.1249,  0.0756, -0.0340, -0.0483,  0.0043,\n",
       "                       0.0593,  0.1125, -0.0572,  0.0291, -0.0247, -0.0569,  0.0935,  0.0718,\n",
       "                       0.0343,  0.0810, -0.0363,  0.0465,  0.0219, -0.0617,  0.0474,  0.0794,\n",
       "                       0.0248,  0.0993,  0.0365,  0.0363,  0.0881, -0.0158, -0.0232, -0.0307,\n",
       "                       0.0918, -0.0123,  0.0091,  0.1317,  0.0139,  0.0423,  0.0902,  0.1149,\n",
       "                       0.1310, -0.0031,  0.0227,  0.0721, -0.0020, -0.0551, -0.0679,  0.1099,\n",
       "                       0.0875,  0.0218,  0.0220,  0.1155, -0.0452,  0.0869, -0.0345, -0.0669,\n",
       "                       0.0130,  0.0045,  0.0825, -0.0185,  0.0579,  0.1285,  0.0782, -0.0775,\n",
       "                      -0.0402,  0.1197, -0.0675,  0.1354,  0.1349,  0.1167,  0.0032,  0.1062,\n",
       "                       0.0508,  0.0054, -0.0082,  0.0337, -0.0469, -0.0129, -0.0416,  0.1024,\n",
       "                      -0.0238,  0.0101,  0.1276,  0.1109,  0.1264,  0.0950,  0.1331,  0.1065,\n",
       "                       0.1115,  0.1149, -0.0216, -0.0153, -0.0019,  0.0267,  0.0924,  0.1066,\n",
       "                       0.0996,  0.1223,  0.1207, -0.0556,  0.0338,  0.0703,  0.0674, -0.0097,\n",
       "                       0.0341,  0.0274,  0.0890, -0.0439,  0.0811,  0.1142,  0.0411,  0.0238,\n",
       "                       0.1263, -0.0084,  0.0224, -0.0161, -0.0228,  0.0436,  0.0137, -0.0095,\n",
       "                       0.1106,  0.0723, -0.0270,  0.1030, -0.0386, -0.0622,  0.0749,  0.0870,\n",
       "                      -0.0238, -0.0078,  0.0832,  0.0116, -0.0138, -0.0248, -0.0027,  0.1339,\n",
       "                      -0.0486, -0.0131, -0.0058, -0.0174,  0.0148, -0.0878, -0.0683,  0.1336,\n",
       "                      -0.0364, -0.0863, -0.0750,  0.0649, -0.1116, -0.1216, -0.0964,  0.0810,\n",
       "                       0.0538, -0.0912,  0.0514, -0.0666, -0.0097,  0.0571, -0.0733,  0.0607,\n",
       "                       0.0463,  0.0431, -0.0343,  0.0176,  0.1115, -0.0660,  0.0917, -0.0244,\n",
       "                      -0.0198,  0.0397,  0.0376,  0.0503, -0.0527,  0.0492, -0.1238,  0.0490,\n",
       "                       0.0658, -0.0877, -0.0242,  0.0771,  0.0562,  0.0285,  0.0066, -0.0309,\n",
       "                      -0.0337, -0.0584, -0.0878, -0.0392, -0.0566,  0.1187,  0.0121, -0.0501,\n",
       "                       0.1145, -0.1223,  0.0847,  0.1252, -0.1340,  0.0235, -0.0908,  0.1317,\n",
       "                      -0.0837, -0.0085,  0.0288, -0.1214, -0.1246, -0.0557,  0.0412,  0.0277,\n",
       "                      -0.0818,  0.0350, -0.0309,  0.0483,  0.0180, -0.0140,  0.0157,  0.1175,\n",
       "                       0.0815,  0.0908, -0.0036, -0.1107, -0.0798,  0.0503,  0.0474, -0.1181,\n",
       "                      -0.0305,  0.0116, -0.0137, -0.1229, -0.0007, -0.0468, -0.0171, -0.0024,\n",
       "                       0.0255, -0.0414,  0.0121, -0.0632,  0.0077,  0.0603,  0.0776,  0.0971,\n",
       "                      -0.0296,  0.0114, -0.0258,  0.1040, -0.0608, -0.0503, -0.0677,  0.0303,\n",
       "                       0.0609, -0.0504,  0.1390, -0.0201,  0.0791,  0.0658, -0.0046,  0.0463,\n",
       "                      -0.0240, -0.0166, -0.1002,  0.0737, -0.0254,  0.1112,  0.0814,  0.0917,\n",
       "                       0.0241,  0.0492,  0.0127,  0.0050,  0.0656,  0.0492, -0.0156,  0.0592,\n",
       "                      -0.0003,  0.0789,  0.0843,  0.0745,  0.0161, -0.0546,  0.0776,  0.0935,\n",
       "                      -0.0454, -0.0406,  0.0590,  0.0651,  0.0380, -0.0007,  0.1239,  0.0106,\n",
       "                       0.0545, -0.0089,  0.0344,  0.1213,  0.0566,  0.1167, -0.0381, -0.0426,\n",
       "                       0.0425,  0.0107,  0.0498,  0.0824,  0.0387,  0.1327,  0.1041, -0.0481,\n",
       "                      -0.0108,  0.0002,  0.0119,  0.1051,  0.0335, -0.0243,  0.1110,  0.0961,\n",
       "                      -0.0053,  0.0134, -0.0057,  0.0694,  0.0853, -0.0230,  0.0982,  0.0253,\n",
       "                       0.0588,  0.0948,  0.1012,  0.1300,  0.0934,  0.0290, -0.0067,  0.0410,\n",
       "                       0.0410, -0.0119,  0.0248,  0.1320, -0.0832,  0.1091,  0.0851,  0.0137])),\n",
       "             ('linear.weight',\n",
       "              tensor([[-0.0682, -0.0412, -0.1063,  ...,  0.1494, -0.0040, -0.0307],\n",
       "                      [ 0.1584, -0.0127,  0.0795,  ..., -0.0898, -0.1593, -0.0479],\n",
       "                      [-0.0652,  0.0419, -0.0440,  ...,  0.0913,  0.0735,  0.1128],\n",
       "                      ...,\n",
       "                      [ 0.0609, -0.0090,  0.0067,  ..., -0.0159, -0.0087, -0.0011],\n",
       "                      [ 0.1328,  0.1424,  0.1385,  ...,  0.0038, -0.0994, -0.0734],\n",
       "                      [-0.0230,  0.1566,  0.1508,  ..., -0.1333, -0.0805, -0.1402]])),\n",
       "             ('linear.bias',\n",
       "              tensor([-0.0205,  0.0325, -0.0160,  ...,  0.0096, -0.0785, -0.0679]))])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()\n",
    "# torch.save(model.state_dict,\"/Users/zhenwuzhou/.keras/models/text8/1m.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型重新load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = RNNModel(vocab_size=VOCAB_SIZE,\n",
    "                 embed_size=EMBEDDING_SIZE,\n",
    "                 hidden_size=HIDDEN_SIZE)\n",
    "if USE_CUDA:\n",
    "    best_model = best_model.to(device)\n",
    "best_model.load_state_dict(torch.load(\"/Users/zhenwuzhou/.keras/models/text8/1m_dict.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('embed.weight',\n",
       "              tensor([[ 1.7062, -0.2245, -0.0810,  ...,  0.1508, -1.0192,  1.1120],\n",
       "                      [-1.8782, -0.2626, -0.4415,  ...,  0.3620,  2.5087,  0.5897],\n",
       "                      [-0.6297,  0.2410, -0.2347,  ..., -0.7184,  0.4643, -0.3540],\n",
       "                      ...,\n",
       "                      [-1.2651,  1.2818, -0.5194,  ..., -0.3482, -0.8409,  0.2882],\n",
       "                      [-1.9637,  1.2868,  0.4306,  ..., -0.6555, -1.0185,  1.4404],\n",
       "                      [ 0.7882, -2.0590,  2.1683,  ...,  0.7193,  1.1773, -0.0051]])),\n",
       "             ('lstm.weight_ih_l0',\n",
       "              tensor([[-0.0038,  0.0273,  0.0748,  ...,  0.0407, -0.0176, -0.0031],\n",
       "                      [ 0.0328,  0.0358, -0.0641,  ...,  0.0410, -0.0140,  0.0070],\n",
       "                      [ 0.0502, -0.0377, -0.0354,  ..., -0.0258, -0.0046, -0.0483],\n",
       "                      ...,\n",
       "                      [-0.0089,  0.0744, -0.1241,  ..., -0.0028, -0.0544, -0.0874],\n",
       "                      [ 0.0563, -0.0370, -0.0970,  ..., -0.0902,  0.0148, -0.0879],\n",
       "                      [-0.0476,  0.1498, -0.0220,  ..., -0.0266,  0.0953, -0.0149]])),\n",
       "             ('lstm.weight_hh_l0',\n",
       "              tensor([[-0.0305, -0.1302, -0.1130,  ...,  0.0864,  0.0748,  0.0544],\n",
       "                      [-0.0405,  0.0103, -0.0063,  ...,  0.1201,  0.0920,  0.0569],\n",
       "                      [-0.0082, -0.0428,  0.0200,  ...,  0.0345,  0.0524, -0.0292],\n",
       "                      ...,\n",
       "                      [-0.1284, -0.0286, -0.0262,  ...,  0.1541, -0.0057,  0.0363],\n",
       "                      [-0.0585, -0.0667, -0.1057,  ...,  0.0283,  0.0337,  0.1096],\n",
       "                      [ 0.0616,  0.0725, -0.0222,  ..., -0.0753, -0.0496,  0.0474]])),\n",
       "             ('lstm.bias_ih_l0',\n",
       "              tensor([-0.0169,  0.1266,  0.0188,  0.0209,  0.1409,  0.1314,  0.0630,  0.0988,\n",
       "                       0.1416,  0.0658,  0.1021, -0.0056,  0.1279,  0.1525, -0.0084,  0.1019,\n",
       "                       0.1349,  0.0634,  0.1105,  0.0018,  0.1094,  0.0808,  0.0434,  0.1485,\n",
       "                       0.0596,  0.1312,  0.0338,  0.1273, -0.0221,  0.1266,  0.0801, -0.0096,\n",
       "                       0.1535,  0.0926, -0.0429,  0.0390,  0.0342, -0.0363,  0.0811,  0.0292,\n",
       "                       0.1179,  0.0545,  0.0127,  0.0849,  0.0366,  0.0260,  0.1467,  0.0062,\n",
       "                       0.0844,  0.0636,  0.0133,  0.1125,  0.1082, -0.0088,  0.0120,  0.0339,\n",
       "                       0.0269,  0.1007,  0.0942,  0.0272, -0.0262,  0.0861,  0.1322,  0.1377,\n",
       "                       0.0672,  0.0135, -0.0553,  0.0717,  0.0935,  0.1489,  0.0181,  0.0641,\n",
       "                      -0.0076,  0.1280,  0.1226,  0.1164,  0.0062,  0.0911, -0.0581, -0.0363,\n",
       "                       0.0816,  0.1465,  0.0376,  0.0567, -0.0177, -0.0330,  0.0662,  0.1473,\n",
       "                       0.0009,  0.0210,  0.1223,  0.0716, -0.0161,  0.1412, -0.0124,  0.0881,\n",
       "                       0.1144,  0.1256, -0.0510,  0.1308,  0.1489,  0.0670,  0.0169,  0.1303,\n",
       "                      -0.0044, -0.0119, -0.0169,  0.1385,  0.0678,  0.1077,  0.0541,  0.0833,\n",
       "                       0.0849,  0.1458,  0.0041,  0.0627,  0.0412,  0.0361,  0.0028, -0.0503,\n",
       "                       0.0894,  0.0792,  0.0810,  0.1004,  0.0974,  0.0707, -0.0339,  0.1105,\n",
       "                       0.0923, -0.0189,  0.1245,  0.0117, -0.0422, -0.0209,  0.0253,  0.0736,\n",
       "                       0.0095,  0.1237,  0.0449, -0.0311,  0.0135,  0.1113,  0.0176,  0.0987,\n",
       "                       0.0590,  0.0416,  0.0850,  0.1408,  0.1144,  0.1112,  0.0903,  0.0484,\n",
       "                       0.0966,  0.0947, -0.0458, -0.0428, -0.0342,  0.0262,  0.0920,  0.0303,\n",
       "                       0.1521, -0.0222,  0.0931, -0.0332,  0.0351,  0.0120,  0.1252, -0.0351,\n",
       "                       0.0764,  0.1356,  0.0681,  0.0156,  0.0121,  0.0795,  0.0910, -0.0276,\n",
       "                      -0.0435, -0.0065,  0.0400, -0.0038, -0.0293,  0.1256, -0.0239,  0.1154,\n",
       "                       0.1050,  0.0224,  0.0148,  0.0342,  0.1212,  0.0563,  0.1203, -0.0017,\n",
       "                       0.0131,  0.0259,  0.1495, -0.0062, -0.0306,  0.0415,  0.1176,  0.1523,\n",
       "                      -0.0754, -0.1132,  0.0303,  0.0094,  0.0686, -0.1496, -0.1071, -0.0136,\n",
       "                      -0.1090, -0.1306,  0.0191,  0.0758, -0.1049, -0.0669, -0.1123,  0.0481,\n",
       "                      -0.1456,  0.0195,  0.1205,  0.0576, -0.1275, -0.0925,  0.0583,  0.0959,\n",
       "                      -0.0169,  0.1320,  0.0080,  0.0386,  0.1302, -0.0681, -0.0498, -0.1198,\n",
       "                      -0.0508,  0.0763,  0.1267,  0.1173, -0.0216, -0.1440,  0.0404, -0.1528,\n",
       "                       0.1014,  0.0131, -0.1405,  0.0640, -0.0933, -0.0876,  0.0707,  0.1032,\n",
       "                      -0.0104,  0.0820, -0.1381, -0.0347, -0.1340,  0.0302, -0.0510, -0.1412,\n",
       "                      -0.0306, -0.0588, -0.0520,  0.0422, -0.0250, -0.0570, -0.1353,  0.1220,\n",
       "                      -0.1406,  0.0790,  0.1447,  0.0381, -0.0654, -0.1045,  0.1331,  0.1285,\n",
       "                       0.0252,  0.0952,  0.0924,  0.1506, -0.1117, -0.0786,  0.0116,  0.0571,\n",
       "                       0.0076,  0.0725, -0.1458, -0.0595, -0.0417,  0.0680, -0.0093, -0.1030,\n",
       "                      -0.0069,  0.1366, -0.0973, -0.0137, -0.1214, -0.0144, -0.0514, -0.1492,\n",
       "                      -0.1271,  0.1294,  0.1102,  0.1386,  0.0573,  0.0332,  0.0322,  0.0566,\n",
       "                      -0.0756,  0.0445, -0.0023,  0.0032, -0.0331,  0.0149,  0.0050,  0.0843,\n",
       "                       0.1097,  0.1173,  0.0611, -0.0066,  0.1181,  0.0303, -0.0143, -0.0116,\n",
       "                      -0.0587,  0.1108, -0.0247,  0.0301, -0.0227,  0.0660, -0.0016,  0.0774,\n",
       "                      -0.0208, -0.0562, -0.0556, -0.0417,  0.1367,  0.0898, -0.0555, -0.0136,\n",
       "                      -0.0216,  0.0462, -0.0121, -0.0483, -0.0290,  0.0185,  0.0128, -0.0308,\n",
       "                      -0.0289, -0.0538, -0.0457,  0.0681, -0.0738, -0.0038,  0.1007,  0.1008,\n",
       "                      -0.0524, -0.0311, -0.0588, -0.0270, -0.0103,  0.0287,  0.0691,  0.0487,\n",
       "                       0.0279,  0.0523, -0.0686,  0.0059, -0.0190, -0.0464, -0.0032, -0.0345,\n",
       "                      -0.0255,  0.0069,  0.0461,  0.0601, -0.0501,  0.0901, -0.0397,  0.1020,\n",
       "                      -0.0409,  0.0642,  0.0957,  0.0720, -0.0341,  0.0580,  0.0521,  0.0314,\n",
       "                      -0.0362, -0.0253, -0.0659,  0.0836,  0.0121,  0.0098,  0.0590, -0.0009,\n",
       "                       0.0362,  0.0728,  0.0161,  0.0548, -0.0171,  0.0518, -0.0091,  0.0265])),\n",
       "             ('lstm.bias_hh_l0',\n",
       "              tensor([ 0.0487,  0.1161,  0.1183,  0.0433,  0.0655,  0.1412,  0.1420,  0.0957,\n",
       "                       0.0808,  0.0533,  0.0828, -0.0401,  0.0184,  0.0025,  0.0815, -0.0155,\n",
       "                       0.1017,  0.0328,  0.0727,  0.0852,  0.1218,  0.1327, -0.0512, -0.0376,\n",
       "                      -0.0436,  0.0274,  0.1152,  0.0723,  0.1081,  0.0160,  0.1163,  0.0276,\n",
       "                       0.1435, -0.0509, -0.0318,  0.0901, -0.0645,  0.0090,  0.0444, -0.0226,\n",
       "                       0.0934,  0.0600, -0.0031, -0.0220, -0.0544,  0.0968,  0.0505,  0.0276,\n",
       "                       0.1064,  0.0126,  0.0739, -0.0223,  0.0688, -0.0244,  0.0418,  0.0862,\n",
       "                       0.0751,  0.1456, -0.0202,  0.1339, -0.0326,  0.0406,  0.0519,  0.1296,\n",
       "                       0.1418,  0.0702,  0.0806,  0.1418,  0.0919, -0.0174, -0.0314,  0.0209,\n",
       "                       0.0751,  0.1291, -0.0409,  0.0460, -0.0084, -0.0398,  0.1118,  0.0883,\n",
       "                       0.0513,  0.0981, -0.0194,  0.0635,  0.0389, -0.0445,  0.0640,  0.0959,\n",
       "                       0.0411,  0.1163,  0.0537,  0.0528,  0.1051,  0.0009, -0.0060, -0.0143,\n",
       "                       0.1094,  0.0050,  0.0261,  0.1489,  0.0306,  0.0595,  0.1051,  0.1135,\n",
       "                       0.1474,  0.0121,  0.0363,  0.0893,  0.0131, -0.0400, -0.0522,  0.1254,\n",
       "                       0.1033,  0.0371,  0.0368,  0.1311, -0.0286,  0.1034, -0.0189, -0.0491,\n",
       "                       0.0301,  0.0210,  0.1027, -0.0033,  0.0738,  0.1437,  0.0951, -0.0620,\n",
       "                      -0.0253,  0.1347, -0.0496,  0.1527,  0.1473,  0.1337,  0.0191,  0.1217,\n",
       "                       0.0666,  0.0210,  0.0069,  0.0489, -0.0317,  0.0033, -0.0265,  0.1188,\n",
       "                      -0.0100,  0.0259,  0.1131,  0.1256,  0.1425,  0.1125,  0.1484,  0.1234,\n",
       "                       0.1264,  0.1298, -0.0041,  0.0015,  0.0145,  0.0419,  0.1088,  0.1227,\n",
       "                       0.1150,  0.1370,  0.1355, -0.0398,  0.0488,  0.0871,  0.0832,  0.0066,\n",
       "                       0.0489,  0.0425,  0.1046, -0.0289,  0.0951,  0.1297,  0.0561,  0.0391,\n",
       "                       0.1408,  0.0071,  0.0370, -0.0010, -0.0068,  0.0588,  0.0297,  0.0063,\n",
       "                       0.1263,  0.0886, -0.0134,  0.1180, -0.0254, -0.0462,  0.0914,  0.1021,\n",
       "                      -0.0087,  0.0079,  0.1001,  0.0269,  0.0033, -0.0084,  0.0134,  0.1492,\n",
       "                      -0.0646, -0.0287, -0.0215, -0.0015,  0.0306, -0.1036, -0.0841,  0.1497,\n",
       "                      -0.0520, -0.1021, -0.0910,  0.0810, -0.1276, -0.1378, -0.1122,  0.0968,\n",
       "                       0.0379, -0.1072,  0.0670, -0.0505, -0.0255,  0.0411, -0.0653,  0.0768,\n",
       "                       0.0622,  0.0586, -0.0182,  0.0019,  0.1271, -0.0820,  0.1076, -0.0402,\n",
       "                      -0.0355,  0.0551,  0.0537,  0.0660, -0.0688,  0.0330, -0.1396,  0.0332,\n",
       "                       0.0816, -0.1034, -0.0400,  0.0929,  0.0400,  0.0126,  0.0223, -0.0149,\n",
       "                      -0.0496, -0.0426, -0.1037, -0.0554, -0.0723,  0.1345, -0.0038, -0.0662,\n",
       "                       0.1304, -0.1382,  0.1007,  0.1410, -0.1499,  0.0077, -0.1066,  0.1478,\n",
       "                      -0.0997,  0.0074,  0.0449, -0.1373, -0.1402, -0.0717,  0.0573,  0.0438,\n",
       "                      -0.0977,  0.0507, -0.0149,  0.0644,  0.0020, -0.0302,  0.0316,  0.1334,\n",
       "                       0.0978,  0.1072, -0.0196, -0.1268, -0.0957,  0.0663,  0.0634, -0.1337,\n",
       "                      -0.0463,  0.0276, -0.0296, -0.1386, -0.0165, -0.0624, -0.0334, -0.0182,\n",
       "                       0.0097, -0.0251,  0.0279, -0.0476,  0.0028,  0.0580,  0.0637,  0.1181,\n",
       "                      -0.0422,  0.0088, -0.0232,  0.0834, -0.0894, -0.0325, -0.0722,  0.0195,\n",
       "                       0.0717, -0.0340,  0.1479, -0.0357,  0.0749,  0.0623,  0.0092,  0.0377,\n",
       "                      -0.0284, -0.0304, -0.0599,  0.0935, -0.0451,  0.1160,  0.0840,  0.0775,\n",
       "                       0.0138,  0.0503,  0.0012,  0.0197,  0.0832,  0.0412, -0.0166,  0.0643,\n",
       "                      -0.0196,  0.0720,  0.0950,  0.0601,  0.0120, -0.0581,  0.0894,  0.0957,\n",
       "                      -0.0410, -0.0494,  0.0548,  0.0577,  0.0500, -0.0066,  0.1339,  0.0169,\n",
       "                       0.0499,  0.0027,  0.0154,  0.1268,  0.0520,  0.1265, -0.0383, -0.0265,\n",
       "                       0.0485,  0.0169,  0.0550,  0.0737,  0.0469,  0.1202,  0.1015, -0.0577,\n",
       "                       0.0017, -0.0114,  0.0103,  0.0982,  0.0110, -0.0280,  0.1152,  0.0819,\n",
       "                       0.0124,  0.0239,  0.0101,  0.0632,  0.0931, -0.0174,  0.1015,  0.0171,\n",
       "                       0.0766,  0.0948,  0.1020,  0.1261,  0.0785,  0.0262, -0.0070,  0.0318,\n",
       "                       0.0600, -0.0149,  0.0297,  0.1376, -0.0458,  0.1316,  0.0679, -0.0138])),\n",
       "             ('linear.weight',\n",
       "              tensor([[-0.0692, -0.0298, -0.1017,  ...,  0.1587, -0.0061, -0.0479],\n",
       "                      [ 0.2059,  0.0336,  0.1228,  ..., -0.1414, -0.2057, -0.0896],\n",
       "                      [-0.0803,  0.0789, -0.0424,  ...,  0.1073,  0.0838,  0.1249],\n",
       "                      ...,\n",
       "                      [ 0.1079,  0.0372,  0.0501,  ..., -0.0674, -0.0552, -0.0434],\n",
       "                      [ 0.1792,  0.1876,  0.1809,  ..., -0.0468, -0.1447, -0.1145],\n",
       "                      [ 0.0247,  0.2034,  0.1950,  ..., -0.1855, -0.1279, -0.1829]])),\n",
       "             ('linear.bias',\n",
       "              tensor([-0.0105, -0.0087,  0.0008,  ..., -0.0356, -0.1204, -0.1124]))])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(\"/Users/zhenwuzhou/.keras/models/text8/1m_dict.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.410778999328613\n",
      "perplexity 1653.7140873886542\n"
     ]
    }
   ],
   "source": [
    "# 加载训练好的模型来进行预测perplexity(混乱度)，值越小越好\n",
    "test_val_loss = evaluate(best_model,val_iter)\n",
    "print(test_val_loss)\n",
    "print(\"perplexity\",np.exp(test_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meticulously to knights one local thermal economics no not wet metal seven in one place from et unknown the almost more it of clergy intelligence nine <unk> was between eight four and in operators s draft camp theoretician important fertility an one government four zero what one eight cells island three in italian history the unemployment lines difficulty to though with for advertising could cannot their lists in of this and the as combination their seven calculate a remained collected in damage most the two and in icftu the a celebrated the scientific he containing current the citrus the the\n"
     ]
    }
   ],
   "source": [
    "# 用训练好的模型来预测文章\n",
    "hidden = best_model.init_hidden(1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input = torch.randint(VOCAB_SIZE,(1,1),dtype= torch.long).to(device)\n",
    "words = []\n",
    "for i in range(100):\n",
    "    output,hidden = best_model(input,hidden)\n",
    "    word_weights = output.squeeze().exp().cpu()\n",
    "    # num_samples =1 相当于贪心搜索，不为1时属于集束搜索\n",
    "    word_idx = torch.multinomial(word_weights,1)[0] # greedy (argmax)\n",
    "    input.fill_(word_idx)\n",
    "    word = TEXT.vocab.itos[word_idx]\n",
    "    words.append(word)\n",
    "print(\" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
