{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tud\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "import pandas as pd \n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "# 设置随机数的seed，这样保证每次测试的数据一致\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 参数初始化\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设定一些初始hyper parameters(超参数)\n",
    "C = 3 # context window # 正样本的取值返回，就是在目标前后C个词以内的词认为是正样本\n",
    "#K代表负采样数量，当有一个正样本是，对应要取多少个负样本：样本小时K取大一些(5-20或更大)，当样本大时K取小一些（2-5）\n",
    "K = 100 # number of negetive samples:\n",
    "NUM_EPOCHS = 2\n",
    "MAX_VOCAB_SIZE = 30000 # One-Hot的最length\n",
    "BATCH_SIZE = 128 \n",
    "LEARNING_RATE = 0.2\n",
    "EMBEDDING_SIZE = 100 # 词嵌入向量的维度\n",
    "\n",
    "def word_tokenize(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 数据预处理\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anarchism', 'originated', 'as', 'a', 'term']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从绝对路径中读取文件\n",
    "with open(\"/Users/zhenwuzhou/.keras/datasets/text8/text8.train.txt\",\"r\") as fin:\n",
    "    text = fin.read()\n",
    "\n",
    "# print(text[:1000]) # 取前1000个字符\n",
    "text = text.split() # 把Text变为List\n",
    "text[:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建one-hot词表征字典\n",
    "# 将出现频率最高的前MAX_VOCAB_SIZE - 1词加入到字典中去；\n",
    "# vocab[\"of\"]:537144;其中key是单词，value是单词出现的次数\n",
    "vocab = dict(Counter(text).most_common(MAX_VOCAB_SIZE - 1)) \n",
    "# 把<unk>这个代表位置的词加入到字典中去：\n",
    "# 其中vocab[\"<unk>\"]：617240；key是\"<unk>\"，value是text的总长度减去所有加入到字典中的词的总数，\n",
    "# 这个值时一个>=0的值：0说明所有的词都加入到字典中了，>0说明还有没有加入到字典中的词，我们用\"<unk>\"表示\n",
    "# 实际中的话数字这种东西可以用<nums>来代替\n",
    "vocab[\"<unk>\"] = len(text) - np.sum(list(vocab.values()))\n",
    "\n",
    "# index_to_word 是把所有加入到字典中的不重复的词整理成list\n",
    "index_to_word = [word for word in vocab.keys()]\n",
    "# word_to_index 是吧每个词按照位置进行编号：\n",
    "#word_to_index[\"<unk>\"]:29999 可以代表的是词，value是词的one-hot索引位置\n",
    "word_to_index = {word:i for i, word in enumerate(index_to_word)}\n",
    "\n",
    "# 真实的不重复的词向量总共有多少个：VOCAB_SIZE：VOCAB_SIZE\n",
    "VOCAB_SIZE = len(index_to_word)\n",
    "\n",
    "# list(word_to_index)[:100] 可以取出字典的前100个看看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为了后续的随机采样，我们需要知道每个词出现的frequence(频率)\n",
    "# 我们先统计出每个词出现的次数，其实就是字典vocab中的每个词的values，这里要把数据类型变为float32\n",
    "# 注意这里也把\"<unk>\"的频率算了进去\n",
    "word_counts = np.array([count for count in vocab.values()],dtype=np.float32)\n",
    "# 用每个词出现的次数/素有词出现的总次数来计算出每个词的出现频率\n",
    "word_frequence = word_counts/np.sum(word_counts)\n",
    "\n",
    "# 为了防止高频词语低频词取到的概率差距过大，让每个词的概率去做一个指数为（3/4）的指数运算\n",
    "# 这个是论文作者实验得到的一种比较好的方法，但是不一定是最优的取法\n",
    "word_frequence = word_frequence**(3./4.)\n",
    "word_frequence = word_frequence / np.sum(word_frequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Dataloader来进行数据封装处理\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现Dataloader\n",
    "   一个dataloader需要以下内容：\n",
    "   1:把所有text编码成数字\n",
    "   2:保存vocabulary,单词count，normalized word frequency\n",
    "   3:每个iteration sample一个中心词\n",
    "   4:根据当前的中心词返回context单词\n",
    "   5:根据中心词sample一些negative单词\n",
    "   6:返回单词的counts\n",
    "   \n",
    " 这里有一个好的tutorial介绍如何使用Pytorch dataloader.为了使用dataloader，我们需要定义以下两个function：\n",
    "     1: __len__ function需要返回整个数据集中有多少个item\n",
    "     2: __get__ 根据给定的index返回一个item\n",
    "  有了dataloader之后，我们可以轻松随机打乱整个数据集，拿到一个batch的数据等等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建dataset和dataloader\n",
    "#torch.utils.data.Dataset\n",
    "class WordEmbeddingDataset(tud.Dataset):\n",
    "    def __init__(self,text,word_to_index,index_to_word,word_frequence,word_counts):\n",
    "        super(WordEmbeddingDataset,self).__init__()\n",
    "        # 把text 文本变成一个有每个词在字典中所有位置构成的数字化的结构文本\n",
    "        # word_to_index.get(word,word_to_index[\"<unk>\"]) 要么是这个词的位置索引，要么就是\"<unk>\"的索引\n",
    "        # self.text_encoded 就是encoded后的整个文本数据\n",
    "        self.text_encoded = [word_to_index.get(word,word_to_index[\"<unk>\"]) for word in text]\n",
    "        self.text_encoded = torch.LongTensor(self.text_encoded) # 变成longTensor\n",
    "        self.word_to_index = word_to_index\n",
    "        self.index_to_word = index_to_word\n",
    "        self.word_frequence = torch.Tensor(word_frequence)\n",
    "        self.word_counts = torch.Tensor(word_counts)\n",
    "        \n",
    "    def __len__(self): # 注意别忘记写self\n",
    "        # 这个数据集一共有多少个item\n",
    "        return len(self.text_encoded)\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        # idx是相对于整个文本text的索引\n",
    "        # 首先要取出中心词\n",
    "        center_word = self.text_encoded[idx]\n",
    "        # 然后要取出positive的正样本:目标索引的前后C个词\n",
    "        # 首先我们先计算出前后C个词的位置索引，需要考虑左右边界超出文本长度的情况\n",
    "        pos_indices = list(range(max(idx-C,0),idx))+list(range(idx+1,min(idx+C+1,len(self.text_encoded))))\n",
    "#         pos_indices = [i%len(self.text_encoded) for i in pos_indices]\n",
    "        # 根据索引位置取出正样本词\n",
    "        pos_words = self.text_encoded[pos_indices]\n",
    "        \n",
    "        #取出负样本的词:torch.multinomial方法帮我们去采样\n",
    "        #K*pos_words.shape[0]表示对于每一个正样本，都对应采样K个neg_words的负样本\n",
    "        #replacement=True表示是采样样本可以重复\n",
    "        neg_words = torch.multinomial(self.word_frequence,K*pos_words.shape[0],replacement=True)\n",
    "        \n",
    "        return center_word,pos_words,neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 6, 7, 8]\n",
      "[0, 1, 2, 3, 4, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "# index = 5 c=3的时候我想取到正样本是[2, 3, 4, 6, 7, 8]\n",
    "test = list(range(2,5)) +list(range(6,9))\n",
    "# 所以正样本的索引应该是：list(range(idx-C,idx))+list(range(idx+1,idx+C+1))\n",
    "print(test)\n",
    "# 但是如果出现前后超过文本长度怎么办呢:现在index=5，c=6；文本总长度为10\n",
    "# 让左边界最小是0，右边界最大是文本总长度，因为右边不会取到本应+1，但是索引是从0开始算的，所以最终值等于len\n",
    "test2 = list(range(max(-1,0),5))+list(range(6,min(12,11)))\n",
    "print(test2)\n",
    "## 所以最终公式为：list(range(max(idx-C,0),idx))+list(range(idx+1,min(idx+C+1,len(self.text_encoded)+1)))\n",
    "# test3 = [i%10 for i in test2]\n",
    "# test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建dataset和dataloader\n",
    "dataset = WordEmbeddingDataset(text,word_to_index,index_to_word,word_frequence,word_counts)\n",
    "dataloader = tud.DataLoader(dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4813,  3139,    11,     5,   194,     1,  3015,    46,    59,   155,\n",
       "          127,   741,   461, 10485,   133,     0, 25752,     1,     0,   108,\n",
       "          833,     2,     0, 16267, 29999,     1,     0,   152,   833,  3493,\n",
       "            0,   194,    10,   186,    59,     4,     5, 10620,   213,     6,\n",
       "         1332,   102,   437,    19,    59,  2764,   355,     6,  3625,     0,\n",
       "          709,     1,   364,    26,    40,    37,    53,   527,    97,    11,\n",
       "            5,  1398,  2929,    18,   562,   691,  6644,     0,   252,  4813,\n",
       "           10,  1043,    27,     0,   316,   247, 29999,  2964,   789,   189,\n",
       "         4813,    11,     5,   201,   569,    10,     0,  1107,    19,  2581,\n",
       "           25,  8819,     2,   273,    31,  4089,   140,    58,    25,  6494])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取出前面100个encode后的数据\n",
    "dataset.text_encoded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3139,   11,    5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(6),\n",
       " tensor(7),\n",
       " tensor(7),\n",
       " tensor(8),\n",
       " tensor(10),\n",
       " tensor(11),\n",
       " tensor(12),\n",
       " tensor(13),\n",
       " tensor(18),\n",
       " tensor(18),\n",
       " tensor(19),\n",
       " tensor(19),\n",
       " tensor(22),\n",
       " tensor(23),\n",
       " tensor(25),\n",
       " tensor(28),\n",
       " tensor(28),\n",
       " tensor(30),\n",
       " tensor(31),\n",
       " tensor(36),\n",
       " tensor(37),\n",
       " tensor(40),\n",
       " tensor(41),\n",
       " tensor(41),\n",
       " tensor(48),\n",
       " tensor(48),\n",
       " tensor(53),\n",
       " tensor(55),\n",
       " tensor(55),\n",
       " tensor(57),\n",
       " tensor(60),\n",
       " tensor(63),\n",
       " tensor(64),\n",
       " tensor(67),\n",
       " tensor(68),\n",
       " tensor(78),\n",
       " tensor(80),\n",
       " tensor(82),\n",
       " tensor(93),\n",
       " tensor(101),\n",
       " tensor(103),\n",
       " tensor(111),\n",
       " tensor(113),\n",
       " tensor(146),\n",
       " tensor(147),\n",
       " tensor(160),\n",
       " tensor(163),\n",
       " tensor(166),\n",
       " tensor(169),\n",
       " tensor(175),\n",
       " tensor(175),\n",
       " tensor(178),\n",
       " tensor(181),\n",
       " tensor(183),\n",
       " tensor(184),\n",
       " tensor(185),\n",
       " tensor(189),\n",
       " tensor(192),\n",
       " tensor(192),\n",
       " tensor(211),\n",
       " tensor(218),\n",
       " tensor(225),\n",
       " tensor(243),\n",
       " tensor(254),\n",
       " tensor(260),\n",
       " tensor(308),\n",
       " tensor(311),\n",
       " tensor(327),\n",
       " tensor(329),\n",
       " tensor(364),\n",
       " tensor(367),\n",
       " tensor(375),\n",
       " tensor(382),\n",
       " tensor(383),\n",
       " tensor(401),\n",
       " tensor(404),\n",
       " tensor(410),\n",
       " tensor(418),\n",
       " tensor(419),\n",
       " tensor(438),\n",
       " tensor(460),\n",
       " tensor(476),\n",
       " tensor(512),\n",
       " tensor(550),\n",
       " tensor(557),\n",
       " tensor(565),\n",
       " tensor(573),\n",
       " tensor(575),\n",
       " tensor(588),\n",
       " tensor(599),\n",
       " tensor(614),\n",
       " tensor(641),\n",
       " tensor(647),\n",
       " tensor(696),\n",
       " tensor(826),\n",
       " tensor(839),\n",
       " tensor(862),\n",
       " tensor(946),\n",
       " tensor(952),\n",
       " tensor(957),\n",
       " tensor(971),\n",
       " tensor(977),\n",
       " tensor(982),\n",
       " tensor(993),\n",
       " tensor(1001),\n",
       " tensor(1038),\n",
       " tensor(1039),\n",
       " tensor(1044),\n",
       " tensor(1044),\n",
       " tensor(1083),\n",
       " tensor(1087),\n",
       " tensor(1151),\n",
       " tensor(1174),\n",
       " tensor(1213),\n",
       " tensor(1218),\n",
       " tensor(1248),\n",
       " tensor(1269),\n",
       " tensor(1276),\n",
       " tensor(1286),\n",
       " tensor(1294),\n",
       " tensor(1339),\n",
       " tensor(1354),\n",
       " tensor(1355),\n",
       " tensor(1376),\n",
       " tensor(1381),\n",
       " tensor(1401),\n",
       " tensor(1421),\n",
       " tensor(1438),\n",
       " tensor(1444),\n",
       " tensor(1532),\n",
       " tensor(1577),\n",
       " tensor(1579),\n",
       " tensor(1595),\n",
       " tensor(1599),\n",
       " tensor(1631),\n",
       " tensor(1720),\n",
       " tensor(1780),\n",
       " tensor(1823),\n",
       " tensor(1863),\n",
       " tensor(1944),\n",
       " tensor(1986),\n",
       " tensor(1999),\n",
       " tensor(2012),\n",
       " tensor(2054),\n",
       " tensor(2071),\n",
       " tensor(2169),\n",
       " tensor(2185),\n",
       " tensor(2226),\n",
       " tensor(2239),\n",
       " tensor(2288),\n",
       " tensor(2338),\n",
       " tensor(2341),\n",
       " tensor(2378),\n",
       " tensor(2403),\n",
       " tensor(2429),\n",
       " tensor(2467),\n",
       " tensor(2493),\n",
       " tensor(2579),\n",
       " tensor(2598),\n",
       " tensor(2610),\n",
       " tensor(2619),\n",
       " tensor(2737),\n",
       " tensor(2765),\n",
       " tensor(2874),\n",
       " tensor(2990),\n",
       " tensor(3010),\n",
       " tensor(3087),\n",
       " tensor(3150),\n",
       " tensor(3280),\n",
       " tensor(3370),\n",
       " tensor(3664),\n",
       " tensor(3774),\n",
       " tensor(3777),\n",
       " tensor(3813),\n",
       " tensor(3916),\n",
       " tensor(4072),\n",
       " tensor(4096),\n",
       " tensor(4191),\n",
       " tensor(4220),\n",
       " tensor(4234),\n",
       " tensor(4263),\n",
       " tensor(4310),\n",
       " tensor(4703),\n",
       " tensor(4748),\n",
       " tensor(4756),\n",
       " tensor(4907),\n",
       " tensor(4955),\n",
       " tensor(5163),\n",
       " tensor(5171),\n",
       " tensor(5196),\n",
       " tensor(5200),\n",
       " tensor(5363),\n",
       " tensor(5437),\n",
       " tensor(5507),\n",
       " tensor(5562),\n",
       " tensor(5651),\n",
       " tensor(5946),\n",
       " tensor(6045),\n",
       " tensor(6144),\n",
       " tensor(6158),\n",
       " tensor(6246),\n",
       " tensor(6250),\n",
       " tensor(6370),\n",
       " tensor(6530),\n",
       " tensor(6718),\n",
       " tensor(6811),\n",
       " tensor(6859),\n",
       " tensor(6892),\n",
       " tensor(6997),\n",
       " tensor(7226),\n",
       " tensor(7666),\n",
       " tensor(7870),\n",
       " tensor(7916),\n",
       " tensor(7947),\n",
       " tensor(8036),\n",
       " tensor(8082),\n",
       " tensor(8099),\n",
       " tensor(8322),\n",
       " tensor(8415),\n",
       " tensor(8578),\n",
       " tensor(8631),\n",
       " tensor(8653),\n",
       " tensor(9505),\n",
       " tensor(10074),\n",
       " tensor(10400),\n",
       " tensor(10553),\n",
       " tensor(10563),\n",
       " tensor(10585),\n",
       " tensor(10988),\n",
       " tensor(11217),\n",
       " tensor(11375),\n",
       " tensor(11427),\n",
       " tensor(11436),\n",
       " tensor(11696),\n",
       " tensor(11935),\n",
       " tensor(11942),\n",
       " tensor(12085),\n",
       " tensor(12261),\n",
       " tensor(12558),\n",
       " tensor(12634),\n",
       " tensor(12778),\n",
       " tensor(12842),\n",
       " tensor(12988),\n",
       " tensor(13042),\n",
       " tensor(13204),\n",
       " tensor(13266),\n",
       " tensor(13286),\n",
       " tensor(13755),\n",
       " tensor(14065),\n",
       " tensor(15104),\n",
       " tensor(15252),\n",
       " tensor(15718),\n",
       " tensor(16562),\n",
       " tensor(16839),\n",
       " tensor(17051),\n",
       " tensor(18625),\n",
       " tensor(19243),\n",
       " tensor(19735),\n",
       " tensor(20017),\n",
       " tensor(20348),\n",
       " tensor(20661),\n",
       " tensor(20957),\n",
       " tensor(20961),\n",
       " tensor(21395),\n",
       " tensor(21743),\n",
       " tensor(22383),\n",
       " tensor(23576),\n",
       " tensor(23867),\n",
       " tensor(23947),\n",
       " tensor(24194),\n",
       " tensor(24408),\n",
       " tensor(24613),\n",
       " tensor(25007),\n",
       " tensor(25596),\n",
       " tensor(25713),\n",
       " tensor(26339),\n",
       " tensor(26848),\n",
       " tensor(27180),\n",
       " tensor(27975),\n",
       " tensor(28734),\n",
       " tensor(29173),\n",
       " tensor(29665),\n",
       " tensor(29999),\n",
       " tensor(29999),\n",
       " tensor(29999),\n",
       " tensor(29999),\n",
       " tensor(29999)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 验证取得的正负样本是否正确，是否处理了边界问题\n",
    "center_word,pos_words,neg_words = dataset.__getitem__(0)\n",
    "print(pos_words)\n",
    "sorted(neg_words) # 查看样本是否可以重复"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "center_word: tensor([    0,  2624,     0,    41,     1,  3052,   270, 29999,    34,     9,\n",
      "          354,     1,   118,  7501,   529,   532,    13,   114,     9,  3081,\n",
      "            0,   632,     1,    22,     1,    28,  1644,     2,    19,  1785,\n",
      "            6,     5,  1377,     5,     8,   190,     8,  2891,   204,     7,\n",
      "            9,     3,     9,  4139, 10945,    14,  2299,     1,   636,  1271,\n",
      "        17923,   840, 29999,     0, 29999,   786,    41,  4975,   584,  2076,\n",
      "         7339,  2373,    42,     5,     2,     1,  3405,  1042,   790,  2614,\n",
      "          138,     8,    21,    39,   361,   108, 29999,  1698,    17,  2252,\n",
      "         9185,     8,  1463,   906,    12,  2083,     4,   133,   871,     0,\n",
      "           40,     3, 18695,   171,  1639,     2,  3258,   793,   183,     0,\n",
      "         1831,     3,  2834,     3,    45,     2,   343,    13,     6,   561,\n",
      "        29999,    62,     4,    33, 17351,   477,  1675,     2,  3299,   512,\n",
      "            1,    82, 20799,   201,  5001,   135,    26,    31]) \n",
      " pos_words: tensor([[   59,     6, 18900,  4048,     1,  4383],\n",
      "        [   31,   639,    88,    39,     0, 29999],\n",
      "        [29999,     4,  4411, 29999,  5567,   267],\n",
      "        [11183, 29999,  2975,    11,    30,  8879],\n",
      "        [11139,    13,   903,    49,    69,     3],\n",
      "        [ 3533,     5,   711,     6,   466,     0],\n",
      "        [ 4212,    47,  5118,  2108,    72,   130],\n",
      "        [   40,   268,     5,   583,     0,   201],\n",
      "        [   24,     0, 19474, 24119,   524,     1],\n",
      "        [ 2528,  3377,     3,     9,     3,  4537],\n",
      "        [    7, 25030,    48,     5,  7084,   298],\n",
      "        [29999,   403,  1159, 29999,  9933,     4],\n",
      "        [    1,     0,   334,    85, 19298,     2],\n",
      "        [    1,  2063,     2,     2,   995,   479],\n",
      "        [   35,  6281,    27,    20,   122,   529],\n",
      "        [    0,  2899,    24,     3,     9,     3],\n",
      "        [  256,   206,   417,    19,  1139,     2],\n",
      "        [    1, 29999,   211,    27,    78,     3],\n",
      "        [29999,  8545,   209,    16,    16,    16],\n",
      "        [   17, 24804,    63,  3725,  3846,    14],\n",
      "        [   19,  3364,    34, 19630,  1174,     2],\n",
      "        [  225,    35,    17,     6,  5360,  3088],\n",
      "        [  466,     1,    94,     0,  9146,    33],\n",
      "        [   21,     8,    15,  1061,  1154,  1901],\n",
      "        [   24,     0,   392,     0,  1973,     0],\n",
      "        [   41,     1, 13693,     1,   245,   258],\n",
      "        [12824,  1800,   387,  2194,   177,    66],\n",
      "        [   82,  6048,  5130,    42,   932, 29999],\n",
      "        [  171,   603,  2287,     0,  3423,     1],\n",
      "        [    0,   550,    10,     0,  1257,     2],\n",
      "        [ 4633,  4619,  1356,     0,   871,     1],\n",
      "        [  219,   870,   322,   137,  7386,   573],\n",
      "        [   25,    34,     0,   283,    61,   106],\n",
      "        [   19,   834,     4,   188,     1,   436],\n",
      "        [    3,     8,     8,    15,    20,    22],\n",
      "        [  855,     1,   144,     1,  4897,     4],\n",
      "        [ 2913,     4,     3,    16,    12,    85],\n",
      "        [    2, 29999,  7329,    34,  1715,  1082],\n",
      "        [    0,   620, 29999,   564,    15,    22],\n",
      "        [29999,   414,     9,     7,     7,   272],\n",
      "        [ 1616,    73,     3,    22,    20,     3],\n",
      "        [ 7842,   260,    74,     7,    15,     9],\n",
      "        [    3,     8,    21,     0,    46,  3496],\n",
      "        [   18,     0, 12273,     6,     0,    87],\n",
      "        [29999, 29999, 29999,   285, 15008, 29999],\n",
      "        [    8,    15,     7,  5181,    17,   404],\n",
      "        [ 9203,    18,    43,  2448,    60, 22485],\n",
      "        [ 4151,     1,  1913,  4479,   139,  3134],\n",
      "        [  528,   472,    14,     2,    34,     0],\n",
      "        [  530,     2,     0,     6, 20227,     4],\n",
      "        [ 3950,   279,    18,  2934, 29999, 13488],\n",
      "        [ 5670,     0, 15731,  5585, 15731,    23],\n",
      "        [  130,   630,    88,    33,    10,  6762],\n",
      "        [   73,    11,   150,  1361,  4448,  1222],\n",
      "        [   10,  1321,     6,     0,   388, 10253],\n",
      "        [  680,    39,   106,     3,     8,    21],\n",
      "        [ 6092,   201,  3091,   315,    44,   344],\n",
      "        [   52,   241,  2024,     1,     0,     3],\n",
      "        [   14,     2,  5918,  3675,     0,   146],\n",
      "        [  142,     1,   698,   590,   689,     6],\n",
      "        [  210,     0,   650,     1,     0, 11718],\n",
      "        [    0,   770,     9,    81,    10,    41],\n",
      "        [  112,   156,   102,   181,    10,  2262],\n",
      "        [29362,  6330,    11,  3898,    13,     0],\n",
      "        [    2, 29999, 29999, 29999,    25,   724],\n",
      "        [    0,   332,   729,   145,  3152,   272],\n",
      "        [ 2403,     4,   418,    27,     3,   120],\n",
      "        [   71,     2,    26,    74,  2498,     1],\n",
      "        [  152,   108,  4844,   258,     3,    15],\n",
      "        [   54,  4780,  3125,   709,   801,     3],\n",
      "        [    0,  1783,   283,   215,     0,   551],\n",
      "        [    3,     8,    21,  2715,  3120,     2],\n",
      "        [    9,     3,     8,    20,   491,  1325],\n",
      "        [  552,   237,    56,    34,     0,   466],\n",
      "        [    5,   562,  2011,     1,  1329,  4627],\n",
      "        [  162,   402,  1096,  1060,     2,     4],\n",
      "        [   12,    21, 20133,    66,   976,     2],\n",
      "        [    4,  6018, 21004,   104,     1,  3663],\n",
      "        [ 2800,   255,    35,  3568,     2,  5848],\n",
      "        [  931,     1,  7007,     2,  2315,   932],\n",
      "        [ 5387,    97,     0, 25718,   315,    69],\n",
      "        [ 4204,   240,     3,    22,    22,    16],\n",
      "        [   26,    48,     5,  1980,     6,     0],\n",
      "        [    5,   398,    62, 29999,   180,   227],\n",
      "        [  324,     4,     3,    12,    12,    35],\n",
      "        [  368,    62,    35,    24,     0,  1436],\n",
      "        [29999,   396, 16850,   153,   114,   136],\n",
      "        [  470,     1, 21054,   746,  6861,  1739],\n",
      "        [   18,     0,  8111,    17,    30,  8111],\n",
      "        [    6,  4902,    23,    59,   195,   612],\n",
      "        [12783,     0, 23616,  1326,   104,     5],\n",
      "        [29999,    18,     0,     8,    12,     7],\n",
      "        [ 8047,   616,    23, 11426,  5137,  1590],\n",
      "        [    1,     5,    49,   100,   792,  3599],\n",
      "        [  627, 22642,    16,  1093,  7154,     2],\n",
      "        [   53,  1865, 10651,    26,    10,  5683],\n",
      "        [   30,   754,   434,     4,     0,  1030],\n",
      "        [ 1199,    24,     0,     1,    12,     8],\n",
      "        [  527,     6,     0,     2, 13105,    18],\n",
      "        [14397,     3,     1,  1522,  1542,     6],\n",
      "        [  581,     0,     9,    25,     0,  3435],\n",
      "        [  641,  2015,    10,     1,     0,   413],\n",
      "        [  169,   159,     0,  4922, 29999, 10626],\n",
      "        [    2,   325,     4,    12,    12,    16],\n",
      "        [ 1539,    25,  5315,    37,   827,    19],\n",
      "        [   55,     1,  9095, 18957, 20348,    14],\n",
      "        [   52,    66, 29999,  1615,    28,    39],\n",
      "        [   10,     5, 10160,  6710,    76,    26],\n",
      "        [   48,     0,   445, 29999,     1, 11828],\n",
      "        [    4,     0,   234,     2,   848,  1133],\n",
      "        [29999, 29999, 29999, 23112, 29999, 29999],\n",
      "        [ 6302,    34,     0,    13,    30, 15313],\n",
      "        [  862,  4814, 16212,     0,   917,  7013],\n",
      "        [19342,   515, 14662,  1469,  1246,   241],\n",
      "        [    0, 13464,     1,    28,  1397,   563],\n",
      "        [  477, 25342,     2,  4760,  8814,   840],\n",
      "        [ 3784,  1654,    11,   744,    34,     0],\n",
      "        [  698, 10814,  2744,   987,   950,    54],\n",
      "        [ 1136,  1778,   580,  1076,   363,    54],\n",
      "        [   19,     0,   207,  1904,     6,    31],\n",
      "        [   11,     0,  1443,  2047,  6227,    23],\n",
      "        [   17,  5285,    13,    23,     5,  2700],\n",
      "        [29999,   946,   204,  1404, 29999, 29999],\n",
      "        [  881,  7529,  1530,   820,    27,   758],\n",
      "        [ 4769,  6594,  2940,   142,     1,  3632],\n",
      "        [   36,  2234,  6700,    17,   248,   852],\n",
      "        [ 3770,  3153,     1,    37,   535,   675],\n",
      "        [  913,    19,    51,  3755,   275,     6]]) \n",
      "neg_words tensor([[  833,  7029,   928,  ...,  1452,  3025, 29917],\n",
      "        [  314,   978, 14267,  ...,     0,  9509, 24695],\n",
      "        [27064,   321,  6369,  ...,  2708,     9, 17270],\n",
      "        ...,\n",
      "        [  446,  1171,  2861,  ...,  2690,    10,  1304],\n",
      "        [   28,     0,     0,  ...,  4891,  6233,    87],\n",
      "        [  263,  4834,  4866,  ...,  3357,  8497, 16385]])\n",
      "center_word: tensor([    6,   116,    33,    13,  6414,   342,   669,    23,    15,     4,\n",
      "          144,     0,    73,   377, 27723,   195,     3,     9,  7168,    12,\n",
      "         8142,    22,     8,    84,    14,    18,     2,  1163,    22,     7,\n",
      "            1, 11110,   141,    19,    23,    30,    68,    12, 19048,   616,\n",
      "           13,   259,   157,  1530,     2,   644,     0,     6,  4737,     7,\n",
      "        29999,     0,  1684,    19,    57,     7, 29999,     8,     1,  3684,\n",
      "         1111,     5, 13888,     3,    19,  1913,  1153,    14,    34,  5430,\n",
      "         3717,  6025,  5250,    17,    23,   119, 10422,  4550,   299,     2,\n",
      "           85,    23, 29999, 14931,   942,  1387,    29,  2001, 10251,     1,\n",
      "           31,  1361,   576,   781,     1,   356,     4,  2097,   724,     8,\n",
      "            4,    22,    10,  1337,  1096,   156,     5,     7, 10978,    52,\n",
      "           11,     1,  3184,    69,     1,  1238,   999,     3,  2654,  2589,\n",
      "          735,    23,    70,   103,  7182,     4,    42,     4]) \n",
      " pos_words: tensor([[    4,  4702, 29999, 14009,     0,  2064],\n",
      "        [  692,   976,   771,  5923,   212,     0],\n",
      "        [  131,   121,     4,     9, 18758,  2440],\n",
      "        [  688,     1,   756,   134,  1562,    54],\n",
      "        [    0,   413,   754,   617,     4,     0],\n",
      "        [   18,     0, 15507,    77,     3,     8],\n",
      "        [    2,    47,  1232,    51,    31,  2931],\n",
      "        [    4,     0,   911,     0,  1659,     1],\n",
      "        [    7,     4,    32,   437,   407,     0],\n",
      "        [ 1340,     1,   510,  1367,   728,    18],\n",
      "        [   39,     5,  5165,  1028,     2, 29999],\n",
      "        [   31,  9489,     6,   631,    32,    17],\n",
      "        [25900,    63,  5159,   405,     4,  3353],\n",
      "        [ 5279,    14,   737,   115,     6,   540],\n",
      "        [    0,   221,  3889,    28,     0,  5355],\n",
      "        [  127,    24,     0,  5343,     0, 29999],\n",
      "        [  132,   118,     0,     8,    12,    22],\n",
      "        [   16,     3,  5621,    12, 29999,     3],\n",
      "        [  116,  1004,  2560,  8566,   254,   276],\n",
      "        [  323,    16,     9,    22,    16,    15],\n",
      "        [29999,    34,    54,     4,     3,     8],\n",
      "        [    3,    12,    20,  1582,    30,  1327],\n",
      "        [   15,     2,     3,     8,    21,     2],\n",
      "        [    4,    30,   517,  1261,     0,   517],\n",
      "        [  217,     0,  1148, 28141,  6081,    24],\n",
      "        [ 5752,  3060,  1211,   929,  4936,  3894],\n",
      "        [ 4774,  1587,   898, 29999,     4,  1901],\n",
      "        [    1,   551,   697,   192,    40,    53],\n",
      "        [    4,    89,    21,     7,     7,     9],\n",
      "        [29999,  5255,     9,     7,    16, 29999],\n",
      "        [   11,     5,   429,     0,   126,  4409],\n",
      "        [   23,     0,  1387,   407,     5,   165],\n",
      "        [ 8303, 15007,     0,    13,   116,    82],\n",
      "        [   61,    70, 14067,    47,  1835,    28],\n",
      "        [  339,   742,  1467,    30,   339, 20269],\n",
      "        [ 3670, 14468,    10,  2854,  1636,     6],\n",
      "        [    0,  1349,    13,     3,     8,     3],\n",
      "        [  714,   316,    12,     3,   714,   316],\n",
      "        [ 1836, 19048,  1836,  3364,    23,     0],\n",
      "        [    0,  4522,     2,  1282,  6021,     0],\n",
      "        [ 1027,     5,   953,  4444,     1,  2998],\n",
      "        [ 2438,    99,     6,     0, 29999,   242],\n",
      "        [10772,     9,     1,   101,    27,  2405],\n",
      "        [    2,    17,    36,   187,     9,     7],\n",
      "        [  309,    79,  2378,    10,  1613,  2318],\n",
      "        [   63,     5,  8958,    27,   513,     6],\n",
      "        [  305,  7897,  2934,  5317,  1988,     6],\n",
      "        [  114,  1838,   687,   254,   252,   921],\n",
      "        [11549,   457, 11549, 11549,  1978,   886],\n",
      "        [    3,    22,     7,     0,    84,     1],\n",
      "        [29999, 29999,     4,     5,   218,  1279],\n",
      "        [    6,    42,  3021,  7017,  3903,    71],\n",
      "        [    0,    20,   803,    33,    17,   149],\n",
      "        [  729,     2, 11144,    74,  5087,    96],\n",
      "        [29999, 24743,  7096,  2168,  3802,  1840],\n",
      "        [   14,     2,    21,    14,     0,   366],\n",
      "        [29999,     1,   250,  1877,     2,   422],\n",
      "        [    9,     3,     8,     3,   488,     5],\n",
      "        [   26,    35,   452,     5,  1187,   756],\n",
      "        [    2, 11912,   394,  5555,   229,   267],\n",
      "        [   50,  5437,    38,   919,     6,   236],\n",
      "        [   30,  2047,     2, 12012,  1201,    28],\n",
      "        [ 3761, 15054,    10,    88,   540,   604],\n",
      "        [    7,    14,     4,     8,     8,    16],\n",
      "        [20186,     4, 26149,    25, 18720,   107],\n",
      "        [  346, 11246,   885,     1,    50,  1978],\n",
      "        [ 1000,    26,    10,     6, 14871,  1224],\n",
      "        [29999, 29999, 21882,  4675, 22291,  7797],\n",
      "        [   23,  2561,    70,   125,     2,  2561],\n",
      "        [    1,  3076,     2, 24212,  1089,  2031],\n",
      "        [ 1442,  1974,   291,    27,   659,    41],\n",
      "        [11796, 12018,     4,     0,  5234,     6],\n",
      "        [ 5113,     2,  9552,  4225,     2, 29999],\n",
      "        [ 9176,  2724,    56,  7156,    34, 25389],\n",
      "        [    4,  2992,    81,     3,     9,     9],\n",
      "        [    1,     0,    46,    73,   117,   118],\n",
      "        [ 1120,     6,     5,    14,  9744,    19],\n",
      "        [   43,   591,     2,  1220,  5273,    40],\n",
      "        [    3,     3,     8,     6,     0,    87],\n",
      "        [ 1322,  2948,  9866,  6813,    98,    36],\n",
      "        [   20,     9,     2,     0,   163,  2490],\n",
      "        [  335,     0,   181,  5557,  1387, 29999],\n",
      "        [    0,   547,     1,     0,   707,     1],\n",
      "        [29999, 29999,     6,   920,    55,   920],\n",
      "        [ 8905,     6,     0,     0,  2416, 10086],\n",
      "        [    3,     1,    29,   192,    48,    53],\n",
      "        [   68,   809,  1332,  1352,     5,   194],\n",
      "        [ 2001, 29999,   156,     3,     8,    12],\n",
      "        [ 2939,     1,     0,  8465,  1905,  3702],\n",
      "        [29999,     6,   157,     0, 29999,     2],\n",
      "        [  618,  1067,     6, 29999,     2,     0],\n",
      "        [  833,     2,     0,   578,   476,    84],\n",
      "        [    3,     9,    20,   163,   297,  4841],\n",
      "        [   98,  9066, 13903,   593,    14,  1215],\n",
      "        [10179,  2850,  2850,  5352,  8093,  2633],\n",
      "        [    2,   319,     1,     4,  2378,  2643],\n",
      "        [    6,  2329,   407,     0,   297,   712],\n",
      "        [    2,  1614,  3949,    49,  2709,     2],\n",
      "        [   33,  1273,  3280,    17,     5,  6312],\n",
      "        [13942,   414,     3,     8,     9,   272],\n",
      "        [    0,  1117,   187,     0,  1289,   119],\n",
      "        [    0,     3,     8,    20,   316,  5969],\n",
      "        [    5,   638,   112, 20209,    83,     2],\n",
      "        [  297,  3321,     6,     4,  8308,   568],\n",
      "        [  258,    15,     2,  1992,   108,    34],\n",
      "        [  581,    19,  2750,     4,   200,     6],\n",
      "        [ 1315,    60, 29999,  1616,     1,     0],\n",
      "        [    3,    28,    16,    15,  1721,  2953],\n",
      "        [   86,    11,     0,   125,  1000,   627],\n",
      "        [  143,     1,   116,  1278,  6412,     2],\n",
      "        [ 2830,    11,   110,     5,   165,  4198],\n",
      "        [  510,     0,   211,  9240,     0,   121],\n",
      "        [  871, 20541,     0,   520,    33,    48],\n",
      "        [   49,   985,   616,     0,   333,   558],\n",
      "        [ 5553,     5, 12621,  1668,  2464,  1887],\n",
      "        [ 3242,    24,   977,     3,    12,    16],\n",
      "        [   44, 23556,     1,   908,    23,  2875],\n",
      "        [  131,    62,    93,     1,     0, 27863],\n",
      "        [ 1516,    51,    31,    24,     0,   874],\n",
      "        [ 1585,  3276,     2,   291,    69,     5],\n",
      "        [   19,  2489,   471,    74,   662,   457],\n",
      "        [29999, 29999, 11838, 17306,   296,     6],\n",
      "        [  247, 13549, 19262,   129,  4781,    19],\n",
      "        [   18,   222,  2945,     0,    95,  3371],\n",
      "        [  215,    55, 25087,  1894,    32, 11057],\n",
      "        [   17,   635, 29999,     0, 13284, 29999],\n",
      "        [    9,  1274,     9,  1055,     1, 10048],\n",
      "        [   79,     6,   490,     3,     8,    15]]) \n",
      "neg_words tensor([[ 3407,   845,  1655,  ...,     0,   610,  3018],\n",
      "        [ 6567,  1122,  1380,  ...,   381,   617,    25],\n",
      "        [12926,  6395,   729,  ...,    35,  4157,    15],\n",
      "        ...,\n",
      "        [26049,   154,  7803,  ...,   353,     1, 13433],\n",
      "        [ 9754,  3714, 16274,  ...,   890,   491,  8624],\n",
      "        [    7,   887,   815,  ..., 10025,     4,  3783]])\n"
     ]
    }
   ],
   "source": [
    "for i, (center_word,pos_words,neg_words) in enumerate(dataloader):\n",
    "    print(\"center_word:\",center_word,\"\\n\",\n",
    "          \"pos_words:\",pos_words,\"\\n\"\n",
    "          \"neg_words\",neg_words)\n",
    "    if i > 0:\n",
    "        break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 定义Pytroch模型\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_size):\n",
    "        super(EmbeddingModel,self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        self.in_embed = nn.Embedding(self.vocab_size, self.embed_size)\n",
    "        self.out_embed = nn.Embedding(self.embed_size,self.embed_size)\n",
    "        \n",
    "        # 这里可以对初始权重进行归一化处理\n",
    "#         initrange = 0.5/ self.embed_size\n",
    "#         self.in_embed.weight.data.uniform_(-initrange,initrange)\n",
    "        \n",
    "    def forward(self,input_labels,pos_labels,neg_labels):\n",
    "        # input_label: [batch_size]\n",
    "        # pos_labels: [batch_size,(window_size * 2)] # 前后各区C个所以要*2，但是越界的时候不一定\n",
    "        # neg_labels: [batch_size,(window_size * 2 * K)] # 每个正样本对应K个负样本\n",
    "        \n",
    "        input_embedding = self.in_embed(input_labels) # [batch_size,embed_size]\n",
    "        pos_embedding = self.in_embed(pos_labels) # [batch_size,(window_size * 2),embed_size]\n",
    "        neg_embedding = self.in_embed(neg_labels) # [batch_size,(window_size * 2 * K),embed_size]\n",
    "        \n",
    "        # 重中之重就是如何定义LossFunction：\n",
    "        # LossFunction要满足越是我们期望的结果值越小，越是我们不期望的结果值越大\n",
    "        # 我们期望目标词和正样本相似度要高：那么当计算出目标词和正样本相似度越高，代价函数值就要越小，反之越大\n",
    "        # 我们不希望目标词和负样本相似度高：当计算出目标词语负样本相似度越高，代价值就要越高，反之越小\n",
    "        # 我们期望目标词语正样本的相似度要高于与负样本的相似度\n",
    "        \n",
    "        # 第一步我们先计算相似度：相似度采用余弦相似度的简易版即为向量內积来作为相似度计算的公式\n",
    "        # 因为维度不统一：input_embedding矩阵的秩为2，而pos_embedding和neg_embedding的矩阵的秩为三\n",
    "        # 所以先对input_embedding做升维操作unsqueeze;squeeze是压紧，unsqueeze可以理解为展开\n",
    "        input_embedding = input_embedding.unsqueeze(2) # [batch_size,embed_size,1]\n",
    "        \n",
    "        \n",
    "        #这里要用到一个矩阵运行方法：https://pytorch.org/docs/stable/torch.html\n",
    "        # torch.bmm(input, mat2, out=None)\n",
    "        # If input is a (b \\times n \\times m)(b×n×m) tensor, \n",
    "        # mat2 is a (b \\times m \\times p)(b×m×p) tensor, \n",
    "        # out will be a (b \\times n \\times p)(b×n×p) tensor.\n",
    "        #         >>> input = torch.randn(10, 3, 4)\n",
    "        #         >>> mat2 = torch.randn(10, 4, 5)\n",
    "        #         >>> res = torch.bmm(input, mat2)\n",
    "        #         >>> res.size()\n",
    "        #         torch.Size([10, 3, 5])\n",
    "        \n",
    "        # 计算目标词与正样本的相似度：\n",
    "        # 计算完[batch_size,(window_size * 2),1],然后用squeeze把最后一维消掉\n",
    "        pos_dot = torch.bmm(pos_embedding,input_embedding).squeeze(2)#[batch_size,(window_size * 2)]\n",
    "        # 用SIGMOD激活后取值变为0-1就可以直接当做相似度值，但是作为loss这个值不合适\n",
    "        # 所以在用一个指数函数log来去定义损失值，在x取值0-1时，logx的取值是-无穷到0\n",
    "        # 对于正样本x越接近1说明越相似，logx的值越大，-logx的值越小，-logx就可以作为损失函数，\n",
    "        # 本质上与sigmoid的二分类损失函数的定义还有有一定相似度的，值不过这里不在依赖标签值了(用文本出现的接近程度)\n",
    "        log_pos = F.logsigmoid(pos_dot).sum(1) # 第二维度的是所有正样本，所以要在第二个维度上做加和操作\n",
    "        \n",
    "        # 同理计算目标词语负样本的相似度\n",
    "        # 注意这里的是-input_embedding，因为正样本需要越相似越好，负样本需要越不相似越好\n",
    "        neg_dot = torch.bmm(neg_embedding,-input_embedding).squeeze(2)#[batch_size,(window_size * 2 * K)]\n",
    "        log_neg = F.logsigmoid(neg_dot).sum(1)\n",
    "        \n",
    "        loss = -(log_pos +log_neg) # 最后别忘了加上负号\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def input_embeddings(self):\n",
    "        return self.in_embed.weight.data.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "model = EmbeddingModel(VOCAB_SIZE,EMBEDDING_SIZE)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30000, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.3948246 , -1.3977673 ,  0.582277  , ...,  1.0021156 ,\n",
       "         0.3033375 ,  0.9187573 ],\n",
       "       [ 1.2000518 , -1.6536685 , -0.5187361 , ...,  1.5897415 ,\n",
       "        -1.7625477 , -2.0025969 ],\n",
       "       [-1.334339  ,  0.32504746, -0.08529827, ..., -1.4065667 ,\n",
       "         0.84071934,  0.58231497],\n",
       "       ...,\n",
       "       [ 0.09752222, -0.23254772,  0.60725886, ..., -0.8021786 ,\n",
       "        -1.4678984 , -0.80487645],\n",
       "       [ 0.3784622 ,  1.4795929 ,  0.39129415, ..., -0.34258682,\n",
       "         1.310756  ,  0.763132  ],\n",
       "       [-2.5029325 ,  1.6724156 , -1.7058324 , ...,  1.409471  ,\n",
       "        -0.16565372, -0.65119296]], dtype=float32)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.in_embed.weight.shape)\n",
    "model.in_embed.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 iteration 0 2587.443603515625\n",
      "epoch 0 iteration 100 1204.2564697265625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-5dba68d58482>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# 进行反向传播\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# 进行梯度更新\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 定义优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "\n",
    "# 开始进行训练\n",
    "for e in range(NUM_EPOCHS):\n",
    "    for i,(input_labels,pos_labels,neg_labels) in enumerate(dataloader):\n",
    "#         print(\"center_word:\",center_word,\"\\n\",\n",
    "#           \"pos_words:\",pos_words,\"\\n\"\n",
    "#           \"neg_words\",neg_words)\n",
    "#         if i > 0:\n",
    "#             break;\n",
    "        input_labels = input_labels.long()\n",
    "        pos_labels = pos_labels.long()\n",
    "        neg_labels = neg_labels.long()\n",
    "        \n",
    "        #在训练数据之前先把grad清0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 定义loss函数\n",
    "        loss = model(input_labels,pos_labels,neg_labels).mean()\n",
    "        \n",
    "        # 进行反向传播\n",
    "        loss.backward()\n",
    "        \n",
    "        # 进行梯度更新\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(\"epoch\",e,\"iteration\",i,loss.item())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 下面是评估模型的代码，已经训练模型的代码 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights = model.input_embeddings()\n",
    "def evaluate(filename,embedding_weights):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        data = pd.read_csv(filename,sep=\",\")\n",
    "    else:\n",
    "        data = pd.read_csv(filename,sep=\"\\t\")\n",
    "    \n",
    "    human_similarity = []\n",
    "    model_similarity = []\n",
    "    for i in data.iloc[:, 0:2].index:\n",
    "        word1, word2 = data.iloc[i, 0], data.iloc[i, 1]\n",
    "        if word1 not in word_to_index or word2 not in word_to_index:\n",
    "            continue\n",
    "        else:\n",
    "            word1_idx, word2_idx = word_to_index[word1],word_to_index[word2]\n",
    "            # 利用训练返回的embedding_weights俩进行单词1和单词2的词向量表征\n",
    "            word1_embed,word2_embed = embedding_weights[[word1_idx]],embedding_weights[[word2_idx]]\n",
    "            #利用sklearn.metrics.pairwise.cosine_similarity来计算两个向量的余弦相似度\n",
    "            model_similarity.append(float(sklearn.metrics.pairwise.cosine_similarity(word1_embed,word2_embed)))\n",
    "            human_similarity.append(float(data.iloc[i,2]))\n",
    "        \n",
    "        return scipy.stats.spearmanr(human_similarity,model_similarity)\n",
    "    \n",
    "def find_nearest(word):\n",
    "    index = word_to_index[word]\n",
    "    embedding = embedding_weights[index]\n",
    "    # 利用scipy.spatial.distance.cosine来找余弦相似度\n",
    "    cos_dis = np.array([scipy.spatial.distance.cosine(e,embedding) for e in embedding_weights])\n",
    "    return [index_to_word[i] for i in cos_dis.argsort()[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['of',\n",
       " 'lamp',\n",
       " 'tracked',\n",
       " 'broca',\n",
       " 'senatorial',\n",
       " 'transparency',\n",
       " 'update',\n",
       " 'regression',\n",
       " 'forecasting',\n",
       " 'lobbied']"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest(\"of\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
