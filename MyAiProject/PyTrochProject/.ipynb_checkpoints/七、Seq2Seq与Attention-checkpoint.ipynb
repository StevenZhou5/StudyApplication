{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq, Attention\n",
    "\n",
    "\n",
    "在这份notebook当中，我们会(尽可能)复现Luong的attention模型\n",
    "\n",
    "由于我们的数据集非常小，只有一万多个句子的训练数据，所以训练出来的模型效果并不好。如果大家想训练一个好一点的模型，可以参考下面的资料。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 更多阅读\n",
    "\n",
    "#### 课件\n",
    "- [cs224d](http://cs224d.stanford.edu/lectures/CS224d-Lecture15.pdf)\n",
    "\n",
    "\n",
    "#### 论文\n",
    "- [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)\n",
    "- [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025?context=cs)\n",
    "- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1406.1078)\n",
    "\n",
    "\n",
    "#### PyTorch代码\n",
    "- [seq2seq-tutorial](https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb)\n",
    "- [Tutorial from Ben Trevett](https://github.com/bentrevett/pytorch-seq2seq)\n",
    "- [IBM seq2seq](https://github.com/IBM/pytorch-seq2seq)\n",
    "- [OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py)\n",
    "\n",
    "\n",
    "#### 更多关于Machine Translation\n",
    "- [Beam Search](https://www.coursera.org/lecture/nlp-sequence-models/beam-search-4EtHZ)\n",
    "- Pointer network 文本摘要\n",
    "- Copy Mechanism 文本摘要\n",
    "- Converage Loss \n",
    "- ConvSeq2Seq\n",
    "- Transformer\n",
    "- Tensor2Tensor\n",
    "\n",
    "#### TODO\n",
    "- 建议同学尝试对中文进行分词\n",
    "\n",
    "#### NER\n",
    "- https://github.com/allenai/allennlp/tree/master/allennlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "from collections import Counter #计数器\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import nltk\n",
    "import jieba # 一个中文分词插件jieba.cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mpunkt\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "ls /usr/local/share/nltk_data/tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "日本\n",
      "有\n",
      "很多\n",
      "温泉\n"
     ]
    }
   ],
   "source": [
    "for word in jieba.cut(\"日本有很多温泉\"): # 这个是用jieba做中文分词\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读入中英文数据\n",
    "- 英文我们使用nltk的word tokenizer来分词，并且使用小写字母\n",
    "- 中文我们直接使用单个汉字作为基本单元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "Today is sunday.\t今天是星期天。\n",
      "\n",
      "['Today is sunday.', '今天是星期天。']\n",
      "Today is sunday.\t今天是星期天。\n",
      "\n",
      "['Today is sunday.', '今天是星期天。']\n",
      "Today is sunday.\t今天是星期天。\n",
      "\n",
      "['Today is sunday.', '今天是星期天。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n",
      "There are a lot of hot springs in Japan.\t日本有很多温泉。\n",
      "['There are a lot of hot springs in Japan.', '日本有很多温泉。']\n"
     ]
    }
   ],
   "source": [
    "def load_data(in_file):\n",
    "    en = [] # 英文\n",
    "    cn = [] # 中文\n",
    "    num_examples = 0\n",
    "    with open(in_file,mode='r') as f: # 逐行读出\n",
    "        for line in f:\n",
    "            print(line) # 把每一环的数据打印出来看一下\n",
    "            line = line.strip().split(\"\\t\") # 分词后用逗号隔开\n",
    "            print(line) # 把分词后的line数据打印出来\n",
    "            \n",
    "            # 在每句话开头加上\"BOS\"，结尾加上\"EOS\"\n",
    "            en.append([\"BOS\"] + nltk.word_tokenize(line[0].lower())+[\"EOS\"])\n",
    "            \n",
    "            # split chinese sentence into characters\n",
    "            # 分割中文句子：这里使用子来分割的，也可以用jieba.cut来进行词的分割\n",
    "#             cn.append([\"BOS\"]+[c for c in line[1]]+[\"EOS\"])\n",
    "            \n",
    "            # 这种使用jieba将中文分成词语\n",
    "            cn.append([\"BOS\"]+ [word for word in jieba.cut(line[1])]+[\"EOS\"])\n",
    "\n",
    "    return en,cn\n",
    "\n",
    "traing_file = \"/Users/zhenwuzhou/AiProject/data/nmt/en-cn/train.txt\"\n",
    "dev_file = \"/Users/zhenwuzhou/AiProject/data/nmt/en-cn/dev.txt\"\n",
    "train_en,train_cn = load_data(traing_file)\n",
    "dev_en,dev_cn = load_data(dev_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['BOS', 'there', 'are', 'a', 'lot', 'of', 'hot', 'springs', 'in', 'japan', '.', 'EOS'], ['BOS', 'there', 'are', 'a', 'lot', 'of', 'hot', 'springs', 'in', 'japan', '.', 'EOS'], ['BOS', 'there', 'are', 'a', 'lot', 'of', 'hot', 'springs', 'in', 'japan', '.', 'EOS'], ['BOS', 'there', 'are', 'a', 'lot', 'of', 'hot', 'springs', 'in', 'japan', '.', 'EOS'], ['BOS', 'there', 'are', 'a', 'lot', 'of', 'hot', 'springs', 'in', 'japan', '.', 'EOS'], ['BOS', 'there', 'are', 'a', 'lot', 'of', 'hot', 'springs', 'in', 'japan', '.', 'EOS'], ['BOS', 'there', 'are', 'a', 'lot', 'of', 'hot', 'springs', 'in', 'japan', '.', 'EOS'], ['BOS', 'there', 'are', 'a', 'lot', 'of', 'hot', 'springs', 'in', 'japan', '.', 'EOS'], ['BOS', 'there', 'are', 'a', 'lot', 'of', 'hot', 'springs', 'in', 'japan', '.', 'EOS'], ['BOS', 'there', 'are', 'a', 'lot', 'of', 'hot', 'springs', 'in', 'japan', '.', 'EOS']]\n"
     ]
    }
   ],
   "source": [
    "print(train_en[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['BOS', '日本', '有', '很多', '温泉', '。', 'EOS'], ['BOS', '日本', '有', '很多', '温泉', '。', 'EOS'], ['BOS', '日本', '有', '很多', '温泉', '。', 'EOS'], ['BOS', '日本', '有', '很多', '温泉', '。', 'EOS'], ['BOS', '日本', '有', '很多', '温泉', '。', 'EOS'], ['BOS', '日本', '有', '很多', '温泉', '。', 'EOS'], ['BOS', '日本', '有', '很多', '温泉', '。', 'EOS'], ['BOS', '日本', '有', '很多', '温泉', '。', 'EOS'], ['BOS', '日本', '有', '很多', '温泉', '。', 'EOS'], ['BOS', '日本', '有', '很多', '温泉', '。', 'EOS']]\n"
     ]
    }
   ],
   "source": [
    "print(train_cn[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建单词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "UNK_IDX = 0\n",
    "PAD_IDX = 1\n",
    "def build_dict(sentences, max_words=50000):\n",
    "    word_count = Counter()\n",
    "    for sentence in sentences:\n",
    "        for s in sentence:\n",
    "            word_count[s] += 1  #word_count这里应该是个字典\n",
    "    ls = word_count.most_common(max_words) \n",
    "    #按每个单词数量排序前50000个,这个数字自己定的，不重复单词数没有50000\n",
    "    print(len(ls)) #train_en：5491\n",
    "    total_words = len(ls) + 2\n",
    "    #加的2是留给\"unk\"和\"pad\"\n",
    "    #ls = [('BOS', 14533), ('EOS', 14533), ('.', 12521), ('i', 4045), .......\n",
    "    word_dict = {w[0]: index+2 for index, w in enumerate(ls)}\n",
    "    #加的2是留给\"unk\"和\"pad\",转换成字典格式。\n",
    "    word_dict[\"UNK\"] = UNK_IDX\n",
    "    word_dict[\"PAD\"] = PAD_IDX\n",
    "    return word_dict, total_words\n",
    "\n",
    "en_dict, en_total_words = build_dict(train_en)\n",
    "cn_dict, cn_total_words = build_dict(train_cn)\n",
    "inv_en_dict = {v: k for k, v in en_dict.items()}\n",
    "#en_dict.items()把字典转换成可迭代对象，取出键值，并调换键值的位置。\n",
    "inv_cn_dict = {v: k for k, v in cn_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BOS': 2, 'there': 3, 'are': 4, 'a': 5, 'lot': 6, 'of': 7, 'hot': 8, 'springs': 9, 'in': 10, 'japan': 11, '.': 12, 'EOS': 13, 'UNK': 0, 'PAD': 1}\n",
      "{2: 'BOS', 3: 'there', 4: 'are', 5: 'a', 6: 'lot', 7: 'of', 8: 'hot', 9: 'springs', 10: 'in', 11: 'japan', 12: '.', 13: 'EOS', 0: 'UNK', 1: 'PAD'}\n"
     ]
    }
   ],
   "source": [
    "print(en_dict)\n",
    "print(inv_en_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BOS': 2, '日本': 3, '有': 4, '很多': 5, '温泉': 6, '。': 7, 'EOS': 8, 'UNK': 0, 'PAD': 1}\n",
      "{2: 'BOS', 3: '日本', 4: '有', 5: '很多', 6: '温泉', 7: '。', 8: 'EOS', 0: 'UNK', 1: 'PAD'}\n"
     ]
    }
   ],
   "source": [
    "print(cn_dict)\n",
    "print(inv_cn_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 把单词全部转变成数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(en_sentences, cn_sentences, en_dict, cn_dict, sort_by_len=True):\n",
    "    '''\n",
    "        Encode the sequences. \n",
    "    '''\n",
    "    length = len(en_sentences)\n",
    "    #en_sentences=[['BOS', 'anyone', 'can', 'do', 'that', '.', 'EOS'],....\n",
    "    \n",
    "    out_en_sentences = [[en_dict.get(w, 0) for w in sent] for sent in en_sentences]\n",
    "    #out_en_sentences=[[2, 328, 43, 14, 28, 4, 3], ....\n",
    "    #.get(w, 0)，返回w对应的值，没有就为0.因题库比较小，这里所有的单词向量都有非零索引。\n",
    "    \n",
    " \n",
    "    out_cn_sentences = [[cn_dict.get(w, 0) for w in sent] for sent in cn_sentences]\n",
    "\n",
    "    # sort sentences by english lengths\n",
    "    def len_argsort(seq):\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "      #sorted()排序,key参数可以自定义规则，按seq[x]的长度排序，seq[0]为第一句话长度\n",
    "       \n",
    "    # 把中文和英文按照同样的顺序排序\n",
    "    if sort_by_len:\n",
    "        sorted_index = len_argsort(out_en_sentences)\n",
    "    #print(sorted_index)\n",
    "    #sorted_index=[63, 1544, 1917, 2650, 3998, 6240, 6294, 6703, ....\n",
    "     #前面的索引都是最短句子的索引\n",
    "      \n",
    "        out_en_sentences = [out_en_sentences[i] for i in sorted_index]\n",
    "     #print(out_en_sentences)\n",
    "     #out_en_sentences=[[2, 475, 4, 3], [2, 1318, 126, 3], [2, 1707, 126, 3], ......\n",
    "     \n",
    "        out_cn_sentences = [out_cn_sentences[i] for i in sorted_index]\n",
    "        \n",
    "    return out_en_sentences, out_cn_sentences\n",
    "\n",
    "train_en, train_cn = encode(train_en, train_cn, en_dict, cn_dict)\n",
    "dev_en, dev_cn = encode(dev_en, dev_cn, en_dict, cn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]]\n"
     ]
    }
   ],
   "source": [
    "print(train_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8]]\n"
     ]
    }
   ],
   "source": [
    "print(train_cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS 日本 有 很多 温泉 。 EOS\n",
      "BOS there are a lot of hot springs in japan . EOS\n"
     ]
    }
   ],
   "source": [
    "k=10\n",
    "print(\" \".join([inv_cn_dict[i] for i in train_cn[k]])) #通过inv字典获取单词\n",
    "print(\" \".join([inv_en_dict[i] for i in train_en[k]])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 把全部的句子分成batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatches(n, minibatch_size, shuffle=True):\n",
    "    idx_list = np.arange(0, n, minibatch_size) # [0, 1, ..., n-1]\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list) #打乱数据\n",
    "    minibatches = []\n",
    "    for idx in idx_list:\n",
    "        minibatches.append(np.arange(idx, min(idx + minibatch_size, n)))\n",
    "        #所有batch放在一个大列表里\n",
    "    return minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]),\n",
       " array([90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
       " array([30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44]),\n",
       " array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14]),\n",
       " array([45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]),\n",
       " array([60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]),\n",
       " array([75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89])]"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 15 个一组，从0-99 分成7个组，每组里面句子顺序打乱\n",
    "get_minibatches(100,15) #随机打乱的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 把数据预处理成最终需要的训练和测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(seqs):\n",
    "#seqs=[[2, 12, 167, 23, 114, 5, 27, 1755, 4, 3], ........\n",
    "    lengths = [len(seq) for seq in seqs]#每个batch里语句的长度统计出来\n",
    "    n_samples = len(seqs) #一个batch有多少语句\n",
    "    max_len = np.max(lengths) #取出最长的的语句长度，后面用这个做padding基准\n",
    "    x = np.zeros((n_samples, max_len)).astype('int32')\n",
    "    #先初始化全零矩阵，后面依次赋值\n",
    "    #print(x.shape) #64*最大句子长度\n",
    "    \n",
    "    x_lengths = np.array(lengths).astype(\"int32\")\n",
    "    #print(x_lengths) \n",
    "#这里看下面的输入语句发现英文句子长度都一样，中文句子长短不一。\n",
    "#说明英文句子是特征，中文句子是标签。\n",
    "\n",
    "\n",
    "    for idx, seq in enumerate(seqs):\n",
    "      #取出一个batch的每条语句和对应的索引\n",
    "        x[idx, :lengths[idx]] = seq\n",
    "        #每条语句按行赋值给x，x会有一些零值没有被赋值。\n",
    "        \n",
    "    return x, x_lengths #x_mask\n",
    "\n",
    "def gen_examples(en_sentences, cn_sentences, batch_size):\n",
    "    minibatches = get_minibatches(len(en_sentences), batch_size)\n",
    "    all_ex = []\n",
    "    for minibatch in minibatches:\n",
    "        mb_en_sentences = [en_sentences[t] for t in minibatch]\n",
    "#按打乱的batch序号分数据，打乱只是batch打乱，一个batach里面的语句还是顺序的。\n",
    "        #print(mb_en_sentences)\n",
    "        \n",
    "        mb_cn_sentences = [cn_sentences[t] for t in minibatch]\n",
    "        mb_x, mb_x_len = prepare_data(mb_en_sentences)\n",
    "        #返回的维度为：mb_x=(64 * 最大句子长度）,mb_x_len=最大句子长度\n",
    "        mb_y, mb_y_len = prepare_data(mb_cn_sentences)\n",
    "        \n",
    "        all_ex.append((mb_x, mb_x_len, mb_y, mb_y_len))\n",
    "  #这里把所有batch数据集合到一起。\n",
    "  #依次为英文句子，英文长度，中文句子翻译，中文句子长度，这四个放在一个列表中\n",
    "  #一个列表为一个batch的数据，所有batch组成一个大列表数据\n",
    "  \n",
    "        \n",
    "    return all_ex\n",
    "\n",
    "batch_size = 64\n",
    "train_data = gen_examples(train_en, train_cn, batch_size)\n",
    "random.shuffle(train_data)\n",
    "dev_data = gen_examples(dev_en, dev_cn, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13]], dtype=int32),\n",
       " array([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "        12, 12, 12, 12, 12, 12], dtype=int32),\n",
       " array([[2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8],\n",
       "        [2, 3, 4, 5, 6, 7, 8]], dtype=int32),\n",
       " array([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7], dtype=int32))"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看一下训练数据:一组里面是64个句子\n",
    "train_data[0] # 第一个array是英文句子，第二个array是英文句子的长度；第三,四array分别是中文句子和其长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 没有Attention的版本\n",
    "下面是一个没有Attention的encoder decoder模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义Encoder模型\n",
    "class PlainEncoder(nn.Module):\n",
    "    def __init__(self,vocab_size,hidden_size,drop_out=0.2):\n",
    "        # 模型的输入需要需要encode的语言的vocab_size,hidden_size,drop_out\n",
    "        # hidden_size，和drop_out都根据网络框架定义，\n",
    "        #以英文为例，vocab_size=5493, hidden_size=100, dropout=0.2\n",
    "        super(PlainEncoder,self).__init__()\n",
    "        \n",
    "        # 第一步先进行Embed操作\n",
    "        self.embed = nn.Embedding(vocab_size,hidden_size)\n",
    "        \n",
    "        # 第二步为了进行drop_out操作\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        \n",
    "        # 第三步进行Rnn训练\n",
    "        # batch_first=True 可以把batch_size移动到第一个维度\n",
    "        # 第一个参是输入特征数量，第二个参数是输出特征数量，这里输入=输出=hidden_size\n",
    "        self.rnn = nn.GRU(hidden_size,hidden_size,batch_first=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,x,lengths):\n",
    "        #x是输入的batch的所有单词，lengths：batch里每个句子的长度\n",
    "        #因为需要把最后一个hidden state取出来，需要知道长度，因为句子长度不一样\n",
    "        ##print(x.shape,lengths),x.sahpe = torch.Size([64, 10])\n",
    "        # lengths= =tensor([10, 10, 10, ..... 10, 10, 10])\n",
    "        \n",
    "        # 把Batch里面的seq按照长度排序;descending=True长的在前。\n",
    "        # 返回两个参数，句子长度和未排序前的索引\n",
    "        # sorted_idx=tensor([41, 40, 46, 45,...... 19, 18, 63])\n",
    "        # sorted_len=tensor([10, 10, 10, ..... 10, 10, 10])\n",
    "        sorted_len, sorted_idx = lengths.sort(0, descending=True)\n",
    "        \n",
    "        # 句子用新的idx，按长度排好序了\n",
    "        x_sorted = x[sorted_idx.long()]\n",
    "        \n",
    "        \n",
    "        embedded = self.dropout(self.embed(x_sorted))\n",
    "        #print(embedded.shape)=torch.Size([64, 10, 100])\n",
    "        #tensor([[[-0.6312, -0.9863, -0.3123,  ..., -0.7384,  0.9230, -0.4311],....\n",
    "\n",
    "        \n",
    "        \n",
    "        # 这个函数就是用来处理不同长度的句子的，https: // www.cnblogs.com / sbj123456789 / p / 9834018. html\n",
    "        # 因为句子在预处理的时候会用padding补全成相同长度，\n",
    "        # 但是我们如果补全后句子长度是100，实际句子长度是7，\n",
    "        # 那么我们想要的输出其实是真实句子长度7后的输出，而不是最后100的输出，并且也不希望去计算后面93词padding\n",
    "        # 所以这里使用pack_padded_sequence方法来处理\n",
    "        # 这个方法需要传入batch数据中每一个句子的真实长度,并且是排序好的（目前只这样）\n",
    "        # 注意我们之前定义的时候使用了batch_first=True，这里要保持一致\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded,\n",
    "                                                             sorted_len.long().cpu().data.numpy(),\n",
    "                                                             batch_first=True)\n",
    "        \n",
    "        packed_out,hid = self.rnn(packed_embedded)\n",
    "        #hid.shape = torch.Size([1, 64, 100])\n",
    "        \n",
    "        # 因为上面用pack_padded_sequence进行了处理，所以这里要用pad_packed_sequence进行处理\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        #out.shape = torch.Size([64, 10, 100]),\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 需要按照原来的idx来重写整理输出，不然返回的batch的句子和结果就对不上了\n",
    "        _, original_idx = sorted_idx.sort(0,descending=False)\n",
    "        out = out[original_idx.long()].contiguous()\n",
    "        hid = hid[:,original_idx.long()].contiguous()\n",
    "        #out.shape = torch.Size([64, 10, 100])\n",
    "        #hid.shape = torch.Size([1, 64, 100])\n",
    "        \n",
    "        return out,hid[[-1]] #有时候num_layers层数多，需要取出最后一层\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义decode模型\n",
    "class PlainDecoder(nn.Module):\n",
    "    def __init__(self,vocab_size,hidden_size,drou_out=0.2):\n",
    "        super(PlainDecoder,self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size,hidden_size)\n",
    "        self.drouout = nn.Dropout(drou_out)\n",
    "\n",
    "        \n",
    "        self.rnn = nn.GRU(hidden_size,hidden_size,batch_first=True)\n",
    "        \n",
    "        # 需要用全连接把hidde_size的结果转成vocabsize\n",
    "        self.out = nn.Linear(hidden_size,vocab_size)\n",
    "        \n",
    "    \n",
    "    def forward(self,y,y_lengths,hid):\n",
    "        #中文的y和y_lengths\n",
    "        #print(y.shape)=torch.Size([64, 12])\n",
    "        #print(hid.shape)=torch.Size([1, 64, 100])\n",
    "        \n",
    "        # 与encode类似，我们也需要做排序，pack_padded 和pad_pacded操作\n",
    "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)\n",
    "        y_sorted = y[sorted_idx.long()]\n",
    "        hid = hid[:, sorted_idx.long()] #隐藏层也要排序\n",
    "\n",
    "        y_embedded = self.drouout(self.embed(y_sorted))\n",
    "        # batch_size, output_length, embed_size\n",
    "        \n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(y_embedded,\n",
    "                                                            sorted_len.long().cpu().data.numpy(),\n",
    "                                                            batch_first=True)\n",
    "        packed_out,hid = self.rnn(packed_embedded,hid) # 加上隐藏层\n",
    "        #print(hid.shape)=torch.Size([1, 64, 100])\n",
    "        \n",
    "        # 因为上面用pack_padded_sequence进行了处理，所以这里要用pad_packed_sequence进行处理\n",
    "        out,_ = nn.utils.rnn.pad_packed_sequence(packed_out,batch_first=True)\n",
    "     \n",
    "        \n",
    "        # 需要按照原来的idx来重写整理输出，不然返回的batch的句子和结果就对不上了\n",
    "        _,original_idx = sorted_idx.sort(0,descending=False)\n",
    "        \n",
    "        output_seq =  out[original_idx.long()].contiguous()\n",
    "        #print(output_seq.shape)=torch.Size([64, 12, 100])\n",
    "        \n",
    "        hid = hid[:,original_idx.long()].contiguous()\n",
    "        #print(hid.shape)=torch.Size([1, 64, 100])\n",
    "        \n",
    "        output = F.log_softmax(self.out(output_seq),-1)\n",
    "        #print(output.shape)=torch.Size([64, 12, 3195])\n",
    "        \n",
    "        return output, hid\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainSeq2Seq(nn.Module):\n",
    "    def __init__(self,encoder,decoder):\n",
    "        #encoder是上面PlainEncoder的实例\n",
    "        #decoder是上面PlainDecoder的实例\n",
    "        super(PlainSeq2Seq,self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    # 把两个模型串起来\n",
    "    def forward(self,x,x_lengths,y,y_lengths):\n",
    "        # 先计算encode\n",
    "        encoder_out, hid = self.encoder(x,x_lengths)\n",
    "        #self.encoder(x, x_lengths)调用PlainEncoder里面forward的方法\n",
    "        #返回forward的out和hid\n",
    "        \n",
    "        decoder_out,hid = self.decoder(y,y_lengths,hid=hid)\n",
    "        \n",
    "        return decoder_out,None\n",
    "        \n",
    "    def translate(self,x,x_lengths,y,max_length=10):\n",
    "        # x是一个句子，用字典中的onehot的数字表示\n",
    "        # x_lengths是句子的长度\n",
    "        # y是“bos”的数值索引=2\n",
    "        \n",
    "        encoder_out, hid = self.encoder(x,x_lengths)\n",
    "        preds = []\n",
    "        batch_size = x.shape[0]\n",
    "        attns = []\n",
    "        for i in range(max_length):\n",
    "            output, hid = self.decoder(y=y,\n",
    "                    y_lengths=torch.ones(batch_size).long().to(y.device),\n",
    "                    hid=hid) \n",
    "            \n",
    "#刚开始循环bos作为模型的首个输入单词，后续更新y，下个预测单词的输入是上个输出单词\n",
    "            y = output.max(2)[1].view(batch_size, 1)\n",
    "            preds.append(y)\n",
    "        \n",
    "        return torch.cat(preds,1), None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "drop_out = 0.2\n",
    "hidden_size = 100\n",
    "\n",
    "# 传入对应参数初始化模型\n",
    "encoder = PlainEncoder(vocab_size=en_total_words,\n",
    "                       hidden_size=hidden_size,\n",
    "                       drop_out=drop_out)\n",
    "decoder = PlainDecoder(vocab_size=cn_total_words,\n",
    "                       hidden_size=hidden_size,\n",
    "                       drou_out=drop_out)\n",
    "model = PlainSeq2Seq(encoder,decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义seq2seq序列模型的交叉熵随时函数\n",
    "class LanguageModelCriterion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LanguageModelCriterion,self).__init__()\n",
    "    \n",
    "    def forward(self,input,target,mask):\n",
    "        # 这里的mask是每个句子都会有一部分遮挡，这个loss函数就是要把遮挡部分的loss忽略掉\n",
    "        # target=tensor([[5,108,8,4,3,0,0,0,0,0,0,0],....\n",
    "        # mask=tensor([[1,1 ,1,1,1,0,0,0,0,0,0,0],.....\n",
    "        # print(input.shape,target.shape,mask.shape)\n",
    "        # torch.Size([64, 12, 3195]) torch.Size([64, 12]) torch.Size([64, 12])\n",
    "        \n",
    "        # input: [batch_size * seq_len]*vocab_size\n",
    "#         print(input.shape)\n",
    "#         print(input)\n",
    "        input = input.contiguous().view(-1,input.size(-1)) # 这里相当于把前面两维整合到了一起\n",
    "        \n",
    "        # target [batch_size * 1=768*1]\n",
    "        target = target.contiguous().view(-1,1)\n",
    "        mask = mask.contiguous().view(-1,1)\n",
    "        # print(-input.gather(1,target))\n",
    "        output = -input.gather(1, target) * mask\n",
    "        #这里算得就是交叉熵损失，前面已经算了F.log_softmax\n",
    "        #.gather的作用https://blog.csdn.net/edogawachia/article/details/80515038\n",
    "        #output.shape=torch.Size([768, 1])\n",
    "        #mask作用是把padding为0的地方重置为零，因为input.gather时，为0的地方不是零了\n",
    "        \n",
    "        #均值损失\n",
    "        output = torch.sum(output) / torch.sum(mask)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5073, 0.6437],\n",
       "         [0.0574, 0.6901],\n",
       "         [0.6368, 0.5890]],\n",
       "\n",
       "        [[0.1623, 0.3672],\n",
       "         [0.6350, 0.0481],\n",
       "         [0.4503, 0.3044]],\n",
       "\n",
       "        [[0.1689, 0.4535],\n",
       "         [0.5477, 0.9881],\n",
       "         [0.9722, 0.0390]],\n",
       "\n",
       "        [[0.8396, 0.7042],\n",
       "         [0.2090, 0.3169],\n",
       "         [0.0758, 0.8810]],\n",
       "\n",
       "        [[0.6250, 0.2300],\n",
       "         [0.0755, 0.1212],\n",
       "         [0.6610, 0.1891]]])"
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#构造一个随机初始化的矩阵：\n",
    "x = torch.rand(5,3,2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 2])\n",
      "torch.Size([5, 3, 2])\n",
      "torch.Size([15, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5073, 0.6437],\n",
       "        [0.0574, 0.6901],\n",
       "        [0.6368, 0.5890],\n",
       "        [0.1623, 0.3672],\n",
       "        [0.6350, 0.0481],\n",
       "        [0.4503, 0.3044],\n",
       "        [0.1689, 0.4535],\n",
       "        [0.5477, 0.9881],\n",
       "        [0.9722, 0.0390],\n",
       "        [0.8396, 0.7042],\n",
       "        [0.2090, 0.3169],\n",
       "        [0.0758, 0.8810],\n",
       "        [0.6250, 0.2300],\n",
       "        [0.0755, 0.1212],\n",
       "        [0.6610, 0.1891]])"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(x.contiguous().shape)\n",
    "y = x.contiguous().view(-1,x.size(2))\n",
    "print(y.shape)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9722, 0.1772, 0.9262],\n",
       "        [0.3455, 0.6602, 0.6919]])"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gather_test = torch.rand(2,3)\n",
    "gather_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3455, 0.6602, 0.6919]])"
      ]
     },
     "execution_count": 582,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_0 = torch.LongTensor([[1,1,1]])\n",
    "gather_test.gather(0,index_0) # 取出每一列的1索引位置(即第二列)的值输出成一个新的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1772],\n",
       "        [-0.6919]])"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_1 = torch.LongTensor([[1],  # 第一行的1代表one-hot词向量位置索引1的那个词，\n",
    "                            [2]] # 第二行的2代表one-hot词向量位置索引2的那个词，\n",
    "                          )\n",
    "-gather_test.gather(1,index_1) # 把每一行中对应索引的预测概率值取出来，取负数就可以当做loss函数的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "loss_fn = LanguageModelCriterion().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    total_num_words = total_loss = 0.\n",
    "    with torch.no_grad():#不需要更新模型，不需要梯度\n",
    "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
    "            mb_x = torch.from_numpy(mb_x).to(device).long()\n",
    "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
    "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
    "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
    "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()\n",
    "            mb_y_len[mb_y_len<=0] = 1\n",
    "\n",
    "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "\n",
    "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
    "            mb_out_mask = mb_out_mask.float()\n",
    "\n",
    "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "\n",
    "            num_words = torch.sum(mb_y_len).item()\n",
    "            total_loss += loss.item() * num_words\n",
    "            total_num_words += num_words\n",
    "    print(\"Evaluation loss\", total_loss/total_num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 iteration 0 loss 2.1663944721221924\n",
      "Epoch 0 Training loss 2.1663944721221924\n",
      "Evaluation loss 2.0161991119384766\n",
      "Epoch 1 iteration 0 loss 2.0190865993499756\n",
      "Epoch 1 Training loss 2.0190865993499756\n",
      "Epoch 2 iteration 0 loss 1.8618738651275635\n",
      "Epoch 2 Training loss 1.8618738651275635\n",
      "Epoch 3 iteration 0 loss 1.7167640924453735\n",
      "Epoch 3 Training loss 1.7167640924453735\n",
      "Epoch 4 iteration 0 loss 1.5836002826690674\n",
      "Epoch 4 Training loss 1.5836002826690674\n",
      "Epoch 5 iteration 0 loss 1.4538174867630005\n",
      "Epoch 5 Training loss 1.4538174867630005\n",
      "Evaluation loss 1.420767903327942\n",
      "Epoch 6 iteration 0 loss 1.3237935304641724\n",
      "Epoch 6 Training loss 1.3237935304641724\n",
      "Epoch 7 iteration 0 loss 1.215800404548645\n",
      "Epoch 7 Training loss 1.215800404548645\n",
      "Epoch 8 iteration 0 loss 1.1033129692077637\n",
      "Epoch 8 Training loss 1.1033129692077637\n",
      "Epoch 9 iteration 0 loss 0.9981317520141602\n",
      "Epoch 9 Training loss 0.9981317520141602\n",
      "Epoch 10 iteration 0 loss 0.9030351638793945\n",
      "Epoch 10 Training loss 0.9030351638793945\n",
      "Evaluation loss 0.9950231313705444\n",
      "Epoch 11 iteration 0 loss 0.8052956461906433\n",
      "Epoch 11 Training loss 0.8052956461906433\n",
      "Epoch 12 iteration 0 loss 0.7302730679512024\n",
      "Epoch 12 Training loss 0.7302730679512024\n",
      "Epoch 13 iteration 0 loss 0.6513870358467102\n",
      "Epoch 13 Training loss 0.6513870358467102\n",
      "Epoch 14 iteration 0 loss 0.5847833156585693\n",
      "Epoch 14 Training loss 0.5847833156585693\n",
      "Epoch 15 iteration 0 loss 0.5155750513076782\n",
      "Epoch 15 Training loss 0.5155750513076782\n",
      "Evaluation loss 0.7258344888687134\n",
      "Epoch 16 iteration 0 loss 0.45782187581062317\n",
      "Epoch 16 Training loss 0.45782187581062317\n",
      "Epoch 17 iteration 0 loss 0.4091911315917969\n",
      "Epoch 17 Training loss 0.4091911315917969\n",
      "Epoch 18 iteration 0 loss 0.36005836725234985\n",
      "Epoch 18 Training loss 0.36005836725234985\n",
      "Epoch 19 iteration 0 loss 0.3210974633693695\n",
      "Epoch 19 Training loss 0.3210974633693695\n",
      "Epoch 20 iteration 0 loss 0.28149905800819397\n",
      "Epoch 20 Training loss 0.28149905800819397\n",
      "Evaluation loss 0.5811206102371216\n",
      "Epoch 21 iteration 0 loss 0.2523212730884552\n",
      "Epoch 21 Training loss 0.2523212730884552\n",
      "Epoch 22 iteration 0 loss 0.22091053426265717\n",
      "Epoch 22 Training loss 0.22091053426265717\n",
      "Epoch 23 iteration 0 loss 0.1948268860578537\n",
      "Epoch 23 Training loss 0.1948268860578537\n",
      "Epoch 24 iteration 0 loss 0.17513920366764069\n",
      "Epoch 24 Training loss 0.17513920366764069\n",
      "Epoch 25 iteration 0 loss 0.15453438460826874\n",
      "Epoch 25 Training loss 0.15453438460826874\n",
      "Evaluation loss 0.5162891149520874\n",
      "Epoch 26 iteration 0 loss 0.13855519890785217\n",
      "Epoch 26 Training loss 0.13855519890785217\n",
      "Epoch 27 iteration 0 loss 0.1230015754699707\n",
      "Epoch 27 Training loss 0.1230015754699707\n",
      "Epoch 28 iteration 0 loss 0.10965175926685333\n",
      "Epoch 28 Training loss 0.10965175926685333\n",
      "Epoch 29 iteration 0 loss 0.09996739774942398\n",
      "Epoch 29 Training loss 0.09996739774942398\n",
      "Epoch 30 iteration 0 loss 0.09025765210390091\n",
      "Epoch 30 Training loss 0.09025765210390091\n",
      "Evaluation loss 0.4948394298553467\n",
      "Epoch 31 iteration 0 loss 0.07965756952762604\n",
      "Epoch 31 Training loss 0.07965756952762604\n",
      "Epoch 32 iteration 0 loss 0.07369336485862732\n",
      "Epoch 32 Training loss 0.07369336485862732\n",
      "Epoch 33 iteration 0 loss 0.06581252068281174\n",
      "Epoch 33 Training loss 0.06581252068281174\n",
      "Epoch 34 iteration 0 loss 0.06014321744441986\n",
      "Epoch 34 Training loss 0.06014321744441986\n",
      "Epoch 35 iteration 0 loss 0.055059950798749924\n",
      "Epoch 35 Training loss 0.055059950798749924\n",
      "Evaluation loss 0.49200379848480225\n",
      "Epoch 36 iteration 0 loss 0.051851581782102585\n",
      "Epoch 36 Training loss 0.051851581782102585\n",
      "Epoch 37 iteration 0 loss 0.04686174914240837\n",
      "Epoch 37 Training loss 0.04686174914240837\n",
      "Epoch 38 iteration 0 loss 0.04321181774139404\n",
      "Epoch 38 Training loss 0.04321181774139404\n",
      "Epoch 39 iteration 0 loss 0.041548024863004684\n",
      "Epoch 39 Training loss 0.041548024863004684\n",
      "Epoch 40 iteration 0 loss 0.037973370403051376\n",
      "Epoch 40 Training loss 0.037973370403051376\n",
      "Evaluation loss 0.4955865442752838\n",
      "Epoch 41 iteration 0 loss 0.035736050456762314\n",
      "Epoch 41 Training loss 0.035736050456762314\n",
      "Epoch 42 iteration 0 loss 0.032910797744989395\n",
      "Epoch 42 Training loss 0.032910797744989395\n",
      "Epoch 43 iteration 0 loss 0.031397946178913116\n",
      "Epoch 43 Training loss 0.031397946178913116\n",
      "Epoch 44 iteration 0 loss 0.02961546927690506\n",
      "Epoch 44 Training loss 0.02961546927690506\n",
      "Epoch 45 iteration 0 loss 0.028897901996970177\n",
      "Epoch 45 Training loss 0.028897901996970177\n",
      "Evaluation loss 0.5009530782699585\n",
      "Epoch 46 iteration 0 loss 0.026749154552817345\n",
      "Epoch 46 Training loss 0.026749154552817345\n",
      "Epoch 47 iteration 0 loss 0.025171255692839622\n",
      "Epoch 47 Training loss 0.025171255692839622\n",
      "Epoch 48 iteration 0 loss 0.023696230724453926\n",
      "Epoch 48 Training loss 0.023696230724453926\n",
      "Epoch 49 iteration 0 loss 0.023391690105199814\n",
      "Epoch 49 Training loss 0.023391690105199814\n",
      "Epoch 50 iteration 0 loss 0.022124486044049263\n",
      "Epoch 50 Training loss 0.022124486044049263\n",
      "Evaluation loss 0.5064628720283508\n",
      "Epoch 51 iteration 0 loss 0.02110118977725506\n",
      "Epoch 51 Training loss 0.02110118977725506\n",
      "Epoch 52 iteration 0 loss 0.020343344658613205\n",
      "Epoch 52 Training loss 0.020343344658613205\n",
      "Epoch 53 iteration 0 loss 0.019650885835289955\n",
      "Epoch 53 Training loss 0.019650885835289955\n",
      "Epoch 54 iteration 0 loss 0.019315723329782486\n",
      "Epoch 54 Training loss 0.019315723329782486\n",
      "Epoch 55 iteration 0 loss 0.01811050996184349\n",
      "Epoch 55 Training loss 0.01811050996184349\n",
      "Evaluation loss 0.5115238428115845\n",
      "Epoch 56 iteration 0 loss 0.017725463956594467\n",
      "Epoch 56 Training loss 0.017725463956594467\n",
      "Epoch 57 iteration 0 loss 0.016861628741025925\n",
      "Epoch 57 Training loss 0.016861628741025925\n",
      "Epoch 58 iteration 0 loss 0.016459280624985695\n",
      "Epoch 58 Training loss 0.016459280624985695\n",
      "Epoch 59 iteration 0 loss 0.016140548512339592\n",
      "Epoch 59 Training loss 0.016140548512339592\n",
      "Epoch 60 iteration 0 loss 0.015268202871084213\n",
      "Epoch 60 Training loss 0.015268202871084213\n",
      "Evaluation loss 0.5160873532295227\n",
      "Epoch 61 iteration 0 loss 0.014960815198719501\n",
      "Epoch 61 Training loss 0.014960815198719501\n",
      "Epoch 62 iteration 0 loss 0.014158251695334911\n",
      "Epoch 62 Training loss 0.014158251695334911\n",
      "Epoch 63 iteration 0 loss 0.014179049991071224\n",
      "Epoch 63 Training loss 0.014179049991071224\n",
      "Epoch 64 iteration 0 loss 0.013676324859261513\n",
      "Epoch 64 Training loss 0.013676324859261513\n",
      "Epoch 65 iteration 0 loss 0.013597813434898853\n",
      "Epoch 65 Training loss 0.013597813434898853\n",
      "Evaluation loss 0.5201724171638489\n",
      "Epoch 66 iteration 0 loss 0.013164250180125237\n",
      "Epoch 66 Training loss 0.013164250180125237\n",
      "Epoch 67 iteration 0 loss 0.012735320255160332\n",
      "Epoch 67 Training loss 0.012735320255160332\n",
      "Epoch 68 iteration 0 loss 0.0125746363773942\n",
      "Epoch 68 Training loss 0.0125746363773942\n",
      "Epoch 69 iteration 0 loss 0.01236816868185997\n",
      "Epoch 69 Training loss 0.01236816868185997\n",
      "Epoch 70 iteration 0 loss 0.01188221387565136\n",
      "Epoch 70 Training loss 0.01188221387565136\n",
      "Evaluation loss 0.5238525867462158\n",
      "Epoch 71 iteration 0 loss 0.011728132143616676\n",
      "Epoch 71 Training loss 0.011728132143616676\n",
      "Epoch 72 iteration 0 loss 0.01146156806498766\n",
      "Epoch 72 Training loss 0.01146156806498766\n",
      "Epoch 73 iteration 0 loss 0.01128152571618557\n",
      "Epoch 73 Training loss 0.01128152571618557\n",
      "Epoch 74 iteration 0 loss 0.011053639464080334\n",
      "Epoch 74 Training loss 0.011053639464080334\n",
      "Epoch 75 iteration 0 loss 0.010725738480687141\n",
      "Epoch 75 Training loss 0.010725738480687141\n",
      "Evaluation loss 0.5271949172019958\n",
      "Epoch 76 iteration 0 loss 0.01059312280267477\n",
      "Epoch 76 Training loss 0.01059312280267477\n",
      "Epoch 77 iteration 0 loss 0.010517788119614124\n",
      "Epoch 77 Training loss 0.010517788119614124\n",
      "Epoch 78 iteration 0 loss 0.01015465334057808\n",
      "Epoch 78 Training loss 0.01015465334057808\n",
      "Epoch 79 iteration 0 loss 0.010102738626301289\n",
      "Epoch 79 Training loss 0.010102738626301289\n",
      "Epoch 80 iteration 0 loss 0.00988942664116621\n",
      "Epoch 80 Training loss 0.00988942664116621\n",
      "Evaluation loss 0.5303131937980652\n",
      "Epoch 81 iteration 0 loss 0.00961798895150423\n",
      "Epoch 81 Training loss 0.00961798895150423\n",
      "Epoch 82 iteration 0 loss 0.009397586807608604\n",
      "Epoch 82 Training loss 0.009397586807608604\n",
      "Epoch 83 iteration 0 loss 0.009360951371490955\n",
      "Epoch 83 Training loss 0.009360951371490955\n",
      "Epoch 84 iteration 0 loss 0.008918780833482742\n",
      "Epoch 84 Training loss 0.008918780833482742\n",
      "Epoch 85 iteration 0 loss 0.009007120504975319\n",
      "Epoch 85 Training loss 0.009007120504975319\n",
      "Evaluation loss 0.533218264579773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 iteration 0 loss 0.008919456973671913\n",
      "Epoch 86 Training loss 0.008919456973671913\n",
      "Epoch 87 iteration 0 loss 0.00880465842783451\n",
      "Epoch 87 Training loss 0.00880465842783451\n",
      "Epoch 88 iteration 0 loss 0.008630175143480301\n",
      "Epoch 88 Training loss 0.008630175143480301\n",
      "Epoch 89 iteration 0 loss 0.008316387422382832\n",
      "Epoch 89 Training loss 0.008316387422382832\n",
      "Epoch 90 iteration 0 loss 0.008283020928502083\n",
      "Epoch 90 Training loss 0.008283020928502083\n",
      "Evaluation loss 0.5359717607498169\n",
      "Epoch 91 iteration 0 loss 0.008138339035212994\n",
      "Epoch 91 Training loss 0.008138339035212994\n",
      "Epoch 92 iteration 0 loss 0.008223367854952812\n",
      "Epoch 92 Training loss 0.008223367854952812\n",
      "Epoch 93 iteration 0 loss 0.0080807413905859\n",
      "Epoch 93 Training loss 0.0080807413905859\n",
      "Epoch 94 iteration 0 loss 0.00772683834657073\n",
      "Epoch 94 Training loss 0.00772683834657073\n",
      "Epoch 95 iteration 0 loss 0.007726415526121855\n",
      "Epoch 95 Training loss 0.007726415526121855\n",
      "Evaluation loss 0.5385764241218567\n",
      "Epoch 96 iteration 0 loss 0.007749893236905336\n",
      "Epoch 96 Training loss 0.007749893236905336\n",
      "Epoch 97 iteration 0 loss 0.007660554256290197\n",
      "Epoch 97 Training loss 0.007660554256290197\n",
      "Epoch 98 iteration 0 loss 0.007432766258716583\n",
      "Epoch 98 Training loss 0.007432766258716583\n",
      "Epoch 99 iteration 0 loss 0.0074606044217944145\n",
      "Epoch 99 Training loss 0.0074606044217944145\n"
     ]
    }
   ],
   "source": [
    "def train(model, data, num_epochs=2):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_num_words = total_loss = 0.\n",
    "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
    "            #（英文batch，英文长度，中文batch，中文长度）\n",
    "            \n",
    "            mb_x = torch.from_numpy(mb_x).to(device).long()\n",
    "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
    "            \n",
    "            #前n-1个单词作为输入，后n-1个单词作为输出，因为输入的前一个单词要预测后一个单词\n",
    "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
    "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
    "            #\n",
    "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()\n",
    "            #输入输出的长度都减一。\n",
    "            \n",
    "            mb_y_len[mb_y_len<=0] = 1\n",
    "            \n",
    "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "            #返回的是类PlainSeq2Seq里forward函数的两个返回值\n",
    "            \n",
    "            # 这个mask就是padding的位置设置为0，其他设置为1,如果\n",
    "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
    "            #mb_out_mask=tensor([[1, 1, 1,  ..., 0, 0, 0],[1, 1, 1,  ..., 0, 0, 0],\n",
    "            #mb_out_mask.shape= (64*19),这句代码咱不懂，这个mask就是padding的位置设置为0，其他设置为1\n",
    "            #mb_out_mask就是LanguageModelCriterion的传入参数mask。\n",
    "\n",
    "            mb_out_mask = mb_out_mask.float()\n",
    "            \n",
    "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "            \n",
    "            num_words = torch.sum(mb_y_len).item()\n",
    "            #一个batch里多少个单词\n",
    "            \n",
    "            total_loss += loss.item() * num_words\n",
    "            #总损失，loss计算的是均值损失，每个单词都是都有损失，所以乘以单词数\n",
    "            \n",
    "            total_num_words += num_words\n",
    "            #总单词数\n",
    "            \n",
    "            # 更新模型\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.)\n",
    "            #为了防止梯度过大，设置梯度的阈值\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            if it % 100 == 0:\n",
    "                print(\"Epoch\", epoch, \"iteration\", it, \"loss\", loss.item())\n",
    "\n",
    "                \n",
    "        print(\"Epoch\", epoch, \"Training loss\", total_loss/total_num_words)\n",
    "        if epoch % 5 == 0:\n",
    "            evaluate(model, dev_data) #评估模型\n",
    "train(model, train_data, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS UNK UNK UNK . EOS\n",
      "BOS UNK UNK UNK 。 EOS\n",
      "日本有很多温泉。\n",
      "\n",
      "BOS UNK UNK UNK . EOS\n",
      "BOS UNK UNK UNK 。 EOS\n",
      "日本有很多温泉。\n",
      "\n",
      "BOS there are a lot of hot springs in japan . EOS\n",
      "BOS 日本 有 很多 温泉 。 EOS\n",
      "日本有很多温泉。\n",
      "\n",
      "BOS there are a lot of hot springs in japan . EOS\n",
      "BOS 日本 有 很多 温泉 。 EOS\n",
      "日本有很多温泉。\n",
      "\n",
      "BOS there are a lot of hot springs in japan . EOS\n",
      "BOS 日本 有 很多 温泉 。 EOS\n",
      "日本有很多温泉。\n",
      "\n",
      "BOS there are a lot of hot springs in japan . EOS\n",
      "BOS 日本 有 很多 温泉 。 EOS\n",
      "日本有很多温泉。\n",
      "\n",
      "BOS there are a lot of hot springs in japan . EOS\n",
      "BOS 日本 有 很多 温泉 。 EOS\n",
      "日本有很多温泉。\n",
      "\n",
      "BOS there are a lot of hot springs in japan . EOS\n",
      "BOS 日本 有 很多 温泉 。 EOS\n",
      "日本有很多温泉。\n",
      "\n",
      "BOS there are a lot of hot springs in japan . EOS\n",
      "BOS 日本 有 很多 温泉 。 EOS\n",
      "日本有很多温泉。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#翻译个句子看看结果咋样\n",
    "def translate_dev(i):\n",
    "    #随便取出句子\n",
    "    en_sent = \" \".join([inv_en_dict[w] for w in dev_en[i]])\n",
    "    print(en_sent)\n",
    "    cn_sent = \" \".join([inv_cn_dict[w] for w in dev_cn[i]])\n",
    "    print(\"\".join(cn_sent))\n",
    "\n",
    "    mb_x = torch.from_numpy(np.array(dev_en[i]).reshape(1, -1)).long().to(device)\n",
    "    #把句子升维，并转换成tensor\n",
    "    \n",
    "    mb_x_len = torch.from_numpy(np.array([len(dev_en[i])])).long().to(device)\n",
    "    #取出句子长度，并转换成tensor\n",
    "    \n",
    "    bos = torch.Tensor([[cn_dict[\"BOS\"]]]).long().to(device)\n",
    "    #bos=tensor([[2]])\n",
    "\n",
    "    translation, attn = model.translate(mb_x, mb_x_len, bos)\n",
    "    #这里传入bos作为首个单词的输入\n",
    "    #translation=tensor([[ 8,  6, 11, 25, 22, 57, 10,  5,  6,  4]])\n",
    "    \n",
    "    translation = [inv_cn_dict[i] for i in translation.data.cpu().numpy().reshape(-1)]\n",
    "    trans = []\n",
    "    for word in translation:\n",
    "        if word != \"EOS\": # 把数值变成单词形式\n",
    "            trans.append(word) #\n",
    "        else:\n",
    "            break\n",
    "    print(\"\".join(trans))\n",
    "\n",
    "for i in range(1,10):\n",
    "    translate_dev(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下面实现Attention模式的seq2seq模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder\n",
    "- Encoder模型的任务是把输入文字传入embedding层和GRU层，转换成一些hidden states作为后续的context vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6,  6,  6, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "        12, 12])"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x_lengths = next(iter(dev_data))[1]\n",
    "test_x_lengths = torch.from_numpy(test_x_lengths).to(device).long()\n",
    "test_x_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,  6,\n",
       "         6,  6]),\n",
       "indices=tensor([11,  9, 18, 17, 16, 15, 14, 13, 12, 19, 10,  8,  7,  6,  5,  4,  3,  1,\n",
       "         2,  0]))"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sort_x = test_x_lengths.sort(0,descending=True)\n",
    "test_sort_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,  7,\n",
       "         7,  7])"
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sort_out = test_sort_x[0] + 1\n",
    "test_sort_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19, 17, 18, 16, 15, 14, 13, 12, 11,  1, 10,  0,  8,  7,  6,  5,  4,  3,\n",
       "         2,  9])"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, original_idx = test_sort_x[1].sort(0,descending=False)\n",
    "original_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7,  7,  7, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
       "        13, 13])"
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把顺序还原:按照original_idx.long()的顺序把test_sort_out的数据进行整理\n",
    "test_sort_out[original_idx.long()].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0123, 0.9678, 0.9676],\n",
       "         [0.0701, 0.3914, 0.9731]],\n",
       "\n",
       "        [[0.9625, 0.4149, 0.7018],\n",
       "         [0.9172, 0.9421, 0.8030]],\n",
       "\n",
       "        [[0.3468, 0.2552, 0.0889],\n",
       "         [0.7382, 0.8738, 0.6047]],\n",
       "\n",
       "        [[0.0547, 0.6712, 0.3401],\n",
       "         [0.5232, 0.7037, 0.6484]],\n",
       "\n",
       "        [[0.8720, 0.0330, 0.6181],\n",
       "         [0.1282, 0.9049, 0.7606]]])"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cat的测试\n",
    "torch_cat_test = torch.rand(5,2,3)\n",
    "torch_cat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0547, 0.6712, 0.3401],\n",
       "        [0.5232, 0.7037, 0.6484]])"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_cat_test[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0547, 0.6712, 0.3401],\n",
       "        [0.5232, 0.7037, 0.6484],\n",
       "        [0.8720, 0.0330, 0.6181],\n",
       "        [0.1282, 0.9049, 0.7606]])"
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([torch_cat_test[-2],torch_cat_test[-1]],dim=0) # 行增加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0547, 0.6712, 0.3401, 0.8720, 0.0330, 0.6181],\n",
       "        [0.5232, 0.7037, 0.6484, 0.1282, 0.9049, 0.7606]])"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([torch_cat_test[-2],torch_cat_test[-1]],dim=1) # 列增加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义双向循环的RNN单层encoder模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_size,encode_hidden_size,decode_hidden_size,drop_out):\n",
    "        super(Encoder,self).__init__()\n",
    "        # 先把onehot进行embedding\n",
    "        self.embed = nn.Embedding(vocab_size,embed_size)\n",
    "        \n",
    "        # 然后dropout\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        \n",
    "        # 双向循环的Rnn神经网络\n",
    "        self.rnn = nn.GRU(embed_size,encode_hidden_size,batch_first=True,\n",
    "                          bidirectional=True) \n",
    "        \n",
    "        # 最后转为全连接输出层\n",
    "        # 因为是双向神经网络，所以最后要把两个方向的encode_hidden_size链接在一起做一层全连接\n",
    "        # 最后转换成decode时需要的输入的维度\n",
    "        self.fc = nn.Linear(2*encode_hidden_size,decode_hidden_size)\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        # 先排序\n",
    "        sorted_len,sorted_idx = lengths.sort(0,descending=True)\n",
    "        x_sorted = x[sorted_idx.long()]\n",
    "        \n",
    "        # embedding并且dropout\n",
    "        embedded = self.dropout(self.embed(x_sorted))\n",
    "        \n",
    "        # pack_padded_sequence处理超出部分用padding补全的部分\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded,\n",
    "                                                            sorted_len.long().cpu().data.numpy(),\n",
    "                                                            batch_first=True)\n",
    "        # 进行rnn操作\n",
    "        packed_out , packed_hid = self.rnn(packed_embedded)\n",
    "        \n",
    "        # pad_packed_sequence处理packed_out\n",
    "        padded_out,_ = nn.utils.rnn.pad_packed_sequence(packed_out,batch_first=True)\n",
    "        \n",
    "        # 把padded_out和packed_hid顺序调回去\n",
    "        _,original_idx = sorted_idx.sort(0,descending=False)\n",
    "        out = padded_out[original_idx.long()].contiguous()\n",
    "        hid = packed_hid[:,original_idx.long()].contiguous()\n",
    "        \n",
    "        \n",
    "        # 因为是双向循环神经网络，所以还要把最后的输出进行连接起来\n",
    "        hid = torch.cat([hid[-2],hid[-1]],dim=1)\n",
    "        \n",
    "        # 最后进行全连接把hide转成decode时需要的shape\n",
    "        hid = torch.tanh(self.fc(hid)).unsqueeze(0)\n",
    "        \n",
    "        return out,hid \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Luong Attention\n",
    "- 根据context vectors和当前的输出hidden states，计算输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,encoder_hidden_size,decode_hidden_size):\n",
    "        super(Attention,self).__init__()\n",
    "        \n",
    "        self.encoder_hidden_size = encoder_hidden_size\n",
    "        self.decode_hidden_size = decode_hidden_size\n",
    "        \n",
    "        # 现将encode的结果进行yi\n",
    "        self.linear_in = nn.Linear(encoder_hidden_size*2,decode_hidden_size,bias=False)\n",
    "        \n",
    "        self.linear_out = nn.Linear(encoder_hidden_size*2+ decode_hidden_size,decode_hidden_size)\n",
    "        \n",
    "    def forward(self,output,context,mask):\n",
    "        # output: batch_size,output_len,decode_hidden_size\n",
    "        # context: batch_size,input_len, 2*encode_hidden_size\n",
    "        \n",
    "        batch_size = output.size(0)\n",
    "        output_len = output.size(1)\n",
    "        input_len = context.size(1)\n",
    "        \n",
    "        # 先把context的前面两维batch_size和input_len合并成一维，\n",
    "        # 进行全连接把最后一维的encoder_hidden_size*2变为decode_hidden_size；\n",
    "        # 在全连接结束之后再把维度转回：[batch_size,input_len,decode_hidden_size]\n",
    "        context_in = self.linear_in(context.view(batch_size*input_len,-1)).view(\n",
    "                        batch_size,input_len,-1) #[batch_size,input_len,decode_hidden_size]\n",
    "        \n",
    "        # context_in.transpose(1,2):batch_size,decode_hidden_size,input_len\n",
    "        # output：batch_size,output_len,decode_hidden_size\n",
    "        attn = torch.bmm(output,context_in.transpose(1,2)) # 矩阵相乘生成注意力矩阵\n",
    "        # batch_size,output_len,input_len\n",
    "        \n",
    "        # 把mask的地方的权重设置成一个非常小的数字，这里是设置成10-6\n",
    "        # 因为这些地方并不需要Attention，\n",
    "        attn.data.masked_fill(mask.bool(),-1e6) \n",
    "        \n",
    "        # 对最后一维进行softmax，\n",
    "        # softmax的值代表需要对input_len里的每个值所保持注意力的权重\n",
    "        attn = F.softmax(attn,dim=2) \n",
    "        # [batch_size,output_len,input_len\n",
    "        \n",
    "        context = torch.bmm(attn,context)\n",
    "        # [batch_size,ouput_len,encode_hidden_size*2]\n",
    "        \n",
    "        \n",
    "        output = torch.cat((context,output),dim=2)\n",
    "        # batch_size,output_len,encoder_hidden_size*2+ decode_hidden_size\n",
    "        \n",
    "        # 把前面两维先合并，然后做全连接\n",
    "        # batch_size*output_len,encoder_hidden_size*2+ decode_hidden_size\n",
    "        output = output.view(output.size(0)*output.size(1),-1) \n",
    "        # 全连接之后用tanh激活\n",
    "        output = torch.tanh(self.linear_out(output))\n",
    "        # 最后再把shape还原成：batch_size，output_len，decode_hidden_size\n",
    "        \n",
    "        return output,attn\n",
    "                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder模型\n",
    "- decoder会根据已经翻译的句子内容，和context vectors，来决定下一个输出的单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_size,encode_hidden_size,\n",
    "                 decode_hidden_size,drop_out=0.2):\n",
    "        super(Decoder,self).__init__()\n",
    "        # 先embedding\n",
    "        self.embed = nn.Embedding(vocab_size,embed_size)\n",
    "        \n",
    "        # 然后dropout\n",
    "        self.dropout = nn.Dropout(drop_out) \n",
    "        \n",
    "        # 单向循环神经网络，因为这个只能由前面的词推断后面的词\n",
    "        self.rnn = nn.GRU(embed_size,decode_hidden_size,batch_first=True)\n",
    "        \n",
    "        # 加上Attention\n",
    "        self.attention = Attention(encode_hidden_size,decode_hidden_size)\n",
    "        \n",
    "        # 最后用全连接转回vocab_size来预测每个词的概率\n",
    "        self.out = nn.Linear(decode_hidden_size,vocab_size)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def create_mask(self,x_len,y_len):\n",
    "        # mask shape: x_len * y_len\n",
    "        device = x_len.device\n",
    "        max_x_len = x_len.max()\n",
    "        max_y_len = y_len.max()\n",
    "        x_mask = torch.arange(max_x_len, device=x_len.device)[None, :] < x_len[:, None]\n",
    "        y_mask = torch.arange(max_y_len, device=x_len.device)[None, :] < y_len[:, None]\n",
    "        mask = torch.logical_not(x_mask[:, :, None] * y_mask[:, None, :]).int()\n",
    "        return mask\n",
    "        \n",
    "    def forward(self,ctx,ctx_lengths,y,y_lengths,hid):\n",
    "        # 先对y和encode最后时刻的最后一层输出的hid排序\n",
    "        sorted_len,sorted_idx = y_lengths.sort(0,descending=True)\n",
    "        y_sorted = y[sorted_idx.long()]\n",
    "        hid = hid[:,sorted_idx.long()]\n",
    "        \n",
    "        #embed 然后dropout\n",
    "        y_embedded = self.dropout(self.embed(y_sorted))\n",
    "        # [batch_size,output_length,embed_size]\n",
    "        \n",
    "        #pack_padded操作\n",
    "        packed_seq = nn.utils.rnn.pack_padded_sequence(y_embedded,\n",
    "                                                sorted_len.long().cpu().data.numpy(),\n",
    "                                                      batch_first=True)\n",
    "        # 进行rnn操作\n",
    "        packed_out,packed_hid = self.rnn(packed_seq,hid)\n",
    "        # pad_packed操作\n",
    "        padded_seq,_ = nn.utils.rnn.pad_packed_sequence(packed_out,batch_first=True)\n",
    "        \n",
    "        # 还原排序\n",
    "        _,original_idx = sorted_idx.sort(0,descending=False)\n",
    "        output_seq = padded_seq[original_idx.long()].contiguous()\n",
    "        hid = hid[:,original_idx.long()].contiguous()\n",
    "        \n",
    "        # 这里要创建mask\n",
    "        mask = self.create_mask(y_lengths,ctx_lengths)\n",
    "        \n",
    "        # 加上Attention\n",
    "        output,attn = self.attention(output_seq,ctx,mask)\n",
    "        \n",
    "        # 最后在用全连接输出，并用log_softmax输出(这是为了后面方便计算loss)\n",
    "        output = F.log_softmax(self.out(output),-1)\n",
    "        \n",
    "        return output,hid,attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq\n",
    "- 最后我们构建Seq2Seq模型把encder，Attention，decoder串到一起"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,encoder,decoder):\n",
    "        super(Seq2Seq,self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self,x,x_lengths,y,y_lengths):\n",
    "        encoder_out, hid = self.encoder(x,x_lengths)\n",
    "        output,hid,attn = self.decoder(ctx=encoder_out,\n",
    "                                      ctx_lengths = x_lengths,\n",
    "                                      y=y,\n",
    "                                      y_lengths=y_lengths,\n",
    "                                      hid=hid)\n",
    "        return output,attn\n",
    "    \n",
    "    def translate(self,x,x_lengths,y,max_length=100):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        preds = []\n",
    "        batch_size = x.shape[0]\n",
    "        attns = []\n",
    "        for i in range(max_length):\n",
    "            output, hid, attn = self.decoder(ctx=encoder_out, \n",
    "                    ctx_lengths=x_lengths,\n",
    "                    y=y,\n",
    "                    y_lengths=torch.ones(batch_size).long().to(y.device),\n",
    "                    hid=hid)\n",
    "            y = output.max(2)[1].view(batch_size, 1)\n",
    "            preds.append(y)\n",
    "            attns.append(attn)\n",
    "        return torch.cat(preds, 1), torch.cat(attns, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.2\n",
    "embed_size = hidden_size = 100\n",
    "encoder = Encoder(vocab_size=en_total_words,\n",
    "                       embed_size=embed_size,\n",
    "                      encode_hidden_size=hidden_size,\n",
    "                       decode_hidden_size=hidden_size,\n",
    "                      drop_out=dropout)\n",
    "decoder = Decoder(vocab_size=cn_total_words,\n",
    "                      embed_size=embed_size,\n",
    "                      encode_hidden_size=hidden_size,\n",
    "                       decode_hidden_size=hidden_size,\n",
    "                      drop_out=dropout)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "model = model.to(device)\n",
    "loss_fn = LanguageModelCriterion().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 iteration 0 loss 2.1646461486816406\n",
      "Epoch 0 Training loss 2.1646461486816406\n",
      "Evaluation loss 2.033919095993042\n",
      "Epoch 1 iteration 0 loss 2.0176072120666504\n",
      "Epoch 1 Training loss 2.0176072120666504\n",
      "Epoch 2 iteration 0 loss 1.876351237297058\n",
      "Epoch 2 Training loss 1.876351237297058\n",
      "Epoch 3 iteration 0 loss 1.7437283992767334\n",
      "Epoch 3 Training loss 1.7437283992767334\n",
      "Epoch 4 iteration 0 loss 1.6022510528564453\n",
      "Epoch 4 Training loss 1.6022510528564453\n",
      "Epoch 5 iteration 0 loss 1.457313060760498\n",
      "Epoch 5 Training loss 1.457313060760498\n",
      "Evaluation loss 1.3850897550582886\n",
      "Epoch 6 iteration 0 loss 1.3056591749191284\n",
      "Epoch 6 Training loss 1.3056591749191284\n",
      "Epoch 7 iteration 0 loss 1.1589945554733276\n",
      "Epoch 7 Training loss 1.1589945554733276\n",
      "Epoch 8 iteration 0 loss 1.0136878490447998\n",
      "Epoch 8 Training loss 1.0136878490447998\n",
      "Epoch 9 iteration 0 loss 0.873263418674469\n",
      "Epoch 9 Training loss 0.873263418674469\n",
      "Epoch 10 iteration 0 loss 0.7512374520301819\n",
      "Epoch 10 Training loss 0.7512374520301819\n",
      "Evaluation loss 0.808236837387085\n",
      "Epoch 11 iteration 0 loss 0.6342769265174866\n",
      "Epoch 11 Training loss 0.6342769265174866\n",
      "Epoch 12 iteration 0 loss 0.5352604985237122\n",
      "Epoch 12 Training loss 0.5352604985237122\n",
      "Epoch 13 iteration 0 loss 0.45088905096054077\n",
      "Epoch 13 Training loss 0.45088905096054077\n",
      "Epoch 14 iteration 0 loss 0.38167858123779297\n",
      "Epoch 14 Training loss 0.38167858123779297\n",
      "Epoch 15 iteration 0 loss 0.32309725880622864\n",
      "Epoch 15 Training loss 0.32309725880622864\n",
      "Evaluation loss 0.5098079442977905\n",
      "Epoch 16 iteration 0 loss 0.27190589904785156\n",
      "Epoch 16 Training loss 0.27190589904785156\n",
      "Epoch 17 iteration 0 loss 0.224736288189888\n",
      "Epoch 17 Training loss 0.224736288189888\n",
      "Epoch 18 iteration 0 loss 0.19832320511341095\n",
      "Epoch 18 Training loss 0.19832320511341095\n",
      "Epoch 19 iteration 0 loss 0.16306334733963013\n",
      "Epoch 19 Training loss 0.16306334733963013\n",
      "Epoch 20 iteration 0 loss 0.1341642290353775\n",
      "Epoch 20 Training loss 0.1341642290353775\n",
      "Evaluation loss 0.3909868001937866\n",
      "Epoch 21 iteration 0 loss 0.11968740820884705\n",
      "Epoch 21 Training loss 0.11968740820884705\n",
      "Epoch 22 iteration 0 loss 0.09923803061246872\n",
      "Epoch 22 Training loss 0.09923803061246872\n",
      "Epoch 23 iteration 0 loss 0.08037292957305908\n",
      "Epoch 23 Training loss 0.08037292957305908\n",
      "Epoch 24 iteration 0 loss 0.06937886029481888\n",
      "Epoch 24 Training loss 0.06937886029481888\n",
      "Epoch 25 iteration 0 loss 0.059738416224718094\n",
      "Epoch 25 Training loss 0.059738416224718094\n",
      "Evaluation loss 0.35810691118240356\n",
      "Epoch 26 iteration 0 loss 0.05172634869813919\n",
      "Epoch 26 Training loss 0.05172634869813919\n",
      "Epoch 27 iteration 0 loss 0.04478977248072624\n",
      "Epoch 27 Training loss 0.04478977248072624\n",
      "Epoch 28 iteration 0 loss 0.03896309435367584\n",
      "Epoch 28 Training loss 0.03896309435367584\n",
      "Epoch 29 iteration 0 loss 0.03443875536322594\n",
      "Epoch 29 Training loss 0.03443875536322594\n"
     ]
    }
   ],
   "source": [
    "train(model, train_data, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "testTensor1 = torch.tensor([[True, False, True, False, True, True],\n",
    "        [True, False, True, True, True, True],\n",
    "        [True, True, True, True, True, True]])\n",
    "testTensor2 = torch.tensor([[True, True, True, True, True, True, True, True, True, True, True, True],\n",
    "        [True, False, True, True, True, False, True, False, True, True, True, True],\n",
    "        [True, True, True, False, True, False, True, True, True, False, True, True]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 6])\n",
      "torch.Size([3, 6, 1])\n",
      "tensor([[ True, False,  True, False,  True,  True],\n",
      "        [ True, False,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ True],\n",
       "         [False],\n",
       "         [ True],\n",
       "         [False],\n",
       "         [ True],\n",
       "         [ True]],\n",
       "\n",
       "        [[ True],\n",
       "         [False],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True]],\n",
       "\n",
       "        [[ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True]]])"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testTensor1\n",
    "print(testTensor1.shape)\n",
    "testTensor1c = testTensor1[:, :, None] \n",
    "print(testTensor1c.shape)\n",
    "print(testTensor1)\n",
    "testTensor1c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 12])\n",
      "torch.Size([3, 1, 12])\n",
      "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True],\n",
      "        [ True, False,  True,  True,  True, False,  True, False,  True,  True,\n",
      "          True,  True],\n",
      "        [ True,  True,  True, False,  True, False,  True,  True,  True, False,\n",
      "          True,  True]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True]],\n",
       "\n",
       "        [[ True, False,  True,  True,  True, False,  True, False,  True,  True,\n",
       "           True,  True]],\n",
       "\n",
       "        [[ True,  True,  True, False,  True, False,  True,  True,  True, False,\n",
       "           True,  True]]])"
      ]
     },
     "execution_count": 605,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testTensor2\n",
    "print(testTensor2.shape)\n",
    "testTensor2c = testTensor2[:, None,: ] \n",
    "print(testTensor2c.shape)\n",
    "print(testTensor2)\n",
    "testTensor2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 6, 12])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True],\n",
       "         [False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True],\n",
       "         [False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True]],\n",
       "\n",
       "        [[ True, False,  True,  True,  True, False,  True, False,  True,  True,\n",
       "           True,  True],\n",
       "         [False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False],\n",
       "         [ True, False,  True,  True,  True, False,  True, False,  True,  True,\n",
       "           True,  True],\n",
       "         [ True, False,  True,  True,  True, False,  True, False,  True,  True,\n",
       "           True,  True],\n",
       "         [ True, False,  True,  True,  True, False,  True, False,  True,  True,\n",
       "           True,  True],\n",
       "         [ True, False,  True,  True,  True, False,  True, False,  True,  True,\n",
       "           True,  True]],\n",
       "\n",
       "        [[ True,  True,  True, False,  True, False,  True,  True,  True, False,\n",
       "           True,  True],\n",
       "         [ True,  True,  True, False,  True, False,  True,  True,  True, False,\n",
       "           True,  True],\n",
       "         [ True,  True,  True, False,  True, False,  True,  True,  True, False,\n",
       "           True,  True],\n",
       "         [ True,  True,  True, False,  True, False,  True,  True,  True, False,\n",
       "           True,  True],\n",
       "         [ True,  True,  True, False,  True, False,  True,  True,  True, False,\n",
       "           True,  True],\n",
       "         [ True,  True,  True, False,  True, False,  True,  True,  True, False,\n",
       "           True,  True]]])"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testTensor1c2c = testTensor1c * testTensor2c\n",
    "print(testTensor1c2c.shape)\n",
    "testTensor1c2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.logical_not(testTensor1c2c).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testTensor2c.size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
