{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2, 1])\n",
      "tensor([[[0.7809],\n",
      "         [0.2537]],\n",
      "\n",
      "        [[0.9649],\n",
      "         [0.1673]],\n",
      "\n",
      "        [[0.8156],\n",
      "         [0.7914]],\n",
      "\n",
      "        [[0.3099],\n",
      "         [0.2186]],\n",
      "\n",
      "        [[0.1139],\n",
      "         [0.0593]]])\n",
      "torch.Size([5, 1, 2])\n",
      "tensor([[[0.7809, 0.2537]],\n",
      "\n",
      "        [[0.9649, 0.1673]],\n",
      "\n",
      "        [[0.8156, 0.7914]],\n",
      "\n",
      "        [[0.3099, 0.2186]],\n",
      "\n",
      "        [[0.1139, 0.0593]]])\n",
      "torch.Size([5, 2, 2])\n",
      "tensor([[[0.7809, 0.2537],\n",
      "         [0.7809, 0.2537]],\n",
      "\n",
      "        [[0.9649, 0.1673],\n",
      "         [0.9649, 0.1673]],\n",
      "\n",
      "        [[0.8156, 0.7914],\n",
      "         [0.8156, 0.7914]],\n",
      "\n",
      "        [[0.3099, 0.2186],\n",
      "         [0.3099, 0.2186]],\n",
      "\n",
      "        [[0.1139, 0.0593],\n",
      "         [0.1139, 0.0593]]])\n"
     ]
    }
   ],
   "source": [
    "# 测试expand方法的作用\n",
    "A = torch.rand(5,2,1)\n",
    "print(A.shape)\n",
    "print(A)\n",
    "B = A.permute(0,2,1)\n",
    "print(B.shape)\n",
    "print(B)\n",
    "C = B.expand(-1,2,-1) # 扩展就是把哪个维度上的值复制几份\n",
    "print(C.shape)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0.3635, 0.6664],\n",
      "        [0.9040, 0.8429],\n",
      "        [0.2058, 0.5958]]), tensor([[0.4231, 0.3937],\n",
      "        [0.9176, 0.7017],\n",
      "        [0.6273, 0.0342]])]\n",
      "torch.Size([3, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3635, 0.4231],\n",
       "         [0.6664, 0.3937]],\n",
       "\n",
       "        [[0.9040, 0.9176],\n",
       "         [0.8429, 0.7017]],\n",
       "\n",
       "        [[0.2058, 0.6273],\n",
       "         [0.5958, 0.0342]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stack操作测试\n",
    "A = [torch.rand(3,2),torch.rand(3,2)]\n",
    "print(A)\n",
    "stackA = torch.stack(A,-1)\n",
    "print(stackA.shape)\n",
    "stackA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "tensor([[2., 5.],\n",
      "        [3., 5.]])\n",
      "tensor([[0.0474, 0.9526],\n",
      "        [0.1192, 0.8808]])\n"
     ]
    }
   ],
   "source": [
    "# F的softmax的作用\n",
    "A = torch.tensor([[2.,5.],\n",
    "                 [3.,5.]])\n",
    "print(A.shape)\n",
    "print(A)\n",
    "print(F.softmax(A,dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2, 3])\n",
      "tensor([[[0.5847, 0.9882, 0.0402],\n",
      "         [0.3335, 0.5820, 0.9439]],\n",
      "\n",
      "        [[0.8150, 0.7132, 0.5204],\n",
      "         [0.9753, 0.1078, 0.1540]],\n",
      "\n",
      "        [[0.3415, 0.0234, 0.2402],\n",
      "         [0.5227, 0.4375, 0.7785]],\n",
      "\n",
      "        [[0.0743, 0.8308, 0.0052],\n",
      "         [0.7166, 0.3797, 0.4055]],\n",
      "\n",
      "        [[0.8669, 0.9656, 0.6711],\n",
      "         [0.1539, 0.7742, 0.6488]]])\n",
      "===============\n",
      "torch.return_types.max(\n",
      "values=tensor([[0.5847, 0.9882, 0.9439],\n",
      "        [0.9753, 0.7132, 0.5204],\n",
      "        [0.5227, 0.4375, 0.7785],\n",
      "        [0.7166, 0.8308, 0.4055],\n",
      "        [0.8669, 0.9656, 0.6711]]),\n",
      "indices=tensor([[0, 0, 1],\n",
      "        [1, 0, 0],\n",
      "        [1, 1, 1],\n",
      "        [1, 0, 1],\n",
      "        [0, 0, 0]]))\n",
      "===============\n",
      "torch.Size([5, 3])\n",
      "tensor([[0.5847, 0.9882, 0.9439],\n",
      "        [0.9753, 0.7132, 0.5204],\n",
      "        [0.5227, 0.4375, 0.7785],\n",
      "        [0.7166, 0.8308, 0.4055],\n",
      "        [0.8669, 0.9656, 0.6711]])\n"
     ]
    }
   ],
   "source": [
    "#torch.max,的测试\n",
    "A = torch.rand(5,2,3)\n",
    "print(A.shape)\n",
    "print(A)\n",
    "print(\"===============\")\n",
    "# dim=0 torch.Size([2, 3])取出了第0维度中的所有2*3=6个数相加和最大的那一组数\n",
    "# dim=1 torch.Size([5, 3])每一组中每一列取最大值构成新的1*3的一组数\n",
    "# dim=2 torch.Size([5, 2])每一组中每一行取最大值构成新的2*1的一组\n",
    "maxA2 = torch.max(A,dim=1)\n",
    "# print(maxA2.shape)\n",
    "print(maxA2)\n",
    "print(\"===============\")\n",
    "print(maxA2[0].shape)\n",
    "print(maxA2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2861,  1.8419],\n",
      "        [ 1.0022, -0.1568],\n",
      "        [-0.4694,  0.9617]])\n",
      "tensor([[ 1.1856, -1.4431],\n",
      "        [-0.0639,  0.3934],\n",
      "        [ 1.1764, -1.4525]])\n",
      "================\n",
      "tensor([[1.1856, 1.8419],\n",
      "        [1.0022, 0.3934],\n",
      "        [1.1764, 0.9617]])\n",
      "tensor([[ 1.1856, -1.4431],\n",
      "        [-0.0639,  0.3934],\n",
      "        [ 1.1764, -1.4525]])\n"
     ]
    }
   ],
   "source": [
    "#torch.max2,的测试\n",
    "A = torch.randn(3,2)\n",
    "print(A)\n",
    "B = torch.randn(3,2)\n",
    "print(B)\n",
    "torch.max(A,B,out=A)\n",
    "print(\"================\")\n",
    "print(A)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# power操作\n",
    "np.power(4,0.5) #  4的0.5次方，在基于负采样的的取词比例中可以用到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n",
      "tensor([[[0.3041, 0.9238, 0.6111],\n",
      "         [0.1681, 0.6938, 0.2251],\n",
      "         [0.0922, 0.0278, 0.6462],\n",
      "         [0.0108, 0.3023, 0.2066]],\n",
      "\n",
      "        [[0.6451, 0.5148, 0.3172],\n",
      "         [0.6216, 0.0511, 0.5387],\n",
      "         [0.8604, 0.3888, 0.2673],\n",
      "         [0.0516, 0.1991, 0.4120]]])\n",
      "tensor([[[0.6252, 0.1548, 0.0711],\n",
      "         [0.3203, 0.8457, 0.4195],\n",
      "         [0.0850, 0.7665, 0.1235],\n",
      "         [0.8308, 0.4123, 0.6631]],\n",
      "\n",
      "        [[0.8067, 0.5322, 0.1281],\n",
      "         [0.8668, 0.5285, 0.2485],\n",
      "         [0.4882, 0.7143, 0.6651],\n",
      "         [0.2776, 0.2260, 0.9271]]])\n",
      "tensor([[[0.7639, 0.7021, 0.6524],\n",
      "         [0.7677, 0.0487, 0.0566],\n",
      "         [0.7327, 0.0390, 0.2843],\n",
      "         [0.3603, 0.0872, 0.1521]],\n",
      "\n",
      "        [[0.9117, 0.2828, 0.6372],\n",
      "         [0.0643, 0.1641, 0.2413],\n",
      "         [0.8836, 0.0985, 0.4856],\n",
      "         [0.9021, 0.3347, 0.1292]]])\n",
      "\n",
      "Q的shape torch.Size([2, 2, 3])\n",
      "tensor([[[0.5753, 1.9478, 1.6890],\n",
      "         [0.5753, 1.9478, 1.6890]],\n",
      "\n",
      "        [[2.1787, 1.1537, 1.5352],\n",
      "         [2.1787, 1.1537, 1.5352]]])\n",
      "K的shape torch.Size([2, 2, 3])\n",
      "tensor([[[1.8613, 2.1793, 1.2771],\n",
      "         [1.8613, 2.1793, 1.2771]],\n",
      "\n",
      "        [[2.4393, 2.0011, 1.9688],\n",
      "         [2.4393, 2.0011, 1.9688]]])\n",
      "V的shape torch.Size([2, 2, 3])\n",
      "tensor([[[2.6245, 0.8769, 1.1454],\n",
      "         [2.6245, 0.8769, 1.1454]],\n",
      "\n",
      "        [[2.7617, 0.8801, 1.4933],\n",
      "         [2.7617, 0.8801, 1.4933]]])\n",
      "attn1的shape: torch.Size([2, 2, 2])\n",
      "tensor([[[ 7.4725,  7.4725],\n",
      "         [ 7.4725,  7.4725]],\n",
      "\n",
      "        [[10.6459, 10.6459],\n",
      "         [10.6459, 10.6459]]])\n",
      "attn2的shape: torch.Size([2, 2, 2])\n",
      "tensor([[[0.9341, 0.9341],\n",
      "         [0.9341, 0.9341]],\n",
      "\n",
      "        [[1.3307, 1.3307],\n",
      "         [1.3307, 1.3307]]])\n",
      "attn3的shape: torch.Size([2, 2, 2])\n",
      "tensor([[[0.5000, 0.5000],\n",
      "         [0.5000, 0.5000]],\n",
      "\n",
      "        [[0.5000, 0.5000],\n",
      "         [0.5000, 0.5000]]])\n",
      "tensor([[[2.6245, 0.8769, 1.1454],\n",
      "         [2.6245, 0.8769, 1.1454]],\n",
      "\n",
      "        [[2.7617, 0.8801, 1.4933],\n",
      "         [2.7617, 0.8801, 1.4933]]])\n"
     ]
    }
   ],
   "source": [
    "# Self Attention的Q,K,V 向量化矩阵计算, 以及bmm测试\n",
    "X = torch.ones(2,2,4)\n",
    "print(X)\n",
    "Wq = torch.rand(2,4,3)\n",
    "print(Wq)\n",
    "Wk = torch.rand(2,4,3)\n",
    "print(Wk)\n",
    "Wv = torch.rand(2,4,3)\n",
    "print(Wv)\n",
    "\n",
    "print()\n",
    "Q = torch.bmm(X,Wq)\n",
    "print(\"Q的shape\",Q.shape)\n",
    "print(Q)\n",
    "K = torch.bmm(X,Wk)\n",
    "print(\"K的shape\",K.shape)\n",
    "print(K)\n",
    "V = torch.bmm(X,Wv)\n",
    "print(\"V的shape\",V.shape)\n",
    "print(V)\n",
    "\n",
    "attn1 = torch.bmm(Q,K.transpose(2,1))\n",
    "print(\"attn1的shape:\",attn1.shape)\n",
    "print(attn1)\n",
    "\n",
    "attn2 = attn1 / 8.0\n",
    "print(\"attn2的shape:\",attn2.shape)\n",
    "print(attn2)\n",
    "\n",
    "attn3 = torch.nn.Softmax(dim=2)(attn2)\n",
    "print(\"attn3的shape:\",attn3.shape)\n",
    "print(attn3)\n",
    "\n",
    "output = torch.bmm(attn3,V)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5647, 0.1576, 0.6414],\n",
      "        [0.3598, 0.8444, 0.0225],\n",
      "        [0.6837, 0.0784, 0.2989],\n",
      "        [0.4277, 0.8252, 0.0287],\n",
      "        [0.2266, 0.9268, 0.3346]])\n",
      "13\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# argmax()测试\n",
    "A = torch.rand(5,3)\n",
    "print(A)\n",
    "print(A.argmax().item()) # 最大的那个值在矩阵被打平为一维List时中的索引位置\n",
    "print(A[0].argmax().item()) # 最大的那个值在列表中的索引位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1464,  1.0439,  0.4604],\n",
      "        [-0.0702,  1.5683, -0.3675],\n",
      "        [-0.4626,  0.3654,  0.7164],\n",
      "        [ 0.7878,  0.6938, -0.7750],\n",
      "        [-0.4429,  0.6910,  0.4281]])\n",
      "tensor(4.4905)\n",
      "torch.Size([5, 3])\n",
      "tensor([[1.3580],\n",
      "        [1.1306],\n",
      "        [0.6191],\n",
      "        [0.7066],\n",
      "        [0.6763]])\n",
      "cumsumA: tensor([[-0.1464,  1.0439,  0.4604],\n",
      "        [-0.2166,  2.6122,  0.0929],\n",
      "        [-0.6792,  2.9776,  0.8093],\n",
      "        [ 0.1086,  3.6714,  0.0343],\n",
      "        [-0.3343,  4.3624,  0.4625]])\n"
     ]
    }
   ],
   "source": [
    "# sum测试\n",
    "A = torch.randn(5,3)\n",
    "print(A)\n",
    "sumA = A.sum()\n",
    "print(sumA)\n",
    "print(A.size())\n",
    "sum_to_sizeA = A.sum_to_size(A.size()[0],1) # 这样相当于每行相加\n",
    "print(sum_to_sizeA)\n",
    "cumsumA = A.cumsum(0) # 0代表列依次递加，1:代表行依次递加\n",
    "print(\"cumsumA:\",cumsumA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.3479, -1.0443, -0.0359, -0.5181, -1.1864],\n",
      "        [ 1.0870,  1.9154,  0.0949, -0.0053,  0.7588],\n",
      "        [ 0.9656, -1.9067,  0.0141, -3.3089, -0.3452],\n",
      "        [ 0.3732, -1.6621,  0.3642, -0.0252, -0.0075],\n",
      "        [ 0.6322, -0.5308,  0.7938, -1.0148,  1.3878],\n",
      "        [-2.0887,  0.1318, -0.2714,  0.0504,  0.0523],\n",
      "        [ 0.4787,  0.4455,  1.2219, -2.2179,  1.4942],\n",
      "        [-0.1906,  0.2788,  0.5600,  2.0845, -1.0782],\n",
      "        [-0.3614, -2.3480, -0.4730, -0.2444,  1.5346],\n",
      "        [-0.3884,  1.0739,  1.2118, -0.8928, -0.1041],\n",
      "        [-0.2341,  0.3947,  0.2525,  0.4502,  0.0733]])\n",
      "tensor([[-1.4368],\n",
      "        [ 3.8508],\n",
      "        [-4.5811],\n",
      "        [-0.9574],\n",
      "        [ 1.2682],\n",
      "        [-2.1256],\n",
      "        [ 1.4223],\n",
      "        [ 1.6545],\n",
      "        [-1.8921],\n",
      "        [ 0.9005],\n",
      "        [ 0.9366]])\n",
      "(tensor([ 1.3479, -1.0443, -0.0359, -0.5181, -1.1864]), tensor([-1.4368]))\n",
      "==================\n",
      "11\n",
      "0 0\n",
      "1 2\n",
      "2 3\n",
      "3 9\n",
      "4 1\n",
      "5 7\n",
      "6 8\n",
      "7 4\n",
      "8 5\n",
      "9 10\n",
      "10 6\n",
      "8\n",
      "================\n",
      "20\n",
      "0 0\n",
      "1 3\n",
      "2 2\n",
      "3 8\n",
      "4 4\n",
      "5 10\n",
      "6 6\n",
      "7 3\n",
      "8 2\n",
      "9 1\n",
      "10 5\n",
      "11 3\n",
      "12 4\n",
      "13 10\n",
      "14 10\n",
      "15 6\n",
      "16 7\n",
      "17 8\n",
      "18 3\n",
      "19 5\n",
      "================\n",
      "6\n",
      "0 [tensor([[ 0.6322, -0.5308,  0.7938, -1.0148,  1.3878],\n",
      "        [ 0.4787,  0.4455,  1.2219, -2.2179,  1.4942]]), tensor([[1.2682],\n",
      "        [1.4223]])]\n",
      "1 [tensor([[-0.3614, -2.3480, -0.4730, -0.2444,  1.5346],\n",
      "        [-0.1906,  0.2788,  0.5600,  2.0845, -1.0782]]), tensor([[-1.8921],\n",
      "        [ 1.6545]])]\n",
      "2 [tensor([[-0.2341,  0.3947,  0.2525,  0.4502,  0.0733],\n",
      "        [ 1.3479, -1.0443, -0.0359, -0.5181, -1.1864]]), tensor([[ 0.9366],\n",
      "        [-1.4368]])]\n",
      "3 [tensor([[ 1.0870,  1.9154,  0.0949, -0.0053,  0.7588],\n",
      "        [-2.0887,  0.1318, -0.2714,  0.0504,  0.0523]]), tensor([[ 3.8508],\n",
      "        [-2.1256]])]\n",
      "4 [tensor([[ 0.3732, -1.6621,  0.3642, -0.0252, -0.0075],\n",
      "        [-0.3884,  1.0739,  1.2118, -0.8928, -0.1041]]), tensor([[-0.9574],\n",
      "        [ 0.9005]])]\n",
      "5 [tensor([[ 0.9656, -1.9067,  0.0141, -3.3089, -0.3452]]), tensor([[-4.5811]])]\n"
     ]
    }
   ],
   "source": [
    "# Data处理测试:使用TensorDataset 和 DataLoader\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "trainX = torch.randn(11,5) # 11个样本，每个样本有5个feature [11,5]\n",
    "trainY = trainX.sum_to_size(trainX.size()[0],1) # [11,1]\n",
    "print(trainX)\n",
    "print(trainY)\n",
    "train_dataset = TensorDataset(trainX,trainY) # 只要传入的tensor的第一个维度值相同即可;\n",
    "print(train_dataset.__getitem__(0)) # TensorDataset里面是一个tuple，可以用__getitem__(index)来取出某一个tensor\n",
    "\n",
    "print(\"==================\")\n",
    "train_sampler_no_replacement = RandomSampler(train_dataset) # 会将原始数据随机打乱\n",
    "print(len(train_sampler_no_replacement))\n",
    "for idx,item in enumerate(train_sampler_no_replacement):\n",
    "    print(idx,item)\n",
    "print(next(iter(train_sampler_no_replacement)))\n",
    "\n",
    "print(\"================\")\n",
    "# replacement表示是否可以重复，num_samples是要取的样本数目(因为可以重复，所以样本数取多少都可以)，默认不填写就是样本总数\n",
    "train_sampler_with_replacemen = RandomSampler(train_dataset,replacement=True,num_samples=20)\n",
    "print(len(train_sampler_with_replacemen))\n",
    "for idx,item in enumerate(train_sampler_with_replacemen):\n",
    "    print(idx,item)\n",
    "\n",
    "print(\"================\")\n",
    "train_dataloader = DataLoader(train_dataset,sampler=train_sampler_no_replacement,batch_size=2)\n",
    "print(len(train_dataloader)) # 这里总共11个样本，2个一组组成一个batch，最终可以组成11/2 +1 =6个batch\n",
    "for idx,item in enumerate(train_dataloader):\n",
    "    print(idx,item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs (tensor([0.6025, 0.8315]), tensor([0.8173, 0.5786]), tensor(0.9411), tensor(0.5614))\n",
      "(tensor([0.6025, 0.8315]), tensor([0.8173, 0.5786]), tensor(0.9411), tensor(0.5614))\n",
      "outputs (tensor([0.5318, 0.5878]), tensor([0.5047, 0.2091]), tensor(0.7838), tensor(0.1442))\n",
      "(tensor([0.5318, 0.5878]), tensor([0.5047, 0.2091]), tensor(0.7838), tensor(0.1442))\n",
      "outputs (tensor([0.5451, 0.7045]), tensor(0.3289))\n",
      "(tensor([0.5451, 0.7045]), tensor(0.3289))\n"
     ]
    }
   ],
   "source": [
    "# 使用DataLoader中的collate_fn 来进行将分好的batch数据进行处理\n",
    "# 在构建每一个batch的前会把batch的数据传过来，这样我们就可以针对一个batch的数据进行做预处理操作\n",
    "def collate_fn_test(one_batch_list):\n",
    "    #[(tensor([0.0573, 0.3567]), tensor(0.8175)), (tensor([0.2944, 0.0531]), tensor(0.3613))]\n",
    "    # print(\"inputs\",inputs)\n",
    "    src_insts, tgt_insts = list(zip(*one_batch_list)) # 把batch里面的src数据和target数据拆出来\n",
    "#     print(\"src_insts:\",src_insts) #(tensor([0.0573, 0.3567]), tensor([0.2944, 0.0531]))\n",
    "#     print(\"tgt_insts:\",tgt_insts) # (tensor(0.8175), tensor(0.3613))\n",
    "    \n",
    "    # 比如我们把batch中的\n",
    "    \n",
    "    \n",
    "    # 把src_insts和tgt_insts依次取出来一个然后顺序连接成一个列表\n",
    "    outputs = (*src_insts,*tgt_insts) \n",
    "    print(\"outputs\",outputs)\n",
    "    return outputs\n",
    "    \n",
    "trainX = torch.rand(5,2)\n",
    "trainY = torch.rand(5)\n",
    "train_dataset = torch.utils.data.TensorDataset(trainX,trainY)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=2,collate_fn=collate_fn_test)\n",
    "for step ,batch in enumerate(train_dataloader):\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([2, 4, 5], [7, 8, 4])\n",
      "(2, 4, 5, 7, 8, 4)\n"
     ]
    }
   ],
   "source": [
    "A = [2,4,5]\n",
    "B = [7,8,4]\n",
    "C = (A,B)\n",
    "D = (*A,*B)\n",
    "print(C)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def return_two():\n",
    "    return 3,5\n",
    "A = return_two()\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2.6666666666666665\n"
     ]
    }
   ],
   "source": [
    "print(8 // 3)\n",
    "print(8/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# max_steps >0；t_total\n",
    "max_steps = 6\n",
    "len_train_dataloader = 5 # 分组后的batch数目\n",
    "gradient_accumulation_steps = 2\n",
    "num_train_epochs = 20 # 需要训练的epochs\n",
    "if max_steps > 0:\n",
    "    t_total = max_steps \n",
    "    num_train_epochs = max_steps // (len_train_dataloader // gradient_accumulation_steps) + 1\n",
    "else:\n",
    "    t_total = len(train_dataloader) // gradient_accumulation_steps * num_train_epochs\n",
    "print(num_train_epochs)\n",
    "print(t_total) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "# max_steps =0\n",
    "max_steps = 0\n",
    "len_train_dataloader = 5 # 分组后的batch数目\n",
    "gradient_accumulation_steps = 2\n",
    "num_train_epochs = 20 # 需要训练的epochs\n",
    "if max_steps > 0:\n",
    "    t_total = max_steps \n",
    "    num_train_epochs = max_steps // (len_train_dataloader // gradient_accumulation_steps) + 1\n",
    "else:\n",
    "    t_total = len(train_dataloader) // gradient_accumulation_steps * num_train_epochs\n",
    "print(num_train_epochs)\n",
    "print(t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5727,  1.4850, -0.3282],\n",
      "        [ 1.3668, -0.9526,  1.1699],\n",
      "        [-1.1580,  0.3889, -1.7746],\n",
      "        [ 0.8349,  0.0372,  0.1035],\n",
      "        [-0.9505, -0.2258,  0.4844]])\n",
      "tensor([[-0.5727,  1.4850, -0.3282],\n",
      "        [ 1.3668, -0.9526,  1.1699],\n",
      "        [-1.1580,  0.3889, -1.7746],\n",
      "        [ 0.8349,  0.0372,  0.1035],\n",
      "        [-0.9505, -0.2258,  0.4844]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# is_sparse\n",
    "A = torch.randn(5,3)\n",
    "print(A)\n",
    "print(A.data)\n",
    "A.data.is_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.8500, 1.9333])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# addcdiv_ 测试\n",
    "A = torch.Tensor([5,2])\n",
    "step_size = 0.1\n",
    "exp_avg = torch.Tensor([3,2])\n",
    "denom = torch.Tensor([2,3])\n",
    "A.data.addcdiv(-step_size,exp_avg,denom) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 10/10 [00:00<00:00, 7017.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# tqdm 进度条trang测试\n",
    "from tqdm import tqdm, trange \n",
    "num_train_epochs = 10\n",
    "train_iterator = trange(int(num_train_epochs),desc=\"Epoch\")\n",
    "for item in train_iterator:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['at', 'loggerheads']\n",
      "心存\n",
      "怨恨\n"
     ]
    }
   ],
   "source": [
    "# jieba ,nltk 分词测试\n",
    "import jieba\n",
    "import nltk\n",
    "seq = [\"at loggerheads\",\"心存怨恨\"]\n",
    "\n",
    "print(nltk.word_tokenize(seq[0]))\n",
    "\n",
    "for word in jieba.cut(seq[1]):\n",
    "    print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# 匿名函数lambda的测试:冒号前面是参数，冒号后面是返回值\n",
    "g = lambda x,y,z : x**2+y-z # 定义一个匿名函数 g(x,y,z) = x**2+y-z\n",
    "print(g(3,2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 tensor([0.0639, 0.3006, 0.6062, 0.7028, 0.3282, 0.5538, 0.5427, 0.9609])\n",
      "2 tensor([0.1567, 0.9463])\n",
      "9 tensor([0.1697, 0.8575, 0.0725, 0.0017, 0.3308, 0.0981, 0.9971, 0.4390, 0.5366])\n",
      "6 tensor([0.1649, 0.7329, 0.6933, 0.9473, 0.3163, 0.5994])\n",
      "7 tensor([0.1473, 0.9783, 0.6828, 0.4604, 0.5569, 0.3818, 0.5582])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([0.1567, 0.9463]),\n",
       " tensor([0.1649, 0.7329, 0.6933, 0.9473, 0.3163, 0.5994]),\n",
       " tensor([0.1473, 0.9783, 0.6828, 0.4604, 0.5569, 0.3818, 0.5582]),\n",
       " tensor([0.0639, 0.3006, 0.6062, 0.7028, 0.3282, 0.5538, 0.5427, 0.9609]),\n",
       " tensor([0.1697, 0.8575, 0.0725, 0.0017, 0.3308, 0.0981, 0.9971, 0.4390, 0.5366])]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorted() 函数测试\n",
    "import random\n",
    "lists = []\n",
    "for i in range(5):\n",
    "    rand = random.randint(1,9)\n",
    "    tensor = torch.rand(rand)\n",
    "    print(len(tensor),tensor)\n",
    "    lists.append(tensor)\n",
    "    \n",
    "# 将lists按照每个item的长度进行排序\n",
    "sorted(lists,key = lambda list_item : len(list_item)) # 其中x就是lists里面的item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 tensor([0.3846, 0.8689, 0.9525, 0.0927, 0.3214, 0.1655, 0.0355, 0.8521, 0.1572])\n",
      "8 tensor([0.9947, 0.3408, 0.5104, 0.8613, 0.6315, 0.1418, 0.5377, 0.9437])\n",
      "5 tensor([0.9180, 0.3106, 0.8648, 0.5428, 0.4305])\n",
      "6 tensor([0.8463, 0.2093, 0.8266, 0.2895, 0.7123, 0.0675])\n",
      "9 tensor([0.8424, 0.4642, 0.6524, 0.2615, 0.0677, 0.1829, 0.9981, 0.0201, 0.0625])\n",
      "[2, 3, 1, 0, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[tensor([0.9180, 0.3106, 0.8648, 0.5428, 0.4305])],\n",
       " [tensor([0.8463, 0.2093, 0.8266, 0.2895, 0.7123, 0.0675])],\n",
       " [tensor([0.9947, 0.3408, 0.5104, 0.8613, 0.6315, 0.1418, 0.5377, 0.9437])],\n",
       " [tensor([0.3846, 0.8689, 0.9525, 0.0927, 0.3214, 0.1655, 0.0355, 0.8521, 0.1572])],\n",
       " [tensor([0.8424, 0.4642, 0.6524, 0.2615, 0.0677, 0.1829, 0.9981, 0.0201, 0.0625])]]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 按照指定索引进行排序\n",
    "import random\n",
    "lists = []\n",
    "for i in range(5):\n",
    "    rand = random.randint(1,9)\n",
    "    tensor = torch.rand(rand)\n",
    "    print(len(tensor),tensor)\n",
    "    lists.append(tensor)\n",
    "    \n",
    "# 如果按照句子长度进行排序的话，lists的排序后的item索引位置相对于原始lists的对应应该是list_idxs\n",
    "list_idxs = sorted(range(len(lists)),key = lambda x : len(lists[x]))\n",
    "print(list_idxs) \n",
    "\n",
    "# 那么现在要求原来的lists，按照list_idxs中的索引位置进行排序，应该怎么排呢\n",
    "lists_new_order = [[lists[idx]] for idx in list_idxs]\n",
    "lists_new_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('embed.weight',\n",
       "              tensor([[ 1.7062, -0.2245, -0.0810,  ...,  0.1508, -1.0192,  1.1120],\n",
       "                      [-1.8782, -0.2626, -0.4415,  ...,  0.3620,  2.5087,  0.5897],\n",
       "                      [-0.6297,  0.2410, -0.2347,  ..., -0.7184,  0.4643, -0.3540],\n",
       "                      ...,\n",
       "                      [-1.2651,  1.2818, -0.5194,  ..., -0.3482, -0.8409,  0.2882],\n",
       "                      [-1.9637,  1.2868,  0.4306,  ..., -0.6555, -1.0185,  1.4404],\n",
       "                      [ 0.7882, -2.0590,  2.1683,  ...,  0.7193,  1.1773, -0.0051]])),\n",
       "             ('lstm.weight_ih_l0',\n",
       "              tensor([[-0.0038,  0.0273,  0.0748,  ...,  0.0407, -0.0176, -0.0031],\n",
       "                      [ 0.0328,  0.0358, -0.0641,  ...,  0.0410, -0.0140,  0.0070],\n",
       "                      [ 0.0502, -0.0377, -0.0354,  ..., -0.0258, -0.0046, -0.0483],\n",
       "                      ...,\n",
       "                      [-0.0089,  0.0744, -0.1241,  ..., -0.0028, -0.0544, -0.0874],\n",
       "                      [ 0.0563, -0.0370, -0.0970,  ..., -0.0902,  0.0148, -0.0879],\n",
       "                      [-0.0476,  0.1498, -0.0220,  ..., -0.0266,  0.0953, -0.0149]])),\n",
       "             ('lstm.weight_hh_l0',\n",
       "              tensor([[-0.0305, -0.1302, -0.1130,  ...,  0.0864,  0.0748,  0.0544],\n",
       "                      [-0.0405,  0.0103, -0.0063,  ...,  0.1201,  0.0920,  0.0569],\n",
       "                      [-0.0082, -0.0428,  0.0200,  ...,  0.0345,  0.0524, -0.0292],\n",
       "                      ...,\n",
       "                      [-0.1284, -0.0286, -0.0262,  ...,  0.1541, -0.0057,  0.0363],\n",
       "                      [-0.0585, -0.0667, -0.1057,  ...,  0.0283,  0.0337,  0.1096],\n",
       "                      [ 0.0616,  0.0725, -0.0222,  ..., -0.0753, -0.0496,  0.0474]])),\n",
       "             ('lstm.bias_ih_l0',\n",
       "              tensor([-0.0169,  0.1266,  0.0188,  0.0209,  0.1409,  0.1314,  0.0630,  0.0988,\n",
       "                       0.1416,  0.0658,  0.1021, -0.0056,  0.1279,  0.1525, -0.0084,  0.1019,\n",
       "                       0.1349,  0.0634,  0.1105,  0.0018,  0.1094,  0.0808,  0.0434,  0.1485,\n",
       "                       0.0596,  0.1312,  0.0338,  0.1273, -0.0221,  0.1266,  0.0801, -0.0096,\n",
       "                       0.1535,  0.0926, -0.0429,  0.0390,  0.0342, -0.0363,  0.0811,  0.0292,\n",
       "                       0.1179,  0.0545,  0.0127,  0.0849,  0.0366,  0.0260,  0.1467,  0.0062,\n",
       "                       0.0844,  0.0636,  0.0133,  0.1125,  0.1082, -0.0088,  0.0120,  0.0339,\n",
       "                       0.0269,  0.1007,  0.0942,  0.0272, -0.0262,  0.0861,  0.1322,  0.1377,\n",
       "                       0.0672,  0.0135, -0.0553,  0.0717,  0.0935,  0.1489,  0.0181,  0.0641,\n",
       "                      -0.0076,  0.1280,  0.1226,  0.1164,  0.0062,  0.0911, -0.0581, -0.0363,\n",
       "                       0.0816,  0.1465,  0.0376,  0.0567, -0.0177, -0.0330,  0.0662,  0.1473,\n",
       "                       0.0009,  0.0210,  0.1223,  0.0716, -0.0161,  0.1412, -0.0124,  0.0881,\n",
       "                       0.1144,  0.1256, -0.0510,  0.1308,  0.1489,  0.0670,  0.0169,  0.1303,\n",
       "                      -0.0044, -0.0119, -0.0169,  0.1385,  0.0678,  0.1077,  0.0541,  0.0833,\n",
       "                       0.0849,  0.1458,  0.0041,  0.0627,  0.0412,  0.0361,  0.0028, -0.0503,\n",
       "                       0.0894,  0.0792,  0.0810,  0.1004,  0.0974,  0.0707, -0.0339,  0.1105,\n",
       "                       0.0923, -0.0189,  0.1245,  0.0117, -0.0422, -0.0209,  0.0253,  0.0736,\n",
       "                       0.0095,  0.1237,  0.0449, -0.0311,  0.0135,  0.1113,  0.0176,  0.0987,\n",
       "                       0.0590,  0.0416,  0.0850,  0.1408,  0.1144,  0.1112,  0.0903,  0.0484,\n",
       "                       0.0966,  0.0947, -0.0458, -0.0428, -0.0342,  0.0262,  0.0920,  0.0303,\n",
       "                       0.1521, -0.0222,  0.0931, -0.0332,  0.0351,  0.0120,  0.1252, -0.0351,\n",
       "                       0.0764,  0.1356,  0.0681,  0.0156,  0.0121,  0.0795,  0.0910, -0.0276,\n",
       "                      -0.0435, -0.0065,  0.0400, -0.0038, -0.0293,  0.1256, -0.0239,  0.1154,\n",
       "                       0.1050,  0.0224,  0.0148,  0.0342,  0.1212,  0.0563,  0.1203, -0.0017,\n",
       "                       0.0131,  0.0259,  0.1495, -0.0062, -0.0306,  0.0415,  0.1176,  0.1523,\n",
       "                      -0.0754, -0.1132,  0.0303,  0.0094,  0.0686, -0.1496, -0.1071, -0.0136,\n",
       "                      -0.1090, -0.1306,  0.0191,  0.0758, -0.1049, -0.0669, -0.1123,  0.0481,\n",
       "                      -0.1456,  0.0195,  0.1205,  0.0576, -0.1275, -0.0925,  0.0583,  0.0959,\n",
       "                      -0.0169,  0.1320,  0.0080,  0.0386,  0.1302, -0.0681, -0.0498, -0.1198,\n",
       "                      -0.0508,  0.0763,  0.1267,  0.1173, -0.0216, -0.1440,  0.0404, -0.1528,\n",
       "                       0.1014,  0.0131, -0.1405,  0.0640, -0.0933, -0.0876,  0.0707,  0.1032,\n",
       "                      -0.0104,  0.0820, -0.1381, -0.0347, -0.1340,  0.0302, -0.0510, -0.1412,\n",
       "                      -0.0306, -0.0588, -0.0520,  0.0422, -0.0250, -0.0570, -0.1353,  0.1220,\n",
       "                      -0.1406,  0.0790,  0.1447,  0.0381, -0.0654, -0.1045,  0.1331,  0.1285,\n",
       "                       0.0252,  0.0952,  0.0924,  0.1506, -0.1117, -0.0786,  0.0116,  0.0571,\n",
       "                       0.0076,  0.0725, -0.1458, -0.0595, -0.0417,  0.0680, -0.0093, -0.1030,\n",
       "                      -0.0069,  0.1366, -0.0973, -0.0137, -0.1214, -0.0144, -0.0514, -0.1492,\n",
       "                      -0.1271,  0.1294,  0.1102,  0.1386,  0.0573,  0.0332,  0.0322,  0.0566,\n",
       "                      -0.0756,  0.0445, -0.0023,  0.0032, -0.0331,  0.0149,  0.0050,  0.0843,\n",
       "                       0.1097,  0.1173,  0.0611, -0.0066,  0.1181,  0.0303, -0.0143, -0.0116,\n",
       "                      -0.0587,  0.1108, -0.0247,  0.0301, -0.0227,  0.0660, -0.0016,  0.0774,\n",
       "                      -0.0208, -0.0562, -0.0556, -0.0417,  0.1367,  0.0898, -0.0555, -0.0136,\n",
       "                      -0.0216,  0.0462, -0.0121, -0.0483, -0.0290,  0.0185,  0.0128, -0.0308,\n",
       "                      -0.0289, -0.0538, -0.0457,  0.0681, -0.0738, -0.0038,  0.1007,  0.1008,\n",
       "                      -0.0524, -0.0311, -0.0588, -0.0270, -0.0103,  0.0287,  0.0691,  0.0487,\n",
       "                       0.0279,  0.0523, -0.0686,  0.0059, -0.0190, -0.0464, -0.0032, -0.0345,\n",
       "                      -0.0255,  0.0069,  0.0461,  0.0601, -0.0501,  0.0901, -0.0397,  0.1020,\n",
       "                      -0.0409,  0.0642,  0.0957,  0.0720, -0.0341,  0.0580,  0.0521,  0.0314,\n",
       "                      -0.0362, -0.0253, -0.0659,  0.0836,  0.0121,  0.0098,  0.0590, -0.0009,\n",
       "                       0.0362,  0.0728,  0.0161,  0.0548, -0.0171,  0.0518, -0.0091,  0.0265])),\n",
       "             ('lstm.bias_hh_l0',\n",
       "              tensor([ 0.0487,  0.1161,  0.1183,  0.0433,  0.0655,  0.1412,  0.1420,  0.0957,\n",
       "                       0.0808,  0.0533,  0.0828, -0.0401,  0.0184,  0.0025,  0.0815, -0.0155,\n",
       "                       0.1017,  0.0328,  0.0727,  0.0852,  0.1218,  0.1327, -0.0512, -0.0376,\n",
       "                      -0.0436,  0.0274,  0.1152,  0.0723,  0.1081,  0.0160,  0.1163,  0.0276,\n",
       "                       0.1435, -0.0509, -0.0318,  0.0901, -0.0645,  0.0090,  0.0444, -0.0226,\n",
       "                       0.0934,  0.0600, -0.0031, -0.0220, -0.0544,  0.0968,  0.0505,  0.0276,\n",
       "                       0.1064,  0.0126,  0.0739, -0.0223,  0.0688, -0.0244,  0.0418,  0.0862,\n",
       "                       0.0751,  0.1456, -0.0202,  0.1339, -0.0326,  0.0406,  0.0519,  0.1296,\n",
       "                       0.1418,  0.0702,  0.0806,  0.1418,  0.0919, -0.0174, -0.0314,  0.0209,\n",
       "                       0.0751,  0.1291, -0.0409,  0.0460, -0.0084, -0.0398,  0.1118,  0.0883,\n",
       "                       0.0513,  0.0981, -0.0194,  0.0635,  0.0389, -0.0445,  0.0640,  0.0959,\n",
       "                       0.0411,  0.1163,  0.0537,  0.0528,  0.1051,  0.0009, -0.0060, -0.0143,\n",
       "                       0.1094,  0.0050,  0.0261,  0.1489,  0.0306,  0.0595,  0.1051,  0.1135,\n",
       "                       0.1474,  0.0121,  0.0363,  0.0893,  0.0131, -0.0400, -0.0522,  0.1254,\n",
       "                       0.1033,  0.0371,  0.0368,  0.1311, -0.0286,  0.1034, -0.0189, -0.0491,\n",
       "                       0.0301,  0.0210,  0.1027, -0.0033,  0.0738,  0.1437,  0.0951, -0.0620,\n",
       "                      -0.0253,  0.1347, -0.0496,  0.1527,  0.1473,  0.1337,  0.0191,  0.1217,\n",
       "                       0.0666,  0.0210,  0.0069,  0.0489, -0.0317,  0.0033, -0.0265,  0.1188,\n",
       "                      -0.0100,  0.0259,  0.1131,  0.1256,  0.1425,  0.1125,  0.1484,  0.1234,\n",
       "                       0.1264,  0.1298, -0.0041,  0.0015,  0.0145,  0.0419,  0.1088,  0.1227,\n",
       "                       0.1150,  0.1370,  0.1355, -0.0398,  0.0488,  0.0871,  0.0832,  0.0066,\n",
       "                       0.0489,  0.0425,  0.1046, -0.0289,  0.0951,  0.1297,  0.0561,  0.0391,\n",
       "                       0.1408,  0.0071,  0.0370, -0.0010, -0.0068,  0.0588,  0.0297,  0.0063,\n",
       "                       0.1263,  0.0886, -0.0134,  0.1180, -0.0254, -0.0462,  0.0914,  0.1021,\n",
       "                      -0.0087,  0.0079,  0.1001,  0.0269,  0.0033, -0.0084,  0.0134,  0.1492,\n",
       "                      -0.0646, -0.0287, -0.0215, -0.0015,  0.0306, -0.1036, -0.0841,  0.1497,\n",
       "                      -0.0520, -0.1021, -0.0910,  0.0810, -0.1276, -0.1378, -0.1122,  0.0968,\n",
       "                       0.0379, -0.1072,  0.0670, -0.0505, -0.0255,  0.0411, -0.0653,  0.0768,\n",
       "                       0.0622,  0.0586, -0.0182,  0.0019,  0.1271, -0.0820,  0.1076, -0.0402,\n",
       "                      -0.0355,  0.0551,  0.0537,  0.0660, -0.0688,  0.0330, -0.1396,  0.0332,\n",
       "                       0.0816, -0.1034, -0.0400,  0.0929,  0.0400,  0.0126,  0.0223, -0.0149,\n",
       "                      -0.0496, -0.0426, -0.1037, -0.0554, -0.0723,  0.1345, -0.0038, -0.0662,\n",
       "                       0.1304, -0.1382,  0.1007,  0.1410, -0.1499,  0.0077, -0.1066,  0.1478,\n",
       "                      -0.0997,  0.0074,  0.0449, -0.1373, -0.1402, -0.0717,  0.0573,  0.0438,\n",
       "                      -0.0977,  0.0507, -0.0149,  0.0644,  0.0020, -0.0302,  0.0316,  0.1334,\n",
       "                       0.0978,  0.1072, -0.0196, -0.1268, -0.0957,  0.0663,  0.0634, -0.1337,\n",
       "                      -0.0463,  0.0276, -0.0296, -0.1386, -0.0165, -0.0624, -0.0334, -0.0182,\n",
       "                       0.0097, -0.0251,  0.0279, -0.0476,  0.0028,  0.0580,  0.0637,  0.1181,\n",
       "                      -0.0422,  0.0088, -0.0232,  0.0834, -0.0894, -0.0325, -0.0722,  0.0195,\n",
       "                       0.0717, -0.0340,  0.1479, -0.0357,  0.0749,  0.0623,  0.0092,  0.0377,\n",
       "                      -0.0284, -0.0304, -0.0599,  0.0935, -0.0451,  0.1160,  0.0840,  0.0775,\n",
       "                       0.0138,  0.0503,  0.0012,  0.0197,  0.0832,  0.0412, -0.0166,  0.0643,\n",
       "                      -0.0196,  0.0720,  0.0950,  0.0601,  0.0120, -0.0581,  0.0894,  0.0957,\n",
       "                      -0.0410, -0.0494,  0.0548,  0.0577,  0.0500, -0.0066,  0.1339,  0.0169,\n",
       "                       0.0499,  0.0027,  0.0154,  0.1268,  0.0520,  0.1265, -0.0383, -0.0265,\n",
       "                       0.0485,  0.0169,  0.0550,  0.0737,  0.0469,  0.1202,  0.1015, -0.0577,\n",
       "                       0.0017, -0.0114,  0.0103,  0.0982,  0.0110, -0.0280,  0.1152,  0.0819,\n",
       "                       0.0124,  0.0239,  0.0101,  0.0632,  0.0931, -0.0174,  0.1015,  0.0171,\n",
       "                       0.0766,  0.0948,  0.1020,  0.1261,  0.0785,  0.0262, -0.0070,  0.0318,\n",
       "                       0.0600, -0.0149,  0.0297,  0.1376, -0.0458,  0.1316,  0.0679, -0.0138])),\n",
       "             ('linear.weight',\n",
       "              tensor([[-0.0692, -0.0298, -0.1017,  ...,  0.1587, -0.0061, -0.0479],\n",
       "                      [ 0.2059,  0.0336,  0.1228,  ..., -0.1414, -0.2057, -0.0896],\n",
       "                      [-0.0803,  0.0789, -0.0424,  ...,  0.1073,  0.0838,  0.1249],\n",
       "                      ...,\n",
       "                      [ 0.1079,  0.0372,  0.0501,  ..., -0.0674, -0.0552, -0.0434],\n",
       "                      [ 0.1792,  0.1876,  0.1809,  ..., -0.0468, -0.1447, -0.1145],\n",
       "                      [ 0.0247,  0.2034,  0.1950,  ..., -0.1855, -0.1279, -0.1829]])),\n",
       "             ('linear.bias',\n",
       "              tensor([-0.0105, -0.0087,  0.0008,  ..., -0.0356, -0.1204, -0.1124]))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.load 测试 ,只能加载torch.save存储的数据\n",
    "# model = \"/Users/zhenwuzhou/.keras/models/text8/1m.pth\"\n",
    "weight_file = \"/Users/zhenwuzhou/.keras/models/text8/1m_dict.pth\"\n",
    "weight = torch.load(weight_file)\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 95, 3, 0, 0, 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2, 95,  3,  0,  0,  0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用 * 数字来添加多了相同元素\n",
    "PAD = 0\n",
    "A = [2, 95, 3]\n",
    "B = [A + [PAD] * 3]\n",
    "print(B)\n",
    "C = np.array([A + [PAD]*3])\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# range测试\n",
    "for i in range(5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 1.00000000e+00 1.58489319e-01 1.58489319e-01\n",
      "  2.51188643e-02 2.51188643e-02 3.98107171e-03 3.98107171e-03\n",
      "  6.30957344e-04 6.30957344e-04]\n",
      " [2.00000000e+00 2.00000000e+00 3.16978638e-01 3.16978638e-01\n",
      "  5.02377286e-02 5.02377286e-02 7.96214341e-03 7.96214341e-03\n",
      "  1.26191469e-03 1.26191469e-03]\n",
      " [3.00000000e+00 3.00000000e+00 4.75467958e-01 4.75467958e-01\n",
      "  7.53565929e-02 7.53565929e-02 1.19432151e-02 1.19432151e-02\n",
      "  1.89287203e-03 1.89287203e-03]\n",
      " [4.00000000e+00 4.00000000e+00 6.33957277e-01 6.33957277e-01\n",
      "  1.00475457e-01 1.00475457e-01 1.59242868e-02 1.59242868e-02\n",
      "  2.52382938e-03 2.52382938e-03]]\n",
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 8.41470985e-01  1.00000000e+00  1.57826640e-01  1.58489319e-01\n",
      "   2.51162229e-02  2.51188643e-02  3.98106119e-03  3.98107171e-03\n",
      "   6.30957303e-04  6.30957344e-04]\n",
      " [ 9.09297427e-01  2.00000000e+00  3.11697146e-01  3.16978638e-01\n",
      "   5.02165994e-02  5.02377286e-02  7.96205928e-03  7.96214341e-03\n",
      "   1.26191435e-03  1.26191469e-03]\n",
      " [ 1.41120008e-01  3.00000000e+00  4.57754548e-01  4.75467958e-01\n",
      "   7.52852930e-02  7.53565929e-02  1.19429312e-02  1.19432151e-02\n",
      "   1.89287090e-03  1.89287203e-03]\n",
      " [-7.56802495e-01  4.00000000e+00  5.92337725e-01  6.33957277e-01\n",
      "   1.00306487e-01  1.00475457e-01  1.59236138e-02  1.59242868e-02\n",
      "   2.52382670e-03  2.52382938e-03]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 8.4147e-01,  5.4030e-01,  1.5783e-01,  9.8747e-01,  2.5116e-02,\n",
       "          9.9968e-01,  3.9811e-03,  9.9999e-01,  6.3096e-04,  1.0000e+00],\n",
       "        [ 9.0930e-01, -4.1615e-01,  3.1170e-01,  9.5018e-01,  5.0217e-02,\n",
       "          9.9874e-01,  7.9621e-03,  9.9997e-01,  1.2619e-03,  1.0000e+00],\n",
       "        [ 1.4112e-01, -9.8999e-01,  4.5775e-01,  8.8908e-01,  7.5285e-02,\n",
       "          9.9716e-01,  1.1943e-02,  9.9993e-01,  1.8929e-03,  1.0000e+00],\n",
       "        [-7.5680e-01, -6.5364e-01,  5.9234e-01,  8.0569e-01,  1.0031e-01,\n",
       "          9.9496e-01,  1.5924e-02,  9.9987e-01,  2.5238e-03,  1.0000e+00]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 相对位置编码测试\n",
    "def get_sinusoid_embedding_position_matrix(max_seq_len, embedding_size, padding_idx=None):\n",
    "    \"\"\"\n",
    "    Sinusoid position encoding table : 正弦型号未知编码矩阵\n",
    "    :param max_seq_len: 最大句子长度(注意是全局最大句子长度，不是batch中最大句子长度)\n",
    "    :param embedding_size: 位置编码输出的维度数，这个要和word的embedding_size的维数相同\n",
    "    :param padding_idx: 如果有PAD的占位，那么这个PAD占位的one-hot编码是多少，如果给定，那么最后关于PAD的embedding编码将全为0\n",
    "    :return: 一个size 为[max_seq_len,embedding_size]的embedding矩阵\n",
    "    \"\"\"\n",
    "\n",
    "    def cal_angle(position_i, embedding_j):\n",
    "        \"\"\"\n",
    "        embedding矩阵初始(i,j)元的值\n",
    "        :param position_i: 矩阵的第i行：对应是是位置i\n",
    "        :param embedding_j: 矩阵的第j列：对应的是位置i在第j个embedding维度上的坐标值\n",
    "        :return: i/((10000^(2*(j//2))*embedding_size)\n",
    "        \"\"\"\n",
    "        return position_i / np.power(10000, 2 * (embedding_j // 2) / embedding_size)\n",
    "\n",
    "    def get_position_i_angle_vec(position_i):\n",
    "        \"\"\"\n",
    "        获取第i行所有元素的值：即one-hot位置编码为i的初始embedding编码\n",
    "        :param position_i: 第i行\n",
    "        :return: 位置i的初始embedding编码\n",
    "        \"\"\"\n",
    "        return [cal_angle(position_i, embedding_j) for embedding_j in range(embedding_size)]\n",
    "\n",
    "    # 首先生成一个size 为[max_seq_len,embedding_size]的embedding矩阵，并初始化每一个(i,j)元\n",
    "    sinusoid_matrix = np.array([get_position_i_angle_vec(pos_i) for pos_i in range(max_seq_len)])\n",
    "    \n",
    "    print(sinusoid_matrix)\n",
    "    \n",
    "    # \n",
    "    sinusoid_matrix[:, 0::2] = np.sin(sinusoid_matrix[:, 0::2])  # dim 2i\n",
    "    print(sinusoid_matrix)\n",
    "    sinusoid_matrix[:, 1::2] = np.cos(sinusoid_matrix[:, 1::2])  # dim 2i+1\n",
    "\n",
    "    # 将占位PAD的embedding值全部置为0\n",
    "    if padding_idx is not None:\n",
    "        # zero vector for padding dimension\n",
    "        sinusoid_matrix[padding_idx] = 0.\n",
    "\n",
    "    assert sinusoid_matrix.shape == (max_seq_len, embedding_size)\n",
    "    return torch.FloatTensor(sinusoid_matrix)\n",
    "A = get_sinusoid_embedding_position_matrix(5,10,0)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8000, 0.1000, 0.0800, 0.0200])\n",
      "温度为1时: tensor([0.4095, 0.2034, 0.1993, 0.1877])\n",
      "温度为2时: tensor([0.3247, 0.2288, 0.2266, 0.2199])\n",
      "温度为0.5时： tensor([0.5904, 0.1456, 0.1399, 0.1241])\n"
     ]
    }
   ],
   "source": [
    "# T temperature 对softmax的影响\n",
    "A = torch.Tensor([0.8,0.1,0.08,0.02])\n",
    "\n",
    "print(A)\n",
    "B = torch.nn.Softmax(dim=0)\n",
    "TB = B(A)\n",
    "print(\"温度为1时:\",TB)\n",
    "\n",
    "T2A = A/2\n",
    "T2B = B(T2A) \n",
    "print(\"温度为2时:\",T2B)\n",
    "\n",
    "T0_5A = A/0.5\n",
    "T0_5B = B(T0_5A)\n",
    "print(\"温度为0.5时：\",T0_5B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9540, -0.3283,  0.4249],\n",
      "        [ 1.3294,  1.4285,  0.2690],\n",
      "        [-1.0591,  0.5096,  0.8885]])\n",
      "tensor([[ True, False, False],\n",
      "        [ True,  True, False],\n",
      "        [ True,  True,  True]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[   -inf, -0.3283,  0.4249],\n",
       "        [   -inf,    -inf,  0.2690],\n",
       "        [   -inf,    -inf,    -inf]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mask 测试:#将 mask必须是一个 ByteTensor 而且shape必须和 a一样 并且元素只能是 0或者1 ，\n",
    "# 是将 mask中为1的 元素所在的索引，在a中相同的的索引处替换为 value  ,mask value必须同为tensor \n",
    "A = torch.randn(3,3)\n",
    "print(A)\n",
    "mask = torch.Tensor([\n",
    "        [1, 0, 0],\n",
    "        [1, 1, 0],\n",
    "        [1, 1, 1]])\n",
    "mask = mask.gt(0) # True, False或者1，0都可以\n",
    "# mask = mask.type(torch.float)\n",
    "print(mask)\n",
    "\n",
    "# mask.byte() 如果mask不是ByteTensor，需要转换为ByteTensor\n",
    "maskedA = A.masked_fill(mask.byte(),value=-np.inf) # 将mask矩阵中为1的值替换为value=-np.inf的值\n",
    "maskedA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inf 无穷小：\n",
    "# python中的正无穷或负无穷，使用float(\"inf\")或float(\"-inf\")来表示。\n",
    "# 这里有点特殊，写成：float(\"inf\")，float(\"INF\")或者float('Inf')都是可以的。\n",
    "# 当涉及 > 和 < 比较时，所有数都比无穷小float(\"-inf\")大，所有数都比无穷大float(\"inf\")小。\n",
    "# 相等比较时，float(\"+inf\")与float(\"+inf\")、float(\"inf\")三者相等\n",
    "float(\"+inf\") == float(\"+inf\")  # True\n",
    "float(\"+inf\") == float(\"inf\")  # True\n",
    "\n",
    "# 简单的加，减，乘操作，还是会返回无穷。\n",
    "float('inf') + 666  # inf\n",
    "float('inf') - 888  # inf\n",
    "float('inf') * 999  # inf\n",
    "float('inf') / 99999  # inf\n",
    "float('inf') + float('inf')  # inf\n",
    "\n",
    "# 特别的：\n",
    "0 * float('inf') # 结果为：nan\n",
    "float('inf') / float('inf') #结果为：nan\n",
    "float('inf') - float('inf') #结果为：nan\n",
    "float('-inf') - float('-inf')# 结果也为：nan\n",
    "\n",
    "# a = np.inf\n",
    "# 50000*a\n",
    "float(\"+inf\") == float(\"-inf\") # False\n",
    "float(\"+inf\") + float(\"-inf\")\n",
    "\n",
    "# 这才是mask的值时负无穷的核心原因，因为e^x次幂，当x=负无穷是，e^x次幂才为零\n",
    "np.exp(float(\"-inf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1189,  0.1209, -0.6030,  0.3495,  0.7878],\n",
       "        [-0.2596,  0.1087,  0.4619,  0.3459,  0.2983],\n",
       "        [-0.3771, -0.6779,  1.5571,  0.9406,  0.1349]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.empty(3,5)\n",
    "print(w)\n",
    "torch.nn.init.xavier_normal_(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 请求 request url 测试\n",
    "from urllib import request\n",
    "# input_url = input()\n",
    "url = 'http://10.220.178.236:33445/'\n",
    "responce = request.urlopen(url,timeout=10)\n",
    "print(responce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# size()[0] \n",
    "A = torch.rand(5,2,3)\n",
    "print(A.size()[0])\n",
    "print(A.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2, 3, 4])\n",
      "torch.Size([3, 5, 2, 4])\n",
      "torch.Size([3, 5, 2, 4])\n",
      "tensor([[[[0.4714, 0.2305, 0.9459, 0.4852],\n",
      "          [0.2764, 0.7511, 0.4704, 0.6629]],\n",
      "\n",
      "         [[0.1100, 0.6929, 0.7017, 0.9346],\n",
      "          [0.0282, 0.5115, 0.8163, 0.3470]],\n",
      "\n",
      "         [[0.4323, 0.7839, 0.5377, 0.3321],\n",
      "          [0.7145, 0.9521, 0.1716, 0.9140]],\n",
      "\n",
      "         [[0.3949, 0.7249, 0.5712, 0.2745],\n",
      "          [0.7954, 0.8462, 0.4438, 0.8894]],\n",
      "\n",
      "         [[0.2763, 0.1533, 0.9317, 0.4697],\n",
      "          [0.8367, 0.0059, 0.4254, 0.8178]]],\n",
      "\n",
      "\n",
      "        [[[0.6567, 0.8809, 0.4095, 0.1144],\n",
      "          [0.3480, 0.0018, 0.0231, 0.2312]],\n",
      "\n",
      "         [[0.3868, 0.2740, 0.2405, 0.5116],\n",
      "          [0.1165, 0.4596, 0.9647, 0.1286]],\n",
      "\n",
      "         [[0.6886, 0.4120, 0.3204, 0.9533],\n",
      "          [0.9490, 0.3948, 0.9956, 0.2800]],\n",
      "\n",
      "         [[0.1088, 0.3903, 0.9092, 0.3009],\n",
      "          [0.7979, 0.8854, 0.3816, 0.0028]],\n",
      "\n",
      "         [[0.3260, 0.9227, 0.8741, 0.4532],\n",
      "          [0.6623, 0.7779, 0.7678, 0.4015]]],\n",
      "\n",
      "\n",
      "        [[[0.7670, 0.4306, 0.5295, 0.5951],\n",
      "          [0.3960, 0.2610, 0.8381, 0.8615]],\n",
      "\n",
      "         [[0.6648, 0.2041, 0.1523, 0.9476],\n",
      "          [0.9644, 0.9791, 0.5015, 0.2240]],\n",
      "\n",
      "         [[0.1760, 0.0506, 0.2871, 0.2331],\n",
      "          [0.2300, 0.5548, 0.7552, 0.8193]],\n",
      "\n",
      "         [[0.3632, 0.2270, 0.0580, 0.4171],\n",
      "          [0.0765, 0.5740, 0.7812, 0.1252]],\n",
      "\n",
      "         [[0.1113, 0.6693, 0.0606, 0.1279],\n",
      "          [0.9144, 0.6653, 0.8294, 0.6959]]]])\n",
      "torch.Size([15, 2, 4])\n",
      "tensor([[[0.4714, 0.2305, 0.9459, 0.4852],\n",
      "         [0.2764, 0.7511, 0.4704, 0.6629]],\n",
      "\n",
      "        [[0.1100, 0.6929, 0.7017, 0.9346],\n",
      "         [0.0282, 0.5115, 0.8163, 0.3470]],\n",
      "\n",
      "        [[0.4323, 0.7839, 0.5377, 0.3321],\n",
      "         [0.7145, 0.9521, 0.1716, 0.9140]],\n",
      "\n",
      "        [[0.3949, 0.7249, 0.5712, 0.2745],\n",
      "         [0.7954, 0.8462, 0.4438, 0.8894]],\n",
      "\n",
      "        [[0.2763, 0.1533, 0.9317, 0.4697],\n",
      "         [0.8367, 0.0059, 0.4254, 0.8178]],\n",
      "\n",
      "        [[0.6567, 0.8809, 0.4095, 0.1144],\n",
      "         [0.3480, 0.0018, 0.0231, 0.2312]],\n",
      "\n",
      "        [[0.3868, 0.2740, 0.2405, 0.5116],\n",
      "         [0.1165, 0.4596, 0.9647, 0.1286]],\n",
      "\n",
      "        [[0.6886, 0.4120, 0.3204, 0.9533],\n",
      "         [0.9490, 0.3948, 0.9956, 0.2800]],\n",
      "\n",
      "        [[0.1088, 0.3903, 0.9092, 0.3009],\n",
      "         [0.7979, 0.8854, 0.3816, 0.0028]],\n",
      "\n",
      "        [[0.3260, 0.9227, 0.8741, 0.4532],\n",
      "         [0.6623, 0.7779, 0.7678, 0.4015]],\n",
      "\n",
      "        [[0.7670, 0.4306, 0.5295, 0.5951],\n",
      "         [0.3960, 0.2610, 0.8381, 0.8615]],\n",
      "\n",
      "        [[0.6648, 0.2041, 0.1523, 0.9476],\n",
      "         [0.9644, 0.9791, 0.5015, 0.2240]],\n",
      "\n",
      "        [[0.1760, 0.0506, 0.2871, 0.2331],\n",
      "         [0.2300, 0.5548, 0.7552, 0.8193]],\n",
      "\n",
      "        [[0.3632, 0.2270, 0.0580, 0.4171],\n",
      "         [0.0765, 0.5740, 0.7812, 0.1252]],\n",
      "\n",
      "        [[0.1113, 0.6693, 0.0606, 0.1279],\n",
      "         [0.9144, 0.6653, 0.8294, 0.6959]]])\n"
     ]
    }
   ],
   "source": [
    "# permute , contiguous 测试\n",
    "A = torch.rand(5,2,3,4)\n",
    "print(A.size()) # torch.Size([5, 2, 3, 4])\n",
    "permuteA = A.permute(2,0,1,3)\n",
    "print(permuteA.size()) # torch.Size([3, 5, 2, 4])\n",
    "# contiguousA 并不会改变tensor的size，但是会把内存地址变为连续的，在经过permute操作后tensor的物理地址不是连续的了，\n",
    "# contiguous()将其变成连续的物理地址是为了加速运算 :https://blog.csdn.net/gdymind/article/details/82662502\n",
    "contiguousA = permuteA.contiguous() \n",
    "print(contiguousA.size()) # torch.Size([3, 5, 2, 4])\n",
    "print(contiguousA)\n",
    "\n",
    "reviewA = contiguousA.view(-1,contiguousA.size()[2],contiguousA.size()[3])\n",
    "print(reviewA.size())\n",
    "print(reviewA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 2)\n",
      "[[[ 0.38180431 -1.37430505]\n",
      "  [-1.73485071  1.31281657]\n",
      "  [-1.53812533  0.40543712]]\n",
      "\n",
      " [[-0.77941135  0.63711421]\n",
      "  [ 0.76403162  0.32525701]\n",
      "  [-0.80860121 -0.39746584]]]\n",
      "(24,)\n",
      "[ 0.38180431  0.38180431 -1.37430505 -1.37430505 -1.73485071 -1.73485071\n",
      "  1.31281657  1.31281657 -1.53812533 -1.53812533  0.40543712  0.40543712\n",
      " -0.77941135 -0.77941135  0.63711421  0.63711421  0.76403162  0.76403162\n",
      "  0.32525701  0.32525701 -0.80860121 -0.80860121 -0.39746584 -0.39746584]\n",
      "(2, 3, 4)\n",
      "[[[ 0.38180431  0.38180431 -1.37430505 -1.37430505]\n",
      "  [-1.73485071 -1.73485071  1.31281657  1.31281657]\n",
      "  [-1.53812533 -1.53812533  0.40543712  0.40543712]]\n",
      "\n",
      " [[-0.77941135 -0.77941135  0.63711421  0.63711421]\n",
      "  [ 0.76403162  0.76403162  0.32525701  0.32525701]\n",
      "  [-0.80860121 -0.80860121 -0.39746584 -0.39746584]]]\n"
     ]
    }
   ],
   "source": [
    "#  nparrat 的 repeat 操作测试 : https://blog.csdn.net/u010496337/article/details/50572866\n",
    "A = np.random.randn(2,3,2)\n",
    "print(A.shape)\n",
    "print(A)\n",
    "\n",
    "# 当 axis = None 或者不传 axis 时：会将A所有元素打平成然后进行复制指定次数\n",
    "repeatA = A.repeat(2,axis = None) \n",
    "print(repeatA.shape)\n",
    "print(repeatA)\n",
    "\n",
    "repeatAasix = A.repeat(2,axis = 2) # 会把axis指定的那个维度的数据进行复制\n",
    "print(repeatAasix.shape)\n",
    "print(repeatAasix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 2])\n",
      "tensor([[[0.5005, 0.0907],\n",
      "         [0.4454, 0.1376],\n",
      "         [0.1669, 0.1448]],\n",
      "\n",
      "        [[0.0815, 0.4490],\n",
      "         [0.9542, 0.7468],\n",
      "         [0.6756, 0.0252]]])\n",
      "torch.Size([4, 6, 2])\n",
      "tensor([[[0.5005, 0.0907],\n",
      "         [0.4454, 0.1376],\n",
      "         [0.1669, 0.1448],\n",
      "         [0.5005, 0.0907],\n",
      "         [0.4454, 0.1376],\n",
      "         [0.1669, 0.1448]],\n",
      "\n",
      "        [[0.0815, 0.4490],\n",
      "         [0.9542, 0.7468],\n",
      "         [0.6756, 0.0252],\n",
      "         [0.0815, 0.4490],\n",
      "         [0.9542, 0.7468],\n",
      "         [0.6756, 0.0252]],\n",
      "\n",
      "        [[0.5005, 0.0907],\n",
      "         [0.4454, 0.1376],\n",
      "         [0.1669, 0.1448],\n",
      "         [0.5005, 0.0907],\n",
      "         [0.4454, 0.1376],\n",
      "         [0.1669, 0.1448]],\n",
      "\n",
      "        [[0.0815, 0.4490],\n",
      "         [0.9542, 0.7468],\n",
      "         [0.6756, 0.0252],\n",
      "         [0.0815, 0.4490],\n",
      "         [0.9542, 0.7468],\n",
      "         [0.6756, 0.0252]]])\n"
     ]
    }
   ],
   "source": [
    "# torch tensor 的 repeat 操作\n",
    "A = torch.rand(2,3,2)\n",
    "print(A.size())\n",
    "print(A)\n",
    "\n",
    "repeatA = A.repeat(2,2,1) # 把对应的维度0，维度1，维度2分别复制参数1，参数2，参数3次\n",
    "print(repeatA.size())\n",
    "print(repeatA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# count() 测试\n",
    "src = \"src sequence length\"\n",
    "src_len = src.count(' ')+1\n",
    "print(src_len)\n",
    "\n",
    "target = \"目标语言句子长度\"\n",
    "target_len = target.count(\" \") + 1\n",
    "print(target_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 1])\n",
      "tensor([[[1.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [0.]]])\n",
      "tensor([[[-2.0446,  0.6588, -0.6521, -0.3864,  0.1387],\n",
      "         [-0.3949, -0.4488,  1.5259,  0.0786,  0.5767]],\n",
      "\n",
      "        [[-0.0515,  0.1520, -0.5918,  2.3968, -0.6222],\n",
      "         [ 2.0832,  1.1323,  1.2282, -0.1192, -0.3583]],\n",
      "\n",
      "        [[-0.0726,  0.2926, -0.0901, -1.3523, -0.4484],\n",
      "         [ 0.7138,  0.2086, -0.5504,  0.9071,  2.2067]]])\n",
      "tensor([[[-2.0446,  0.6588, -0.6521, -0.3864,  0.1387],\n",
      "         [-0.0000, -0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.0000,  0.0000, -0.0000,  0.0000, -0.0000],\n",
      "         [ 2.0832,  1.1323,  1.2282, -0.1192, -0.3583]],\n",
      "\n",
      "        [[-0.0726,  0.2926, -0.0901, -1.3523, -0.4484],\n",
      "         [ 0.0000,  0.0000, -0.0000,  0.0000,  0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "# non_pad_mask 测试\n",
    "PAD = 0\n",
    "non_pad_mask = torch.tensor([[ 1., 0],\n",
    "        [0, 1.],\n",
    "        [ 1., 0]]).ne(PAD).type(torch.float).unsqueeze(-1) # unsqueeze操作增加了一个维度\n",
    "print(non_pad_mask.size())\n",
    "print(non_pad_mask)\n",
    "\n",
    "A = torch.randn(3,2,5)\n",
    "print(A)\n",
    "A *= non_pad_mask\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 0.]])\n",
      "tensor([[ True, False, False],\n",
      "        [ True,  True,  True],\n",
      "        [ True,  True, False]])\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# tensor.ne 操作\n",
    "PAD = 0\n",
    "non_pad_mask = torch.tensor([[ 1., 0, 0],\n",
    "        [1., 1.,1.],\n",
    "        [ 1., 1.,0]])\n",
    "print(non_pad_mask)\n",
    "ne_non_pad_mask = non_pad_mask.ne(PAD)\n",
    "print(ne_non_pad_mask)\n",
    "ne_non_pad_mask_float = ne_non_pad_mask.type(torch.float)\n",
    "print(ne_non_pad_mask_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len_seq_q 3\n",
      "torch.Size([3, 3])\n",
      "tensor([[False,  True,  True],\n",
      "        [False, False, False],\n",
      "        [False, False,  True]])\n",
      "torch.Size([3, 1, 3])\n",
      "tensor([[[False,  True,  True]],\n",
      "\n",
      "        [[False, False, False]],\n",
      "\n",
      "        [[False, False,  True]]])\n",
      "torch.Size([3, 3, 3])\n",
      "tensor([[[False,  True,  True],\n",
      "         [False,  True,  True],\n",
      "         [False,  True,  True]],\n",
      "\n",
      "        [[False, False, False],\n",
      "         [False, False, False],\n",
      "         [False, False, False]],\n",
      "\n",
      "        [[False, False,  True],\n",
      "         [False, False,  True],\n",
      "         [False, False,  True]]])\n"
     ]
    }
   ],
   "source": [
    "# attention q ,k pad mask\n",
    "PAD = 0\n",
    "seq_q = torch.tensor([[ 1., 0, 0],\n",
    "        [1., 1.,1.],\n",
    "        [ 1., 1.,0]])\n",
    "seq_k= torch.tensor([[ 1., 0, 0],\n",
    "        [1., 1.,1.],\n",
    "        [ 1., 1.,0]])\n",
    "len_seq_q = seq_q.size(1)\n",
    "print(\"len_seq_q\",len_seq_q)\n",
    "mask = seq_k.eq(PAD)\n",
    "print(mask.size())\n",
    "print(mask)\n",
    "mask_unsqueeze = mask.unsqueeze(1) # 在维度1上扩展一维：[3, 3] 变成 [3,1,3]\n",
    "print(mask_unsqueeze.size())\n",
    "print(mask_unsqueeze)\n",
    "mask_unsqueeze_expand = mask_unsqueeze.expand(-1,len_seq_q,-1) # 把[3,1,3]变成了[3, 3, 3]\n",
    "print(mask_unsqueeze_expand.size())\n",
    "print(mask_unsqueeze_expand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "=======上三角测试======\n",
      "tensor([[0., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "=======下三角测试======\n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "=============\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "=============\n",
      "tensor([[1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "=============\n",
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 上三角行列式(主对角线以下全为0)，或者下三角行列式， torch.triu :上三角 ； torch.tril :下三角 ;\n",
    "A = torch.ones(5,5)\n",
    "print(A)\n",
    "print(\"=======上三角测试======\")\n",
    "triuA0 = torch.triu(A,diagonal=1) # \n",
    "print(triuA0)\n",
    "\n",
    "\n",
    "print(\"=======下三角测试======\")\n",
    "trilA = torch.tril(A) # 不传diagonal什么也不变\n",
    "print(A)\n",
    "print(\"=============\")\n",
    "trilA0 = torch.tril(A,diagonal=0) # diagonal=0 ：主对角线以上全为0，不含主对角线的元素\n",
    "print(trilA0)\n",
    "print(\"=============\")\n",
    "trilA1 = torch.tril(A,diagonal=2) # diagonal>0 : 主对角线以上，靠近主对角线切和主对角线平行的diagonal行不为零，其余全为零\n",
    "print(trilA1)\n",
    "print(\"=============\")\n",
    "trilA_neg1 = torch.tril(A,diagonal=-1) # -1 ，-2 原理同上\n",
    "print(trilA_neg1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True,  True],\n",
       "        [False, False,  True,  True,  True],\n",
       "        [False, False, False,  True,  True],\n",
       "        [False, False, False, False,  True],\n",
       "        [False, False, False, False, False]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.gt()操作\n",
    "A = torch.tensor([[0., 1., 1., 1., 1.],\n",
    "        [0., 0., 1., 1., 1.],\n",
    "        [0., 0., 0., 1., 1.],\n",
    "        [0., 0., 0., 0., 1.],\n",
    "        [0., 0., 0., 0., 0.]])\n",
    "A.gt(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "tensor([[-0.5466, -1.0194, -0.3357],\n",
      "        [-0.6542, -0.0847,  0.2539],\n",
      "        [ 0.4246,  0.5816, -0.3865],\n",
      "        [ 1.1795,  0.2402, -0.3308],\n",
      "        [-0.5970, -1.8826,  2.3186]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# filter 测试:过滤出那些requires_grad为true的tensor\n",
    "A = torch.randn(5,3,requires_grad=True)\n",
    "print(A.requires_grad)\n",
    "B = torch.randn(3,2,requires_grad=False)\n",
    "print(B.requires_grad)\n",
    "filterC = filter(lambda x:x.requires_grad,[A,B])\n",
    "print(iter(filterC).__next__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1., -1.,  0.,  2.,  1.],\n",
      "        [-1., -1.,  0.,  2.,  1.],\n",
      "        [-1., -1.,  0.,  2.,  1.]], requires_grad=True)\n",
      "tensor([3, 1, 4])\n",
      "tensor(5.4152, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# cross_entropy 测试\n",
    "#[seq_len,ont_hot_embedding_size] [句子中有三个词，ont_hot_embedding_size为5] ;这些都是最原始的全连接输出的值\n",
    "predict = torch.tensor([[-1, -1, 0,  2, 1],\n",
    "        [-1, -1, 0,  2, 1],\n",
    "        [-1, -1, 0,  2, 1]],dtype=torch.float32,requires_grad=True)\n",
    "print(predict)\n",
    "\n",
    "\n",
    "# target = torch.randint(5,(3,),dtype=torch.int64) # tensor([0, 1, 4]) # 5代表ont_hot_embedding_size，3代表seq_len\n",
    "# target的维度会比predict少一维，因为target直接用数字表示了one-hot对应的位置\n",
    "target = torch.tensor([3, 1, 4],dtype=torch.int64) # 3, 1, 4分别对应词表中的第4, 2, 5位置上的词\n",
    "print(target)\n",
    "\n",
    "# cross_entropy 会先进行log_softmax求出\n",
    "loss = torch.functional.F.cross_entropy(predict,target,reduction='mean') # mean是平均loss\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.032058604\n",
      "0.087144315\n",
      "0.2368828\n",
      "0.6439142\n",
      "==========================\n",
      "-5.320569\n",
      "-1.773523\n"
     ]
    }
   ],
   "source": [
    "# 对上面的计算进行手动的求cross_entropy\n",
    "# 先求对数值\n",
    "en1 = np.exp(-1,dtype=np.float32) # 0.36787944117144233\n",
    "e0 = np.exp(0,dtype=np.float32) # 1.0\n",
    "e1 = np.exp(1,dtype=np.float32) # 2.718281828459045\n",
    "e2 = np.exp(2,dtype=np.float32) # 7.38905609893065\n",
    "# 对数加和\n",
    "sum_e = en1 + e0 + e1+ e2 # 11.475217368561138\n",
    "# 求softmax的概率\n",
    "pen1 = np.divide(en1,sum_e,dtype=np.float32) # 0.03205860328008499\n",
    "print(pen1)\n",
    "pe0 =  np.divide(e0,sum_e,dtype=np.float32)  # 0.08714431874203257\n",
    "print(pe0)\n",
    "pe1 = np.divide(e1,sum_e,dtype=np.float32)  # 0.23688281808991013\n",
    "print(pe1)\n",
    "pe2 = np.divide(e2,sum_e,dtype=np.float32) # 0.6439142598879724\n",
    "print(pe2)\n",
    "\n",
    "print(\"==========================\")\n",
    "# 3, 1, 4位置分别代表了pe2，pen1，pe1的值\n",
    "log_sum = np.log(pe2,dtype=np.float32)+np.log(pen1,dtype=np.float32)+np.log(pe1,dtype=np.float32)\n",
    "print(log_sum)\n",
    "log_sum_mean = np.divide(log_sum,3,dtype=np.float32)\n",
    "print(log_sum_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0841, -0.5210,  0.9787],\n",
      "        [-0.9357,  0.0630,  0.6080],\n",
      "        [ 0.1092,  0.4593, -0.7256],\n",
      "        [ 1.6098, -0.1594, -0.2879],\n",
      "        [-2.5540, -0.4800, -1.1671]])\n",
      "tensor([ 0.0841, -0.5210,  0.9787, -0.9357,  0.0630,  0.6080,  0.1092,  0.4593,\n",
      "        -0.7256,  1.6098, -0.1594, -0.2879, -2.5540, -0.4800, -1.1671])\n"
     ]
    }
   ],
   "source": [
    "# view(-1) 测试\n",
    "A = torch.randn(5,3)\n",
    "print(A)\n",
    "print(A.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0976, -1.8255],\n",
      "         [ 0.1194, -0.7030],\n",
      "         [-1.5159,  1.4731]],\n",
      "\n",
      "        [[ 0.0549, -1.5263],\n",
      "         [-0.1463, -1.8223],\n",
      "         [-0.4064,  0.5603]],\n",
      "\n",
      "        [[ 0.3360,  1.8366],\n",
      "         [ 1.0848, -0.3860],\n",
      "         [ 1.4632, -0.8524]],\n",
      "\n",
      "        [[ 0.2769, -0.9465],\n",
      "         [-0.3930,  1.2240],\n",
      "         [-1.3663,  0.2402]],\n",
      "\n",
      "        [[-0.3528, -1.3213],\n",
      "         [ 0.3954, -1.2281],\n",
      "         [-0.3469, -0.6532]]])\n",
      "tensor([[[ 0.0976, -1.8255],\n",
      "         [ 0.1194, -0.7030]],\n",
      "\n",
      "        [[ 0.0549, -1.5263],\n",
      "         [-0.1463, -1.8223]],\n",
      "\n",
      "        [[ 0.3360,  1.8366],\n",
      "         [ 1.0848, -0.3860]],\n",
      "\n",
      "        [[ 0.2769, -0.9465],\n",
      "         [-0.3930,  1.2240]],\n",
      "\n",
      "        [[-0.3528, -1.3213],\n",
      "         [ 0.3954, -1.2281]]])\n",
      "tensor([[[-0.5264,  1.2036]],\n",
      "\n",
      "        [[-1.6966, -0.5756]],\n",
      "\n",
      "        [[ 1.2386,  1.0165]],\n",
      "\n",
      "        [[ 1.3282,  0.0920]],\n",
      "\n",
      "        [[ 0.5388, -0.6176]]])\n"
     ]
    }
   ],
   "source": [
    "# [:,1:] 操作是把第二维度的索引位置为0位置的那个值去掉，因为这是预测下一个值\n",
    "A = torch.randn(5,3,2)\n",
    "print(A)\n",
    "\n",
    "trainB = A[:,:-1] # decoder训练的时候不需要最后一个词\n",
    "print(trainB)\n",
    "\n",
    "goldB = A[:,1:] # 维度1不变，维度2从1开始(相当于维度2去掉了第一组数) # loss回归的时候需要用的taget去掉第一个位置的词\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
